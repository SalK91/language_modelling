# Evaluating NLU Systems

Evaluating the performance of Natural Language Understanding (NLU) systems requires standardized benchmarks and carefully chosen evaluation metrics. Unlike open-ended generation tasks, NLU tasks are typically close-ended, meaning they have a limited and well-defined set of correct outputsâ€”such as binary, categorical, or span-based labels.

These tasks lend themselves to automatic evaluation using standard supervised learning techniques, making them particularly well-suited for large-scale benchmarking. Their constrained output space allows for clear-cut metrics such as accuracy, precision, recall, and F1-score, enabling reproducible and objective comparisons across models.

### Common Close-ended NLP Tasks and Benchmarks

- Sentiment Analysis: Classify text by sentiment polarity (positive, negative, neutral).
     - Benchmarks: SST-2, IMDB, Yelp Reviews

- Natural Language Inference (NLI): Determine whether a hypothesis is entailed, contradicted by, or neutral to a premise.
    - Benchmarks: SNLI, MultiNLI

- Named Entity Recognition (NER): Identify and classify named entities (e.g., people, organizations, locations) in text.
     - Benchmark: CoNLL-2003

- Part-of-Speech (POS) Tagging: Assign syntactic categories to each token in a sentence.
     - Benchmark: Penn Treebank (PTB)

- Coreference Resolution: Identify mentions in text that refer to the same entity.
     - Benchmark: Winograd Schema Challenge (WSC)

- Question Answering (QA): Answer factual questions based on a passage of text.
     - Benchmark: SQuAD v2.0 (includes unanswerable questions)

- Multi-task Benchmarking:
      - SuperGLUE is a prominent multi-task benchmark for close-ended evaluation. It extends GLUE by including harder tasks with a focus on deeper linguistic and reasoning skills.

### Evaluation Metrics

The choice of evaluation metric depends on the nature of the task and class balance:

- Accuracy: Proportion of correct predictions. Suitable for balanced datasets with a single label per instance.

- Precision / Recall / F1-score: Useful in imbalanced settings.
    - Precision: Ratio of true positives to all predicted positives.
    - Recall: Ratio of true positives to all actual positives.
    - F1-score: Harmonic mean of precision and recall.

- ROC-AUC: Area under the receiver operating characteristic curve. Appropriate for binary classifiers, especially in imbalanced settings.

Aggregating Performance Across Tasks or Metrics:  
For multi-task evaluations (e.g., SuperGLUE), scores across tasks may be aggregated by computing a macro-average or weighted average of individual task metrics. However, task heterogeneity can make aggregate scoring misleading without task-level inspection.

## Evaluation Challenges

- Source of Labels: Many benchmarks rely on crowd-sourced annotations, which can introduce label noise and inconsistencies. It's important to consider inter-annotator agreement and annotation protocols.

- Spurious Correlations: Models often exploit superficial patterns in training data (e.g., specific lexical cues) that do not generalize to out-of-distribution examples. This can result in overestimated performance on test sets that share similar artifacts.

- Benchmark Saturation: Many established benchmarks (e.g., GLUE, SQuAD) have been nearly saturated by large models. Performance gains on these datasets may no longer reflect true improvements in generalization or reasoning.

- Generalization vs Memorization: Close-ended evaluations often do not measure generalization under distribution shift. It is critical to supplement them with robustness tests or adversarial datasets.

## Summary

Close-ended evaluations remain a cornerstone of NLP model assessment due to their scalability and reliability. They are particularly useful for low-level tasks (e.g., tagging, classification) and for benchmarking multi-task generalization. However, their automatic nature can obscure deeper limitations in model reasoning, generalization, and alignment with real-world goals. Careful metric selection, robust dataset construction, and task-specific error analysis remain essential.
