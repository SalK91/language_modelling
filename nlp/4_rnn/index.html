<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on Natural Language Processing.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/language_modelling/nlp/4_rnn/">
      
      
        <link rel="prev" href="../3_nn/">
      
      
        <link rel="next" href="../5_lstm/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>4. Recurrent Neural Networks - NLP Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/print-site.css">
    
      <link rel="stylesheet" href="../../css/print-site-material.css">
    
      <link rel="stylesheet" href="../../styles/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="NLP Lecture Notes" class="md-header__button md-logo" aria-label="NLP Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            NLP Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              4. Recurrent Neural Networks
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to dark mode" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan" aria-label="Switch to light mode" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../1_intro/" class="md-tabs__link">
          
  
  
    
  
  Natural Language Processing

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../print_page/" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="NLP Lecture Notes" class="md-nav__button md-logo" aria-label="NLP Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    NLP Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Natural Language Processing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. NLP an introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../2_lmnp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Language Models and Word Sequence Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../3_nn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Window-based Neural Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    4. Recurrent Neural Networks
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../5_lstm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Long Short-Term Memory (LSTM) Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../6_app_rnn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Applications of RNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../7_eval_nlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Evaluation of Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../8_attention_seq2seq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Attention Mechanism in Sequence-to-Sequence Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../9_selfattention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Self- Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10_archi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Transformer Architectures
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11_tokens/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Tokenization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12_pretraining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Pre-training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13_pretrain_strat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Pre-training Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../14_finetuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Fine-tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../15_cot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    15. Chain of Thought Reasoning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../16_nlg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    16. Nature Language Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../17_imp_nlg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    17. Improving Generation and Training for NLG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../18_eval_nlu/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    18. Evaluation of NLU Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../19_eval_nlg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    19. Evaluation of NLG Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../20_post_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    20. Post-training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../21_advanced_topics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    21. Advanced Topics in Language Modelling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../22_llm_training_basics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    22. LLM Training Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../23_reasoning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    23. Reasoning in LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../print_page/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SalK91/edit/main/docs/nlp/4_rnn.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SalK91/raw/main/docs/nlp/4_rnn.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


  <h1>4. Recurrent Neural Networks</h1>

<h2 id="chapter-4-recurrent-neural-networks">Chapter 4: Recurrent Neural Networks<a class="headerlink" href="#chapter-4-recurrent-neural-networks" title="Permanent link">¶</a></h2>
<p>Unlike traditional language models that condition on a fixed-size window of previous tokens, RNNs are capable of conditioning on the entire history of previous tokens.</p>
<p>This is achieved by maintaining a recurrent hidden state that summarizes past information and is updated at every time-step.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../images/rnn_3step.png" data-desc-position="bottom"><img alt="A Recurrent Neural Network" src="../images/rnn_3step.png"></a></p>
<h2 id="rnn-architecture">RNN Architecture<a class="headerlink" href="#rnn-architecture" title="Permanent link">¶</a></h2>
<p>The RNN architecture (Figure above) shows three time-steps. Each hidden layer at time-step <span class="arithmatex">\(t\)</span> receives two inputs:</p>
<ul>
<li>The input vector at time <span class="arithmatex">\(t\)</span>, <span class="arithmatex">\(x_t\)</span></li>
<li>The hidden state from the previous time-step, <span class="arithmatex">\(h_{t-1}\)</span></li>
</ul>
<p>These are combined using weights <span class="arithmatex">\(W_{hh}\)</span> and <span class="arithmatex">\(W_{hx}\)</span>:</p>
<div class="arithmatex">\[
h_t = \sigma(W_{hh} h_{t-1} + W_{hx} x_t + b_1)
\]</div>
<p>The hidden state <span class="arithmatex">\(h_t\)</span> acts as a continuous memory vector that compresses information from all previous inputs <span class="arithmatex">\(x_1, \dots, x_t\)</span>.</p>
<p>The output is calculated as:</p>
<div class="arithmatex">\[
\hat{y}_t = \text{softmax}(W_S h_t + b_2)
\]</div>
<p>Key parameters:</p>
<ul>
<li><span class="arithmatex">\(x_t \in \mathbb{R}^d\)</span>: Input word vector  </li>
<li><span class="arithmatex">\(W_{hx} \in \mathbb{R}^{D_h \times d}\)</span>: Input weight matrix  </li>
<li><span class="arithmatex">\(W_{hh} \in \mathbb{R}^{D_h \times D_h}\)</span>: Hidden state weight matrix  </li>
<li><span class="arithmatex">\(W_S \in \mathbb{R}^{|V| \times D_h}\)</span>: Output weight matrix  </li>
<li><span class="arithmatex">\(\sigma\)</span>: Non-linear function such as tanh  </li>
<li><span class="arithmatex">\(\hat{y}_t \in \mathbb{R}^{|V|}\)</span>: Probability distribution over vocabulary  </li>
</ul>
<h2 id="training-an-rnn-language-model">Training an RNN Language Model<a class="headerlink" href="#training-an-rnn-language-model" title="Permanent link">¶</a></h2>
<p>To train a Recurrent Neural Network Language Model (RNN-LM), we begin with a large corpus of text, represented as a sequence of word tokens. The model is trained to predict the next word at each time-step, given all preceding words in the sequence.</p>
<p>At each time-step <span class="arithmatex">\(t\)</span>:</p>
<ul>
<li>The model receives an input vector <span class="arithmatex">\(x_t\)</span> and computes a hidden state <span class="arithmatex">\(h_t\)</span> using the current input and the previous hidden state <span class="arithmatex">\(h_{t-1}\)</span>.</li>
<li>The output layer produces a probability distribution <span class="arithmatex">\(\hat{y}_t\)</span> over the vocabulary.</li>
<li>The cross-entropy loss is computed between <span class="arithmatex">\(\hat{y}_t\)</span> and the ground-truth one-hot vector <span class="arithmatex">\(y_t\)</span>.</li>
</ul>
<p>Training an RNN language model corresponds to maximizing the log-likelihood of the observed sequence:</p>
<div class="arithmatex">\[\log P(x_1, \ldots, x_T)
= \sum_{t=1}^{T} \log P\!\left(x_{t+1} \mid x_1, \ldots, x_t\right)\]</div>
<p>The total training objective is to minimize the average cross-entropy loss across all time-steps and sequences in the training set.</p>
<div class="arithmatex">\[
J^{(t)}(\theta) = -\sum_{j=1}^{|V|} y_{t,j} \log \hat{y}_{t,j}
\]</div>
<h3 id="teacher-forcing">Teacher Forcing<a class="headerlink" href="#teacher-forcing" title="Permanent link">¶</a></h3>
<p>During training, a technique called teacher forcing is commonly used. Instead of feeding the model’s own prediction <span class="arithmatex">\(\hat{y}_{t-1}\)</span> as input at the next time-step, we use the ground-truth word <span class="arithmatex">\(y_{t-1}\)</span>. This stabilizes and accelerates learning by preventing the model from compounding its own mistakes during early training. However, it introduces a discrepancy between training and inference, known as exposure bias, since the model must rely on its own predictions at test time. Exposure bias becomes more pronounced for long sequences, where early prediction errors can cascade through the remainder of the sequence.</p>
<h3 id="backpropagation-through-time">Backpropagation Through Time<a class="headerlink" href="#backpropagation-through-time" title="Permanent link">¶</a></h3>
<p>Training an RNN requires computing gradients over sequences of time-dependent operations. This is done using Backpropagation Through Time (BPTT):</p>
<ul>
<li>The RNN is unrolled over <span class="arithmatex">\(T\)</span> time-steps, creating a computational graph where the same parameters are shared at each step.</li>
<li>Gradients are computed by propagating errors backward through time from <span class="arithmatex">\(t = T\)</span> to <span class="arithmatex">\(t = 0\)</span>, summing contributions from all steps.</li>
<li>This process allows the model to adjust its parameters based on dependencies that span multiple time-steps.</li>
</ul>
<p>In practice, full BPTT over long sequences can be computationally expensive and unstable. Therefore, a variant called truncated BPTT is often used, where backpropagation is limited to a fixed number of time-steps (e.g., 20 or 50). This trades off full temporal context for efficiency and stability. Truncated BPTT implicitly limits the temporal horizon over which dependencies can be learned, acting as a practical approximation to full sequence optimization.</p>
<p>BPTT is sensitive to gradient vanishing and exploding problems, especially with non-linear activations like tanh or sigmoid. Techniques such as gradient clipping and using gated architectures like LSTMs or GRUs are commonly employed to address these issues.</p>
<h3 id="efficient-training-with-mini-batches">Efficient Training with Mini-Batches<a class="headerlink" href="#efficient-training-with-mini-batches" title="Permanent link">¶</a></h3>
<p>Computing the loss and gradients over the entire corpus simultaneously is impractical due to memory limitations. Instead, training is performed on smaller units using mini-batches:</p>
<ul>
<li>The text corpus is divided into sequences such as sentences or fixed-length token chunks.</li>
<li>A batch of these sequences is processed together to compute the average loss and gradients.</li>
<li>Stochastic Gradient Descent (SGD) or its variants (e.g., Adam) are used to update parameters based on each batch.</li>
</ul>
<p>Handling variable-length sequences within batches requires padding and masking:</p>
<ul>
<li>Shorter sequences are padded with special tokens to match the longest sequence in the batch.</li>
<li>Padding positions are masked to prevent them from contributing to the loss or gradient updates.</li>
</ul>
<p>This mini-batch training paradigm enables scalable computation and leverages hardware acceleration such as GPUs and TPUs.</p>
<h3 id="training-vs-inference">Training vs. Inference<a class="headerlink" href="#training-vs-inference" title="Permanent link">¶</a></h3>
<p>At inference time, teacher forcing is not available. Each predicted word is fed back into the model as input for the next step. This difference in input distribution between training and inference can degrade performance if the model is not robust to its own prediction errors. Techniques such as scheduled sampling or fine-tuning with generated sequences are sometimes used to mitigate this mismatch.</p>
<h3 id="perplexity">Perplexity<a class="headerlink" href="#perplexity" title="Permanent link">¶</a></h3>
<ul>
<li>The standard evaluation metric for language models is perplexity.</li>
</ul>
<div class="arithmatex">\[
\text{perplexity} =
\left(
\prod_{t=1}^{T}
\frac{1}{P_{\text{LM}}(x^{(t+1)} \mid x^{(1)}, \ldots, x^{(t)})}
\right)^{\frac{1}{T}}
\]</div>
<ul>
<li>Inverse probability of corpus according to the language model, normalized by the number of words.</li>
<li>This is equal to the exponential of the cross-entropy loss <span class="arithmatex">\(J(\theta)\)</span>:</li>
</ul>
<div class="arithmatex">\[
\left(
\prod_{t=1}^{T}
\frac{1}{\hat{y}^{(t)}_{x_{t+1}}}
\right)^{\frac{1}{T}}
=
\exp\left(
\frac{1}{T}
\sum_{t=1}^{T}
-\log \hat{y}^{(t)}_{x_{t+1}}
\right)
=
\exp(J(\theta))
\]</div>
<p>Perplexity can be viewed as the geometric mean of the inverse predicted probabilities assigned to the true next tokens.</p>
<p>Perplexity can be interpreted as the effective average number of choices the model is considering at each time-step:</p>
<ul>
<li>A perplexity of 1 means the model predicts every word perfectly.</li>
<li>A perplexity of 50 means the model is as uncertain as choosing uniformly from 50 possible words.</li>
</ul>
<p>Lower perplexity indicates better predictive performance and correlates with fluency and accuracy in language generation.</p>
<p>Note: Perplexity is sensitive to vocabulary size and tokenization. Models should be compared under identical preprocessing conditions.</p>
<h2 id="memory-scaling-in-recurrent-neural-networks">Memory Scaling in Recurrent Neural Networks<a class="headerlink" href="#memory-scaling-in-recurrent-neural-networks" title="Permanent link">¶</a></h2>
<p>From a machine learning perspective, the memory requirements of RNNs can be divided into model parameters and runtime memory.</p>
<h3 id="model-parameters-static-memory">Model Parameters (Static Memory)<a class="headerlink" href="#model-parameters-static-memory" title="Permanent link">¶</a></h3>
<p>These are the weights and biases defining the RNN structure:</p>
<ul>
<li><span class="arithmatex">\(W_{hx} \in \mathbb{R}^{D_h \times d}\)</span></li>
<li><span class="arithmatex">\(W_{hh} \in \mathbb{R}^{D_h \times D_h}\)</span></li>
<li><span class="arithmatex">\(W_S \in \mathbb{R}^{|V| \times D_h}\)</span></li>
<li>Bias vectors for each layer</li>
</ul>
<p>The memory required to store these parameters is constant with respect to corpus size. That is, increasing the number of sentences or tokens in the dataset does not change the size of these matrices.</p>
<h3 id="runtime-memory-dynamic-per-sequence">Runtime Memory (Dynamic, per sequence)<a class="headerlink" href="#runtime-memory-dynamic-per-sequence" title="Permanent link">¶</a></h3>
<p>During training with BPTT, the RNN stores:</p>
<ul>
<li>Input vectors <span class="arithmatex">\(x_t\)</span></li>
<li>Hidden states <span class="arithmatex">\(h_t\)</span></li>
<li>Intermediate gradient-related states</li>
</ul>
<p>Runtime memory scales linearly with sequence length <span class="arithmatex">\(T\)</span>. A sentence with <span class="arithmatex">\(T\)</span> words requires storage for <span class="arithmatex">\(T\)</span> hidden states and possibly <span class="arithmatex">\(T\)</span> sets of gradients.</p>
<ul>
<li>Model parameter count depends only on architecture parameters <span class="arithmatex">\(D_h\)</span>, <span class="arithmatex">\(d\)</span>, and <span class="arithmatex">\(|V|\)</span>.</li>
<li>Training memory grows proportionally with input sequence length.</li>
</ul>
<p>Insight: RNNs are more memory-efficient than traditional <span class="arithmatex">\(n\)</span>-gram models in terms of model size, but training on long sequences increases memory usage due to the need to store intermediate activations over time.</p>
<h2 id="advantages-and-disadvantages">Advantages and Disadvantages<a class="headerlink" href="#advantages-and-disadvantages" title="Permanent link">¶</a></h2>
<p>Advantages:</p>
<ol>
<li>Can handle variable-length sequences  </li>
<li>Parameter size independent of input length  </li>
<li>Can model long-range dependencies  </li>
<li>Weight sharing across time-steps  </li>
</ol>
<p>Disadvantages:</p>
<ol>
<li>Sequential computation limits parallelization  </li>
<li>Prone to vanishing and exploding gradients  </li>
<li>Difficulty in learning very long-term dependencies in practice</li>
</ol>
<h2 id="vanishing-and-exploding-gradients">Vanishing and Exploding Gradients<a class="headerlink" href="#vanishing-and-exploding-gradients" title="Permanent link">¶</a></h2>
<p>The total gradient with respect to parameters is:</p>
<div class="arithmatex">\[
\frac{\partial E}{\partial W}
=
\sum_{t=1}^{T}
\sum_{k=1}^{t}
\frac{\partial E_t}{\partial y_t}
\cdot
\frac{\partial y_t}{\partial h_t}
\cdot
\left(
\prod_{j=k+1}^{t}
\frac{\partial h_j}{\partial h_{j-1}}
\right)
\cdot
\frac{\partial h_k}{\partial W}
\]</div>
<p>Jacobian norm bound:</p>
<div class="arithmatex">\[
\left\|
\frac{\partial h_t}{\partial h_k}
\right\|
\le
(\beta_W \beta_h)^{t-k}
\]</div>
<p>Gradient clipping:</p>
<div class="arithmatex">\[
\text{if } \|g\| \ge \text{threshold},
\quad
g \leftarrow
\frac{\text{threshold}}{\|g\|} \cdot g
\]</div>
<p>Solutions to vanishing gradients:</p>
<ul>
<li>Identity initialization for <span class="arithmatex">\(W_{hh}\)</span></li>
<li>Use ReLU instead of sigmoid or tanh</li>
</ul>
<p>While RNNs provide a principled framework for sequence modeling and variable-length context, their training difficulties and limited parallelism motivated the development of gated recurrent architectures and attention-based models, which we study next.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../3_nn/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 3. Window-based Neural Language Models">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                3. Window-based Neural Language Models
              </div>
            </div>
          </a>
        
        
          
          <a href="../5_lstm/" class="md-footer__link md-footer__link--next" aria-label="Next: 5. Long Short-Term Memory (LSTM) Networks">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                5. Long Short-Term Memory (LSTM) Networks
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/reinforcement_learning" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>