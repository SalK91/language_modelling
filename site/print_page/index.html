
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Structured lecture notes on Natural Language Processing.">
      
      
        <meta name="author" content="Salman Khan">
      
      
        <link rel="canonical" href="https://salk91.github.io/language_modelling/print_page/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Print/PDF - NLP Lecture Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Merriweather";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
      <link rel="stylesheet" href="../styles/extra.css">
    
    <script>__md_scope=new URL("/language_modelling/",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#index" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="NLP Lecture Notes" class="md-header__button md-logo" aria-label="NLP Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            NLP Lecture Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print/PDF
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../nlp/1_intro/" class="md-tabs__link">
          
  
  
    
  
  Natural Language Processing

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Print/PDF

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="NLP Lecture Notes" class="md-nav__button md-logo" aria-label="NLP Lecture Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    NLP Lecture Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SalK91/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Natural Language Processing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/1_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. NLP an introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/2_lmnp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Language Models and Word Sequence Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/3_nn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Window-based Neural Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/4_rnn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Recurrent Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/5_lstm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Long Short-Term Memory (LSTM) Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/6_app_rnn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Applications of RNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/7_eval_nlp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Evaluation of Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/8_attention_seq2seq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Attention Mechanism in Sequence-to-Sequence Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/9_selfattention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Self- Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/10_archi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    10. Transformer Architectures
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/11_tokens/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Tokenization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/12_pretraining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Pre-training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/13_pretrain_strat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    13. Pre-training Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/14_finetuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    14. Fine-tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/15_cot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    15. Chain of Thought Reasoning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/16_nlg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    16. Nature Language Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/17_imp_nlg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    17. Improving Generation and Training for NLG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/18_eval_nlu/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    18. Evaluation of NLU Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/19_eval_nlg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    19. Evaluation of NLG Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/20_post_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    20. Post-training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/21_advanced_topics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    21. Advanced Topics in Language Modelling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/22_llm_training_basics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    22. LLM Training Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nlp/23_reasoning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    23. Reasoning in LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Print/PDF
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1 Home
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-2" class="md-nav__link">
    <span class="md-ellipsis">
      2 Natural Language Processing
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Natural Language Processing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nlp-1_intro" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 1. NLP an introduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-2_lmnp" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 2. Language Models and Word Sequence Probability
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-3_nn" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 3. Window-based Neural Language Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-4_rnn" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 4. Recurrent Neural Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-5_lstm" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 5. Long Short-Term Memory (LSTM) Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-6_app_rnn" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 6. Applications of RNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-7_eval_nlp" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 7. Evaluation of Language Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-8_attention_seq2seq" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 8. Attention Mechanism in Sequence-to-Sequence Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-9_selfattention" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 9. Self- Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-10_archi" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 10. Transformer Architectures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-11_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 11. Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-12_pretraining" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 12. Pre-training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-13_pretrain_strat" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 13. Pre-training Strategies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-14_finetuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 14. Fine-tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-15_cot" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 15. Chain of Thought Reasoning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-16_nlg" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 16. Nature Language Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-17_imp_nlg" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 17. Improving Generation and Training for NLG
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-18_eval_nlu" class="md-nav__link">
    <span class="md-ellipsis">
      2.18 18. Evaluation of NLU Systems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-19_eval_nlg" class="md-nav__link">
    <span class="md-ellipsis">
      2.19 19. Evaluation of NLG Systems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-20_post_training" class="md-nav__link">
    <span class="md-ellipsis">
      2.20 20. Post-training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-21_advanced_topics" class="md-nav__link">
    <span class="md-ellipsis">
      2.21 21. Advanced Topics in Language Modelling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-22_llm_training_basics" class="md-nav__link">
    <span class="md-ellipsis">
      2.22 22. LLM Training Basics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-23_reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      2.23 23. Reasoning in LLMs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#index" class="md-nav__link">
    <span class="md-ellipsis">
      1 Home
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-2" class="md-nav__link">
    <span class="md-ellipsis">
      2 Natural Language Processing
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 Natural Language Processing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nlp-1_intro" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 1. NLP an introduction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-2_lmnp" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 2. Language Models and Word Sequence Probability
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-3_nn" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 3. Window-based Neural Language Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-4_rnn" class="md-nav__link">
    <span class="md-ellipsis">
      2.4 4. Recurrent Neural Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-5_lstm" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 5. Long Short-Term Memory (LSTM) Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-6_app_rnn" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 6. Applications of RNN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-7_eval_nlp" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 7. Evaluation of Language Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-8_attention_seq2seq" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 8. Attention Mechanism in Sequence-to-Sequence Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-9_selfattention" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 9. Self- Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-10_archi" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 10. Transformer Architectures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-11_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 11. Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-12_pretraining" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 12. Pre-training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-13_pretrain_strat" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 13. Pre-training Strategies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-14_finetuning" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 14. Fine-tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-15_cot" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 15. Chain of Thought Reasoning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-16_nlg" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 16. Nature Language Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-17_imp_nlg" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 17. Improving Generation and Training for NLG
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-18_eval_nlu" class="md-nav__link">
    <span class="md-ellipsis">
      2.18 18. Evaluation of NLU Systems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-19_eval_nlg" class="md-nav__link">
    <span class="md-ellipsis">
      2.19 19. Evaluation of NLG Systems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-20_post_training" class="md-nav__link">
    <span class="md-ellipsis">
      2.20 20. Post-training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-21_advanced_topics" class="md-nav__link">
    <span class="md-ellipsis">
      2.21 21. Advanced Topics in Language Modelling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-22_llm_training_basics" class="md-nav__link">
    <span class="md-ellipsis">
      2.22 22. LLM Training Basics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nlp-23_reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      2.23 23. Reasoning in LLMs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index" heading-number="1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="lectures-on-language-modelling">Lectures on Language Modelling<a class="headerlink" href="#index-lectures-on-language-modelling" title="Permanent link">¶</a></h1>
<p>Welcome to <em>Lectures on RLanguage Modelling</em>, a structured set of lecture notes designed to build the mathematical foundations required to understand, analyze, and develop modern language models.</p>
<p>These notes are inspired by and draw heavily on material from:</p>
<ul>
<li>Stanford CS224N: NLP with Deep Learning (Spring 2024)</li>
<li>Stanford CME295: Transformers &amp; LLMs (Autumn 2025) </li>
</ul>
<p>The goal is not to reproduce these courses, but to synthesize their core ideas into a coherent, optimization and mathematics first perspective suitable for practitioners and researchers.</p>
<h2 id="index-resume-from">Resume from:<a class="headerlink" href="#index-resume-from" title="Permanent link">¶</a></h2>
<p>https://cme295.stanford.edu/syllabus/</p>
<p>https://www.youtube.com/watch?v=PmW_TMQ3l0I&amp;t=17s</p>
<p>https://cs329a.stanford.edu/
https://cseweb.ucsd.edu/~yiying/cse291a-fall25/reading/
https://rdi.berkeley.edu/agentic-ai/f25</p></body></html></section>
                    <section class='print-page md-section' id='section-2' heading-number='2'>
                        <h1>Natural Language Processing<a class='headerlink' href='#section-2' title='Permanent link'></a>
                        </h1>
                    <section class="print-page" id="nlp-1_intro" heading-number="2.1"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-1-nlp-an-introduction">Chapter 1: NLP an introduction<a class="headerlink" href="#nlp-1_intro-chapter-1-nlp-an-introduction" title="Permanent link">¶</a></h1>
<p>Natural language processing (NLP) focuses on enabling machines to understand and work with human language. </p>
<p>At a high level, NLP systems follow a common pipeline: text is first represented numerically, then modeled statistically or with neural networks, and finally evaluated on language tasks. This chapter focuses on the first and most fundamental step—how language is represented.</p>
<h2 id="nlp-1_intro-word-representations">Word Representations<a class="headerlink" href="#nlp-1_intro-word-representations" title="Permanent link">¶</a></h2>
<p>Early NLP systems relied on fixed, count-based representations. While simple and interpretable, these approaches struggle to capture deeper semantic relationships, motivating the move toward learned representations.</p>
<ul>
<li>
<p>One-hot encoding: Represents each word as a sparse vector with a single 1. Fails to capture semantic similarity or relationships between words.</p>
</li>
<li>
<p>n-grams: Represent text as sequences of <span class="arithmatex">\(n\)</span> consecutive words or characters (e.g., bigrams, trigrams). Capture limited context but increase sparsity and dimensionality with higher <span class="arithmatex">\(n\)</span>.</p>
</li>
<li>
<p>Bag of Words (BoW): Represents documents by word frequency vectors. Simple and effective, but ignores word order and context.</p>
</li>
<li>
<p>TF-IDF (Term Frequency – Inverse Document Frequency): Adjusts raw word counts by penalizing frequent words and highlighting informative ones.</p>
</li>
<li>TF (term frequency): Measures how often a word appears in a document.</li>
<li>IDF (inverse document frequency): Reduces the weight of common words across documents.</li>
<li>
<p>TF-IDF score for term <span class="arithmatex">\(t\)</span> in document <span class="arithmatex">\(d\)</span>:
    <script type="math/tex; mode=display">
    \text{TF-IDF}(t, d) = \text{TF}(t, d) \cdot \log\left(\frac{N}{\text{DF}(t)}\right)
    </script>
    where <span class="arithmatex">\(N\)</span> is the total number of documents and <span class="arithmatex">\(\text{DF}(t)\)</span> is the number of documents containing <span class="arithmatex">\(t\)</span>.</p>
</li>
<li>
<p>Co-occurrence Matrix: Counts how often words appear near each other within a context window. Captures word associations but leads to large, sparse matrices.</p>
</li>
<li>
<p>Latent Semantic Analysis (LSA): Applies Singular Value Decomposition (SVD) to the co-occurrence matrix to uncover latent semantic dimensions and reduce dimensionality.</p>
</li>
</ul>
<h2 id="nlp-1_intro-word2vec-learning-word-representations">Word2Vec: Learning Word Representations<a class="headerlink" href="#nlp-1_intro-word2vec-learning-word-representations" title="Permanent link">¶</a></h2>
<p>Word2Vec was one of the first widely successful methods to demonstrate that simple neural objectives, trained at scale, can produce rich semantic representations without explicit supervision.</p>
<p>Goal: Learn word embeddings such that words appearing in similar contexts have similar vector representations.</p>
<ul>
<li>Input: Large corpus of text, fixed vocabulary of size <span class="arithmatex">\(|V|\)</span>.</li>
<li>Output: Two embedding matrices:</li>
<li>Input (center) word matrix <span class="arithmatex">\(W \in \mathbb{R}^{d \times |V|}\)</span></li>
<li>Output (context) word matrix <span class="arithmatex">\(W' \in \mathbb{R}^{d \times |V|}\)</span></li>
<li>Two main training architectures:</li>
</ul>
<h3 id="nlp-1_intro-1-skip-gram-model">1. Skip-gram Model<a class="headerlink" href="#nlp-1_intro-1-skip-gram-model" title="Permanent link">¶</a></h3>
<ul>
<li>Predict context words <span class="arithmatex">\(w_{t+j}\)</span> given a center word <span class="arithmatex">\(w_t\)</span>, for <span class="arithmatex">\(-m \le j \le m,\ j \neq 0\)</span>.</li>
<li>Maximize conditional likelihood:
  <script type="math/tex; mode=display">
  \prod_{t=1}^T \prod_{-m \le j \le m,\ j \neq 0} P(w_{t+j} \mid w_t)
  </script>
</li>
<li>Softmax formulation:
  <script type="math/tex; mode=display">
  P(o \mid c) = \frac{\exp(\mathbf{v}_o^\top \mathbf{v}_c)}{\sum_{w \in V} \exp(\mathbf{v}_w^\top \mathbf{v}_c)}
  </script>
</li>
<li>Challenge: Softmax denominator scales with vocabulary size <span class="arithmatex">\(|V|\)</span>, making training expensive.</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/Skipgram.png" data-desc-position="bottom"><img alt="Skip-Gram" src="../nlp/images/Skipgram.png"></a></p>
<h3 id="nlp-1_intro-2-cbow-continuous-bag-of-words">2. CBOW (Continuous Bag of Words)<a class="headerlink" href="#nlp-1_intro-2-cbow-continuous-bag-of-words" title="Permanent link">¶</a></h3>
<ul>
<li>Predict the center word <span class="arithmatex">\(w_t\)</span> from its surrounding context words.</li>
<li>Average context embeddings:
  <script type="math/tex; mode=display">
  \hat{\mathbf{v}} = \frac{1}{2m} \sum_{-m \le j \le m,\ j \neq 0} \mathbf{v}_{w_{t+j}}
  </script>
</li>
<li>Predict <span class="arithmatex">\(w_t\)</span> using softmax:
  <script type="math/tex; mode=display">
  P(w_t \mid \text{context}) = \frac{\exp(\mathbf{v}_{w_t}^\top \hat{\mathbf{v}})}{\sum_{w \in V} \exp(\mathbf{v}_w^\top \hat{\mathbf{v}})}
  </script>
</li>
<li>Typically faster to train than Skip-gram, and better for frequent words.</li>
</ul>
<p>Together, Skip-gram and CBOW illustrate two complementary perspectives: predicting context from a word versus predicting a word from its context.</p>
<h2 id="nlp-1_intro-glove-global-vectors-for-word-representation">GloVe: Global Vectors for Word Representation<a class="headerlink" href="#nlp-1_intro-glove-global-vectors-for-word-representation" title="Permanent link">¶</a></h2>
<p>Unlike Word2Vec, which learns embeddings implicitly through prediction tasks, GloVe makes the statistical structure of language explicit through global co-occurrence counts.</p>
<p>Goal: Capture the meaning of words using statistics of word co-occurrence across the entire corpus.</p>
<ul>
<li>While Word2Vec uses local context windows, GloVe (Global Vectors) constructs a global co-occurrence matrix, where <span class="arithmatex">\(X_{ij}\)</span> is the number of times word <span class="arithmatex">\(j\)</span> appears near word <span class="arithmatex">\(i\)</span>.</li>
<li>GloVe seeks to find word vectors <span class="arithmatex">\(\mathbf{w}_i\)</span>, <span class="arithmatex">\(\tilde{\mathbf{w}}_j\)</span> such that:
  <script type="math/tex; mode=display">
  \mathbf{w}_i^\top \tilde{\mathbf{w}}_j + b_i + \tilde{b}_j = \log(X_{ij})
  </script>
</li>
<li>The underlying insight: ratios of co-occurrence probabilities encode meaning. For example, consider how a target word <span class="arithmatex">\(x\)</span> relates to two context words <span class="arithmatex">\(a\)</span> and <span class="arithmatex">\(b\)</span>:
  <script type="math/tex; mode=display">
  \frac{P(x \mid a)}{P(x \mid b)} \quad \text{can be modeled as} \quad \mathbf{w}_x^\top (\mathbf{w}_a - \mathbf{w}_b)
  </script>
  This shows that linear differences between word vectors reflect meaningful relationships (e.g., <span class="arithmatex">\(\texttt{king} - \texttt{man} + \texttt{woman} \approx \texttt{queen}\)</span>).</li>
<li>To learn these representations, GloVe minimizes a weighted least squares loss:
  <script type="math/tex; mode=display">
  J = \sum_{i,j=1}^{|V|} f(X_{ij}) \left( \mathbf{w}_i^\top \tilde{\mathbf{w}}_j + b_i + \tilde{b}_j - \log(X_{ij}) \right)^2
  </script>
</li>
<li>The weighting function <span class="arithmatex">\(f(X_{ij})\)</span> ensures that very frequent co-occurrences do not dominate the loss, and very rare ones do not introduce noise. A typical choice:
  <script type="math/tex; mode=display">
  f(X_{ij}) =
  \begin{cases}
  \left(\frac{X_{ij}}{x_{\text{max}}}\right)^\alpha & \text{if } X_{ij} < x_{\text{max}} \\
  1 & \text{otherwise}
  \end{cases}
  </script>
  where <span class="arithmatex">\(\alpha \in [0,1]\)</span>, e.g., <span class="arithmatex">\(\alpha = 0.75\)</span>.</li>
<li>Interpretation: GloVe combines the global statistical strength of matrix factorization (like LSA) with the expressive power of vector differences. This allows it to learn embeddings that perform well on tasks like analogy completion, clustering, and semantic similarity.</li>
</ul>
<p>Both Word2Vec and GloVe produce static word embeddings, where each word has a single vector regardless of context. While powerful, this assumption will later be revisited with contextualized representations.</p>
<h2 id="nlp-1_intro-evaluating-word-vectors-intrinsic-vs-extrinsic">Evaluating Word Vectors: Intrinsic vs. Extrinsic<a class="headerlink" href="#nlp-1_intro-evaluating-word-vectors-intrinsic-vs-extrinsic" title="Permanent link">¶</a></h2>
<p>Because embeddings are intermediate representations rather than end goals, evaluating their quality requires careful consideration of what “good” representations mean in practice.</p>
<ul>
<li>Evaluation of word vectors can be divided into intrinsic and extrinsic methods.</li>
</ul>
<h3 id="nlp-1_intro-intrinsic-evaluation">Intrinsic Evaluation<a class="headerlink" href="#nlp-1_intro-intrinsic-evaluation" title="Permanent link">¶</a></h3>
<ul>
<li>Tests word vectors on intermediate, well-defined subtasks.</li>
<li>Fast to compute and useful for understanding specific aspects of embeddings.</li>
<li>However, it is not always clear if performance on intrinsic tasks translates to improvements in real-world tasks.</li>
<li>Example: Compute cosine similarity between word pairs and compare with human similarity ratings.</li>
</ul>
<h3 id="nlp-1_intro-extrinsic-evaluation">Extrinsic Evaluation<a class="headerlink" href="#nlp-1_intro-extrinsic-evaluation" title="Permanent link">¶</a></h3>
<ul>
<li>Measures performance on a real downstream NLP task.</li>
<li>Slower and more expensive to compute but provides more practical insight.</li>
<li>Hard to isolate whether improvements are due to the word vectors or other parts of the system.</li>
<li>Example: Use word embeddings in a Named Entity Recognition (NER) system and evaluate changes in accuracy.</li>
<li>If replacing one embedding model with another improves end-task performance, the new model is considered better.</li>
</ul>
<p>This chapter establishes the foundational idea that representation learning lies at the heart of modern NLP, setting the stage for deeper neural architectures and contextual language models explored in later chapters.</p></body></html></section><section class="print-page" id="nlp-2_lmnp" heading-number="2.2"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-2-language-models-and-word-sequence-probability">Chapter 2: Language Models and Word Sequence Probability<a class="headerlink" href="#nlp-2_lmnp-chapter-2-language-models-and-word-sequence-probability" title="Permanent link">¶</a></h1>
<p>Language models formalize the idea that natural language is not random, but follows statistical regularities. By assigning probabilities to word sequences, they provide a principled way to reason about which sentences are more likely than others.</p>
<h3 id="nlp-2_lmnp-purpose-of-language-models">Purpose of Language Models<a class="headerlink" href="#nlp-2_lmnp-purpose-of-language-models" title="Permanent link">¶</a></h3>
<ul>
<li>A language model computes the probability of a sequence of words occurring.</li>
<li>Useful in applications like machine translation and speech recognition.</li>
</ul>
<p>At their core, language models answer a simple question: given what we have seen so far, what word is likely to come next?</p>
<h3 id="nlp-2_lmnp-probability-of-word-sequences">Probability of Word Sequences<a class="headerlink" href="#nlp-2_lmnp-probability-of-word-sequences" title="Permanent link">¶</a></h3>
<ul>
<li>The probability of a word sequence <span class="arithmatex">\(\{w_1, w_2, \ldots, w_m\}\)</span> is:
  <script type="math/tex; mode=display">
  P(w_1, \ldots, w_m) = \prod_{i=1}^{m} P(w_i \mid w_1, \ldots, w_{i-1})
  </script>
</li>
<li>Approximated using only the previous <span class="arithmatex">\(n\)</span> words (n-gram model):
  <script type="math/tex; mode=display">
  P(w_1, \ldots, w_m) \approx \prod_{i=1}^{m} P(w_i \mid w_{i-n}, \ldots, w_{i-1})
  </script>
</li>
</ul>
<p>This approximation reflects a practical trade-off: modeling long-range dependencies is difficult with limited data and computation.</p>
<h3 id="nlp-2_lmnp-importance-in-machine-translation">Importance in Machine Translation<a class="headerlink" href="#nlp-2_lmnp-importance-in-machine-translation" title="Permanent link">¶</a></h3>
<ul>
<li>Systems generate multiple candidate translations.</li>
<li>Examples: {I have, I had, I has, me have, me had}</li>
<li>Each candidate sequence is scored using a probability function.</li>
<li>The system selects the one with the highest score.</li>
</ul>
<p>In this setting, the language model acts as a fluency filter, favoring grammatically and statistically plausible sentences.</p>
<h2 id="nlp-2_lmnp-n-gram-language-models">N-gram Language Models<a class="headerlink" href="#nlp-2_lmnp-n-gram-language-models" title="Permanent link">¶</a></h2>
<p>To compute word sequence probabilities, we can use the frequency of <span class="arithmatex">\(n\)</span>-grams compared to the frequency of shorter sequences (e.g., uni-grams). This is the foundation of the n-gram language model.</p>
<p>These models rely purely on observed counts and make no use of semantic representations.</p>
<ul>
<li>Bigram model:
  <script type="math/tex; mode=display">
  P(w_2 \mid w_1) = \frac{\text{count}(w_1, w_2)}{\text{count}(w_1)}
  </script>
</li>
<li>Trigram model:
  <script type="math/tex; mode=display">
  P(w_3 \mid w_1, w_2) = \frac{\text{count}(w_1, w_2, w_3)}{\text{count}(w_1, w_2)}
  </script>
</li>
</ul>
<p>These models use a fixed-size window (e.g., previous <span class="arithmatex">\(n\)</span> words) to predict the next word. However, choosing the right window size is critical. For example:</p>
<ul>
<li>In the sentence: “As the proctor started the clock, the students opened their...”</li>
<li>A 3-word context window (“the students opened their”) may predict “books.”</li>
<li>A longer window capturing “proctor” may increase the likelihood of predicting “exam.”</li>
</ul>
<p>This illustrates how limited context can restrict the model’s ability to capture meaning.</p>
<h3 id="nlp-2_lmnp-challenges-with-n-gram-models">Challenges with N-gram Models<a class="headerlink" href="#nlp-2_lmnp-challenges-with-n-gram-models" title="Permanent link">¶</a></h3>
<h4 id="nlp-2_lmnp-1-sparsity">1. Sparsity<a class="headerlink" href="#nlp-2_lmnp-1-sparsity" title="Permanent link">¶</a></h4>
<ul>
<li>If a specific <span class="arithmatex">\(n\)</span>-gram (e.g., <span class="arithmatex">\((w_1, w_2, w_3)\)</span>) never appears, the model assigns a zero probability.</li>
<li>Smoothing: Add a small value <span class="arithmatex">\(\delta\)</span> to all counts to avoid zero probabilities.</li>
<li>Backoff: If the context (e.g., <span class="arithmatex">\(w_1, w_2\)</span>) is missing, fall back to a shorter context (e.g., <span class="arithmatex">\(w_2\)</span> alone).</li>
<li>Increasing <span class="arithmatex">\(n\)</span> makes sparsity worse, which is why typically <span class="arithmatex">\(n \leq 5\)</span>.</li>
</ul>
<p>Sparsity fundamentally limits how much context classical count-based models can exploit.</p>
<h4 id="nlp-2_lmnp-2-storage">2. Storage<a class="headerlink" href="#nlp-2_lmnp-2-storage" title="Permanent link">¶</a></h4>
<ul>
<li>All <span class="arithmatex">\(n\)</span>-gram counts must be stored.</li>
<li>Larger <span class="arithmatex">\(n\)</span> and larger corpora increase model size significantly.</li>
</ul>
<p>Together, sparsity and storage constraints motivate the transition from count-based language models to neural language models, which replace explicit counting with learned distributed representations.</p></body></html></section><section class="print-page" id="nlp-3_nn" heading-number="2.3"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-3-window-based-neural-language-model">Chapter 3: Window-based Neural Language Model<a class="headerlink" href="#nlp-3_nn-chapter-3-window-based-neural-language-model" title="Permanent link">¶</a></h1>
<p>To address sparsity and storage issues, Bengio et al. proposed a Neural Probabilistic Language Model, the first large-scale deep learning model for NLP.</p>
<ul>
<li>Learns distributed representations (embeddings) for words  </li>
<li>Uses a neural network to compute probabilities instead of raw counts  </li>
</ul>
<p>Proposed in 2003, this model marked a shift from purely count-based language models to neural approaches that learn representations jointly with the language model.</p>
<h3 id="nlp-3_nn-neural-architecture-simplified">Neural Architecture (Simplified)<a class="headerlink" href="#nlp-3_nn-neural-architecture-simplified" title="Permanent link">¶</a></h3>
<p>Given a fixed-length context window, the model estimates the conditional probability of the next word:
<script type="math/tex; mode=display">
P(w_t \mid w_{t-1}, \dots, w_{t-n})
</script>
</p>
<ul>
<li>
<p>Input: Concatenated word embeddings<br>
<script type="math/tex; mode=display">
  \mathbf{e} = [e^{(1)}; e^{(2)}; e^{(3)}; e^{(4)}]
  </script>
</p>
</li>
<li>
<p>Hidden layer:<br>
<script type="math/tex; mode=display">
  \mathbf{h} = f(W \mathbf{e} + \mathbf{b}_1)
  </script>
</p>
</li>
<li>
<p>Output (softmax over vocabulary):<br>
<script type="math/tex; mode=display">
  \hat{y} = \text{softmax}(U \mathbf{h} + \mathbf{b}_2)
  </script>
</p>
</li>
<li>
<p>Learns distributed representations (embeddings) for words that capture syntactic and semantic similarity.</p>
</li>
</ul>
<h3 id="nlp-3_nn-full-model-equation">Full Model Equation<a class="headerlink" href="#nlp-3_nn-full-model-equation" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
\hat{y} = \text{softmax}\!\left(
W^{(2)} \tanh(W^{(1)} \mathbf{x} + \mathbf{b}^{(1)})
+ W^{(3)} \mathbf{x} + \mathbf{b}^{(3)}
\right)
\]</div>
<ul>
<li><span class="arithmatex">\(W^{(1)}\)</span> applied to word vectors (input <span class="arithmatex">\(\rightarrow\)</span> hidden layer)  </li>
<li><span class="arithmatex">\(W^{(2)}\)</span> applied to hidden layer (hidden <span class="arithmatex">\(\rightarrow\)</span> output)  </li>
<li><span class="arithmatex">\(W^{(3)}\)</span> connects input directly to output (shortcut connection)  </li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/fix_window_nn.png" data-desc-position="bottom"><img alt="Window-based Neural Language Model" src="../nlp/images/fix_window_nn.png"></a></p>
<h3 id="nlp-3_nn-limitations-of-the-window-based-model">Limitations of the Window-based Model<a class="headerlink" href="#nlp-3_nn-limitations-of-the-window-based-model" title="Permanent link">¶</a></h3>
<p>While the window-based neural language model marked a significant advancement, it still suffers from key limitations:</p>
<ul>
<li>
<p>Fixed context size (Markov assumption):<br>
  The model relies on a fixed-size context window (e.g., 4 or 5 previous words), enforcing a strong Markov assumption. This limits the model’s ability to capture long-range dependencies and contextual information beyond the window. As a result, important linguistic patterns that span wider contexts may be missed.</p>
</li>
<li>
<p>Position-specific parameterization:<br>
  Each word in the context window is embedded and concatenated in a fixed order, meaning that words at different positions are treated differently by the model. This prevents the model from generalizing across positions; for example, the same word appearing in position 1 versus position 4 will be processed differently due to distinct weights applied in the hidden layer.</p>
</li>
<li>
<p>Lack of permutation invariance and poor scalability:<br>
  Since the architecture depends on fixed positions and fully connected layers, it does not scale well with longer contexts and lacks flexibility for variable-length input. It also cannot handle unseen word orders or reorderings effectively.</p>
</li>
</ul>
<p>These limitations were among the motivations for later architectures such as recurrent neural networks (RNNs) and transformers, which can model variable-length contexts and better capture sequential dependencies.</p></body></html></section><section class="print-page" id="nlp-4_rnn" heading-number="2.4"><h1 id="nlp-4_rnn-nlp-4_rnn">4. Recurrent Neural Networks</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="chapter-4-recurrent-neural-networks">Chapter 4: Recurrent Neural Networks<a class="headerlink" href="#nlp-4_rnn-chapter-4-recurrent-neural-networks" title="Permanent link">¶</a></h2>
<p>Unlike traditional language models that condition on a fixed-size window of previous tokens, RNNs are capable of conditioning on the entire history of previous tokens.</p>
<p>This is achieved by maintaining a recurrent hidden state that summarizes past information and is updated at every time-step.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/rnn_3step.png" data-desc-position="bottom"><img alt="A Recurrent Neural Network" src="../nlp/images/rnn_3step.png"></a></p>
<h2 id="nlp-4_rnn-rnn-architecture">RNN Architecture<a class="headerlink" href="#nlp-4_rnn-rnn-architecture" title="Permanent link">¶</a></h2>
<p>The RNN architecture (Figure above) shows three time-steps. Each hidden layer at time-step <span class="arithmatex">\(t\)</span> receives two inputs:</p>
<ul>
<li>The input vector at time <span class="arithmatex">\(t\)</span>, <span class="arithmatex">\(x_t\)</span></li>
<li>The hidden state from the previous time-step, <span class="arithmatex">\(h_{t-1}\)</span></li>
</ul>
<p>These are combined using weights <span class="arithmatex">\(W_{hh}\)</span> and <span class="arithmatex">\(W_{hx}\)</span>:</p>
<div class="arithmatex">\[
h_t = \sigma(W_{hh} h_{t-1} + W_{hx} x_t + b_1)
\]</div>
<p>The hidden state <span class="arithmatex">\(h_t\)</span> acts as a continuous memory vector that compresses information from all previous inputs <span class="arithmatex">\(x_1, \dots, x_t\)</span>.</p>
<p>The output is calculated as:</p>
<div class="arithmatex">\[
\hat{y}_t = \text{softmax}(W_S h_t + b_2)
\]</div>
<p>Key parameters:</p>
<ul>
<li><span class="arithmatex">\(x_t \in \mathbb{R}^d\)</span>: Input word vector  </li>
<li><span class="arithmatex">\(W_{hx} \in \mathbb{R}^{D_h \times d}\)</span>: Input weight matrix  </li>
<li><span class="arithmatex">\(W_{hh} \in \mathbb{R}^{D_h \times D_h}\)</span>: Hidden state weight matrix  </li>
<li><span class="arithmatex">\(W_S \in \mathbb{R}^{|V| \times D_h}\)</span>: Output weight matrix  </li>
<li><span class="arithmatex">\(\sigma\)</span>: Non-linear function such as tanh  </li>
<li><span class="arithmatex">\(\hat{y}_t \in \mathbb{R}^{|V|}\)</span>: Probability distribution over vocabulary  </li>
</ul>
<h2 id="nlp-4_rnn-training-an-rnn-language-model">Training an RNN Language Model<a class="headerlink" href="#nlp-4_rnn-training-an-rnn-language-model" title="Permanent link">¶</a></h2>
<p>To train a Recurrent Neural Network Language Model (RNN-LM), we begin with a large corpus of text, represented as a sequence of word tokens. The model is trained to predict the next word at each time-step, given all preceding words in the sequence.</p>
<p>At each time-step <span class="arithmatex">\(t\)</span>:</p>
<ul>
<li>The model receives an input vector <span class="arithmatex">\(x_t\)</span> and computes a hidden state <span class="arithmatex">\(h_t\)</span> using the current input and the previous hidden state <span class="arithmatex">\(h_{t-1}\)</span>.</li>
<li>The output layer produces a probability distribution <span class="arithmatex">\(\hat{y}_t\)</span> over the vocabulary.</li>
<li>The cross-entropy loss is computed between <span class="arithmatex">\(\hat{y}_t\)</span> and the ground-truth one-hot vector <span class="arithmatex">\(y_t\)</span>.</li>
</ul>
<p>Training an RNN language model corresponds to maximizing the log-likelihood of the observed sequence:</p>
<div class="arithmatex">\[\log P(x_1, \ldots, x_T)
= \sum_{t=1}^{T} \log P\!\left(x_{t+1} \mid x_1, \ldots, x_t\right)\]</div>
<p>The total training objective is to minimize the average cross-entropy loss across all time-steps and sequences in the training set.</p>
<div class="arithmatex">\[
J^{(t)}(\theta) = -\sum_{j=1}^{|V|} y_{t,j} \log \hat{y}_{t,j}
\]</div>
<h3 id="nlp-4_rnn-teacher-forcing">Teacher Forcing<a class="headerlink" href="#nlp-4_rnn-teacher-forcing" title="Permanent link">¶</a></h3>
<p>During training, a technique called teacher forcing is commonly used. Instead of feeding the model’s own prediction <span class="arithmatex">\(\hat{y}_{t-1}\)</span> as input at the next time-step, we use the ground-truth word <span class="arithmatex">\(y_{t-1}\)</span>. This stabilizes and accelerates learning by preventing the model from compounding its own mistakes during early training. However, it introduces a discrepancy between training and inference, known as exposure bias, since the model must rely on its own predictions at test time. Exposure bias becomes more pronounced for long sequences, where early prediction errors can cascade through the remainder of the sequence.</p>
<h3 id="nlp-4_rnn-backpropagation-through-time">Backpropagation Through Time<a class="headerlink" href="#nlp-4_rnn-backpropagation-through-time" title="Permanent link">¶</a></h3>
<p>Training an RNN requires computing gradients over sequences of time-dependent operations. This is done using Backpropagation Through Time (BPTT):</p>
<ul>
<li>The RNN is unrolled over <span class="arithmatex">\(T\)</span> time-steps, creating a computational graph where the same parameters are shared at each step.</li>
<li>Gradients are computed by propagating errors backward through time from <span class="arithmatex">\(t = T\)</span> to <span class="arithmatex">\(t = 0\)</span>, summing contributions from all steps.</li>
<li>This process allows the model to adjust its parameters based on dependencies that span multiple time-steps.</li>
</ul>
<p>In practice, full BPTT over long sequences can be computationally expensive and unstable. Therefore, a variant called truncated BPTT is often used, where backpropagation is limited to a fixed number of time-steps (e.g., 20 or 50). This trades off full temporal context for efficiency and stability. Truncated BPTT implicitly limits the temporal horizon over which dependencies can be learned, acting as a practical approximation to full sequence optimization.</p>
<p>BPTT is sensitive to gradient vanishing and exploding problems, especially with non-linear activations like tanh or sigmoid. Techniques such as gradient clipping and using gated architectures like LSTMs or GRUs are commonly employed to address these issues.</p>
<h3 id="nlp-4_rnn-efficient-training-with-mini-batches">Efficient Training with Mini-Batches<a class="headerlink" href="#nlp-4_rnn-efficient-training-with-mini-batches" title="Permanent link">¶</a></h3>
<p>Computing the loss and gradients over the entire corpus simultaneously is impractical due to memory limitations. Instead, training is performed on smaller units using mini-batches:</p>
<ul>
<li>The text corpus is divided into sequences such as sentences or fixed-length token chunks.</li>
<li>A batch of these sequences is processed together to compute the average loss and gradients.</li>
<li>Stochastic Gradient Descent (SGD) or its variants (e.g., Adam) are used to update parameters based on each batch.</li>
</ul>
<p>Handling variable-length sequences within batches requires padding and masking:</p>
<ul>
<li>Shorter sequences are padded with special tokens to match the longest sequence in the batch.</li>
<li>Padding positions are masked to prevent them from contributing to the loss or gradient updates.</li>
</ul>
<p>This mini-batch training paradigm enables scalable computation and leverages hardware acceleration such as GPUs and TPUs.</p>
<h3 id="nlp-4_rnn-training-vs-inference">Training vs. Inference<a class="headerlink" href="#nlp-4_rnn-training-vs-inference" title="Permanent link">¶</a></h3>
<p>At inference time, teacher forcing is not available. Each predicted word is fed back into the model as input for the next step. This difference in input distribution between training and inference can degrade performance if the model is not robust to its own prediction errors. Techniques such as scheduled sampling or fine-tuning with generated sequences are sometimes used to mitigate this mismatch.</p>
<h3 id="nlp-4_rnn-perplexity">Perplexity<a class="headerlink" href="#nlp-4_rnn-perplexity" title="Permanent link">¶</a></h3>
<ul>
<li>The standard evaluation metric for language models is perplexity.</li>
</ul>
<div class="arithmatex">\[
\text{perplexity} =
\left(
\prod_{t=1}^{T}
\frac{1}{P_{\text{LM}}(x^{(t+1)} \mid x^{(1)}, \ldots, x^{(t)})}
\right)^{\frac{1}{T}}
\]</div>
<ul>
<li>Inverse probability of corpus according to the language model, normalized by the number of words.</li>
<li>This is equal to the exponential of the cross-entropy loss <span class="arithmatex">\(J(\theta)\)</span>:</li>
</ul>
<div class="arithmatex">\[
\left(
\prod_{t=1}^{T}
\frac{1}{\hat{y}^{(t)}_{x_{t+1}}}
\right)^{\frac{1}{T}}
=
\exp\left(
\frac{1}{T}
\sum_{t=1}^{T}
-\log \hat{y}^{(t)}_{x_{t+1}}
\right)
=
\exp(J(\theta))
\]</div>
<p>Perplexity can be viewed as the geometric mean of the inverse predicted probabilities assigned to the true next tokens.</p>
<p>Perplexity can be interpreted as the effective average number of choices the model is considering at each time-step:</p>
<ul>
<li>A perplexity of 1 means the model predicts every word perfectly.</li>
<li>A perplexity of 50 means the model is as uncertain as choosing uniformly from 50 possible words.</li>
</ul>
<p>Lower perplexity indicates better predictive performance and correlates with fluency and accuracy in language generation.</p>
<p>Note: Perplexity is sensitive to vocabulary size and tokenization. Models should be compared under identical preprocessing conditions.</p>
<h2 id="nlp-4_rnn-memory-scaling-in-recurrent-neural-networks">Memory Scaling in Recurrent Neural Networks<a class="headerlink" href="#nlp-4_rnn-memory-scaling-in-recurrent-neural-networks" title="Permanent link">¶</a></h2>
<p>From a machine learning perspective, the memory requirements of RNNs can be divided into model parameters and runtime memory.</p>
<h3 id="nlp-4_rnn-model-parameters-static-memory">Model Parameters (Static Memory)<a class="headerlink" href="#nlp-4_rnn-model-parameters-static-memory" title="Permanent link">¶</a></h3>
<p>These are the weights and biases defining the RNN structure:</p>
<ul>
<li><span class="arithmatex">\(W_{hx} \in \mathbb{R}^{D_h \times d}\)</span></li>
<li><span class="arithmatex">\(W_{hh} \in \mathbb{R}^{D_h \times D_h}\)</span></li>
<li><span class="arithmatex">\(W_S \in \mathbb{R}^{|V| \times D_h}\)</span></li>
<li>Bias vectors for each layer</li>
</ul>
<p>The memory required to store these parameters is constant with respect to corpus size. That is, increasing the number of sentences or tokens in the dataset does not change the size of these matrices.</p>
<h3 id="nlp-4_rnn-runtime-memory-dynamic-per-sequence">Runtime Memory (Dynamic, per sequence)<a class="headerlink" href="#nlp-4_rnn-runtime-memory-dynamic-per-sequence" title="Permanent link">¶</a></h3>
<p>During training with BPTT, the RNN stores:</p>
<ul>
<li>Input vectors <span class="arithmatex">\(x_t\)</span></li>
<li>Hidden states <span class="arithmatex">\(h_t\)</span></li>
<li>Intermediate gradient-related states</li>
</ul>
<p>Runtime memory scales linearly with sequence length <span class="arithmatex">\(T\)</span>. A sentence with <span class="arithmatex">\(T\)</span> words requires storage for <span class="arithmatex">\(T\)</span> hidden states and possibly <span class="arithmatex">\(T\)</span> sets of gradients.</p>
<ul>
<li>Model parameter count depends only on architecture parameters <span class="arithmatex">\(D_h\)</span>, <span class="arithmatex">\(d\)</span>, and <span class="arithmatex">\(|V|\)</span>.</li>
<li>Training memory grows proportionally with input sequence length.</li>
</ul>
<p>Insight: RNNs are more memory-efficient than traditional <span class="arithmatex">\(n\)</span>-gram models in terms of model size, but training on long sequences increases memory usage due to the need to store intermediate activations over time.</p>
<h2 id="nlp-4_rnn-advantages-and-disadvantages">Advantages and Disadvantages<a class="headerlink" href="#nlp-4_rnn-advantages-and-disadvantages" title="Permanent link">¶</a></h2>
<p>Advantages:</p>
<ol>
<li>Can handle variable-length sequences  </li>
<li>Parameter size independent of input length  </li>
<li>Can model long-range dependencies  </li>
<li>Weight sharing across time-steps  </li>
</ol>
<p>Disadvantages:</p>
<ol>
<li>Sequential computation limits parallelization  </li>
<li>Prone to vanishing and exploding gradients  </li>
<li>Difficulty in learning very long-term dependencies in practice</li>
</ol>
<h2 id="nlp-4_rnn-vanishing-and-exploding-gradients">Vanishing and Exploding Gradients<a class="headerlink" href="#nlp-4_rnn-vanishing-and-exploding-gradients" title="Permanent link">¶</a></h2>
<p>The total gradient with respect to parameters is:</p>
<div class="arithmatex">\[
\frac{\partial E}{\partial W}
=
\sum_{t=1}^{T}
\sum_{k=1}^{t}
\frac{\partial E_t}{\partial y_t}
\cdot
\frac{\partial y_t}{\partial h_t}
\cdot
\left(
\prod_{j=k+1}^{t}
\frac{\partial h_j}{\partial h_{j-1}}
\right)
\cdot
\frac{\partial h_k}{\partial W}
\]</div>
<p>Jacobian norm bound:</p>
<div class="arithmatex">\[
\left\|
\frac{\partial h_t}{\partial h_k}
\right\|
\le
(\beta_W \beta_h)^{t-k}
\]</div>
<p>Gradient clipping:</p>
<div class="arithmatex">\[
\text{if } \|g\| \ge \text{threshold},
\quad
g \leftarrow
\frac{\text{threshold}}{\|g\|} \cdot g
\]</div>
<p>Solutions to vanishing gradients:</p>
<ul>
<li>Identity initialization for <span class="arithmatex">\(W_{hh}\)</span></li>
<li>Use ReLU instead of sigmoid or tanh</li>
</ul>
<p>While RNNs provide a principled framework for sequence modeling and variable-length context, their training difficulties and limited parallelism motivated the development of gated recurrent architectures and attention-based models, which we study next.</p></body></html></section><section class="print-page" id="nlp-5_lstm" heading-number="2.5"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-5-long-short-term-memory">Chapter 5: Long Short-term Memory<a class="headerlink" href="#nlp-5_lstm-chapter-5-long-short-term-memory" title="Permanent link">¶</a></h1>
<p>LSTMs were introduced specifically to address the vanishing gradient problem encountered in vanilla RNNs when modeling long sequences. LSTMs are a special kind of RNN capable of learning long-term dependencies. At each time-step <span class="arithmatex">\(t\)</span>, the LSTM maintains a hidden state <span class="arithmatex">\(h^{(t)}\)</span> and a cell state <span class="arithmatex">\(c^{(t)}\)</span>. The key components of an LSTM are the input, forget, and output gates, which regulate the flow of information.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/lstm.png" data-desc-position="bottom"><img alt="LSTM" src="../nlp/images/lstm.png"></a></p>
<h3 id="nlp-5_lstm-gates-and-their-roles">Gates and Their Roles<a class="headerlink" href="#nlp-5_lstm-gates-and-their-roles" title="Permanent link">¶</a></h3>
<p>Each gate is implemented as a sigmoid-activated affine transformation of the previous hidden state and the current input.</p>
<ul>
<li>
<p>Forget Gate: Controls what information from the previous cell state <span class="arithmatex">\(c^{(t-1)}\)</span> should be kept versus forgotten:
  <script type="math/tex; mode=display">
  f^{(t)} = \sigma(W_f h^{(t-1)} + U_f x^{(t)} + b_f)
  </script>
</p>
</li>
<li>
<p>Input Gate: Determines what new information should be stored in the cell state:
  <script type="math/tex; mode=display">
  i^{(t)} = \sigma(W_i h^{(t-1)} + U_i x^{(t)} + b_i)
  </script>
</p>
</li>
<li>
<p>Output Gate: Decides what part of the cell state should be output:
  <script type="math/tex; mode=display">
  o^{(t)} = \sigma(W_o h^{(t-1)} + U_o x^{(t)} + b_o)
  </script>
</p>
</li>
</ul>
<h3 id="nlp-5_lstm-internal-computation">Internal Computation<a class="headerlink" href="#nlp-5_lstm-internal-computation" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>New cell content: this is the new content to be written to the cell:</p>
<div class="arithmatex">\[\tilde{c}^{(t)} = \tanh(W_c h^{(t-1)} + U_c x^{(t)} + b_c)\]</div>
</li>
<li>
<p>Cell state update: erase (forget) some content from the last cell state and write (input) some new cell content:
  <script type="math/tex; mode=display">
  c^{(t)} = f^{(t)} \odot c^{(t-1)} + i^{(t)} \odot \tilde{c}^{(t)}
  </script>
</p>
<p>The cell state therefore evolves through time via controlled addition and deletion of information, rather than complete overwriting.</p>
</li>
<li>
<p>Hidden state output: read (output) some content from the cell:
  <script type="math/tex; mode=display">
  h^{(t)} = o^{(t)} \odot \tanh(c^{(t)})
  </script>
</p>
<p>While the cell state stores long-term information, the hidden state serves as the short-term representation exposed to the rest of the network.</p>
</li>
</ul>
<p>All gate outputs are vectors with values in <span class="arithmatex">\([0, 1]\)</span> using the sigmoid function <span class="arithmatex">\(\sigma\)</span>. The operator <span class="arithmatex">\(\odot\)</span> denotes element-wise (Hadamard) product.</p>
<p>LSTMs mitigate the vanishing gradient problem by enabling the cell state <span class="arithmatex">\(c^{(t)}\)</span> to carry forward important information over long sequences.</p>
<h3 id="nlp-5_lstm-lstm-additive-memory-and-gradient-flow">LSTM: Additive Memory and Gradient Flow<a class="headerlink" href="#nlp-5_lstm-lstm-additive-memory-and-gradient-flow" title="Permanent link">¶</a></h3>
<p>One of the key strengths of LSTMs lies in the additive nature of its cell state update, which is visually emphasized in the diagram by the <span class="arithmatex">\(\oplus\)</span> (plus) operation in the center of the cell.</p>
<h4 id="nlp-5_lstm-why-the-is-the-secret">Why the “+” Is the Secret<a class="headerlink" href="#nlp-5_lstm-why-the-is-the-secret" title="Permanent link">¶</a></h4>
<p>The key architectural difference between LSTMs and vanilla RNNs lies in how information is propagated through time. Traditional RNNs use repeated multiplications when propagating hidden states across time, which can lead to gradients either vanishing (becoming very small) or exploding (becoming very large). This makes learning long-term dependencies extremely difficult.</p>
<p>LSTMs avoid this through the structure of their cell state update:
<script type="math/tex; mode=display">
c^{(t)} = f^{(t)} \odot c^{(t-1)} + i^{(t)} \odot \tilde{c}^{(t)}
</script>
</p>
<p>When the forget gate is close to 1 and the input gate is close to 0, the cell state behaves like an identity mapping across time-steps.</p>
<p>This equation is element-wise additive, rather than multiplicative. The additive interaction allows gradients to flow back through time more stably.</p>
<h4 id="nlp-5_lstm-mitigating-vanishing-gradients">Mitigating Vanishing Gradients<a class="headerlink" href="#nlp-5_lstm-mitigating-vanishing-gradients" title="Permanent link">¶</a></h4>
<ul>
<li>The forget gate <span class="arithmatex">\(f^{(t)}\)</span> controls how much of the previous cell state is retained. When <span class="arithmatex">\(f^{(t)}\)</span> is close to 1, <span class="arithmatex">\(c^{(t-1)}\)</span> is passed forward nearly unchanged.</li>
<li>This allows information and gradients to flow across many time-steps with minimal decay.</li>
<li>Because of the additive pathway through <span class="arithmatex">\(c^{(t)}\)</span>, backpropagation through time (BPTT) can preserve useful gradients over long sequences.</li>
</ul>
<h4 id="nlp-5_lstm-summary">Summary<a class="headerlink" href="#nlp-5_lstm-summary" title="Permanent link">¶</a></h4>
<p>The “<span class="arithmatex">\(\oplus\)</span>” operator (additive memory update) is the core innovation in LSTMs. It allows the model to accumulate and preserve information over long time horizons, effectively addressing the vanishing gradient problem that affects vanilla RNNs.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/lstm_2.png" data-desc-position="bottom"><img alt="LSTM: Additive Memory" src="../nlp/images/lstm_2.png"></a></p>
<p>This design makes LSTMs significantly more effective than vanilla RNNs for tasks involving long-range dependencies, such as language modeling and sequence labeling.</p></body></html></section><section class="print-page" id="nlp-6_app_rnn" heading-number="2.6"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-6-application-of-rnns">Chapter 6: Application of RNNs<a class="headerlink" href="#nlp-6_app_rnn-chapter-6-application-of-rnns" title="Permanent link">¶</a></h1>
<p>Recurrent neural networks are particularly well suited for tasks involving sequential structure, where predictions depend on both current input and past context.</p>
<ol>
<li>Part-of-Speech Tagging and Named Entity Recognition</li>
</ol>
<p>These are sequence labeling tasks where each word in a sentence is assigned a tag. An RNN processes the input word sequence one token at a time and produces an output tag for each time-step. In these tasks, the output sequence has the same length as the input sequence.</p>
<ul>
<li>Input: A sequence of word vectors <span class="arithmatex">\((x_1, x_2, \dots, x_T)\)</span>  </li>
<li>Architecture: A unidirectional RNN (e.g., vanilla RNN, LSTM, or GRU) computes hidden states <span class="arithmatex">\((h_1, h_2, \dots, h_T)\)</span>  </li>
<li>Output: Each <span class="arithmatex">\(h_t\)</span> is passed through a softmax classifier to predict a tag <span class="arithmatex">\(y_t\)</span> for the corresponding word  </li>
</ul>
<p>Bidirectional RNNs are also commonly used in practice, as tag predictions often depend on both left and right context.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/rnn_pos.png" data-desc-position="bottom"><img alt="RNN applied to POS Tagging / NER" src="../nlp/images/rnn_pos.png"></a></p>
<ol>
<li>
<p>Sentiment Analysis
Unlike sequence labeling, sentiment analysis produces a single label for the entire sequence.
This is a sequence classification task where the entire input sequence is mapped to a single output label (e.g., positive or negative sentiment).</p>
</li>
<li>
<p>Input: A sequence of word vectors <span class="arithmatex">\((x_1, x_2, \dots, x_T)\)</span>  </p>
</li>
<li>Architecture: A bidirectional RNN (BiRNN) is commonly used to capture both past and future context. Hidden states from both directions <span class="arithmatex">\((\overrightarrow{h_t}, \overleftarrow{h_t})\)</span> are concatenated  </li>
<li>Output: All hidden states are typically aggregated (e.g., by mean pooling, attention, or taking the final state), then passed through a feedforward layer and softmax for classification. Aggregation converts a variable-length sequence of hidden states into a fixed-dimensional representation suitable for classification.</li>
</ol>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/rnn_sentimental.png" data-desc-position="bottom"><img alt="RNN applied to Sentiment Analysis" src="../nlp/images/rnn_sentimental.png"></a></p>
<p>Note: Bidirectional architectures often yield better performance in sentiment tasks because sentiment cues may appear anywhere in the sequence and depend on both preceding and following words.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/rnn_bi.png" data-desc-position="bottom"><img alt="RNN - Bidirectional" src="../nlp/images/rnn_bi.png"></a></p>
<ol>
<li>Multi-layer RNNs</li>
</ol>
<p>Stacking multiple RNN layers allows the model to learn increasingly abstract representations of the input sequence, similar to how deeper CNNs or MLPs work in vision and other tasks. Each layer processes the full sequence, allowing higher layers to operate on representations produced by lower layers.</p>
<ul>
<li>Lower layers: Learn low-level features (e.g., short-range dependencies or local syntax)  </li>
<li>Higher layers: Learn high-level abstractions (e.g., semantics, global structure)  </li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/rnn_multi.png" data-desc-position="bottom"><img alt="RNN - Multilayer" src="../nlp/images/rnn_multi.png"></a></p>
<ol>
<li>
<p>Seq-to-seq
Some tasks require transforming one sequence into another sequence of different length.</p>
</li>
<li>
<p>Sequence-to-sequence models are based on the encoder-decoder architecture  </p>
</li>
<li>The encoder processes the input sequence into a fixed-length neural representation  </li>
<li>The decoder generates the output sequence from this encoded representation  </li>
<li>When both the input and output are sequences, the model is referred to as a seq2seq model  </li>
</ol>
<p>Early seq2seq models rely on a fixed-length encoded representation, which can limit performance on long sequences.</p>
<p>Applications of sequence-to-sequence models beyond machine translation:</p>
<ul>
<li>Summarization: Long documents <span class="arithmatex">\(\rightarrow\)</span> short summaries  </li>
<li>Dialogue generation: Previous utterances <span class="arithmatex">\(\rightarrow\)</span> next response  </li>
<li>Syntactic parsing: Input text <span class="arithmatex">\(\rightarrow\)</span> parse tree (as a sequence)  </li>
<li>Code generation: Natural language <span class="arithmatex">\(\rightarrow\)</span> source code (e.g., Python)  </li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/rnn_seq_to_seq.png" data-desc-position="bottom"><img alt="RNN - Seq-to-seq" src="../nlp/images/rnn_seq_to_seq.png"></a></p>
<p>These applications illustrate how RNNs can be adapted to a wide range of sequence modeling problems by varying the architecture and output structure.</p></body></html></section><section class="print-page" id="nlp-7_eval_nlp" heading-number="2.7"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="chapter-7-evaluation-metrics-in-nlp">Chapter 7: Evaluation Metrics in NLP<a class="headerlink" href="#nlp-7_eval_nlp-chapter-7-evaluation-metrics-in-nlp" title="Permanent link">¶</a></h1>
<p>Evaluating NLP models is essential to ensure their performance, generalizability, and utility. The appropriate metric depends on the specific task, data characteristics, and desired outcome. Below, we detail evaluation methods used for key NLP tasks and provide a comparative table of metrics.</p>
<p>Evaluation metrics quantify different aspects of model behavior, such as accuracy, fluency, semantic adequacy, and robustness, and no single metric is sufficient for all tasks.</p>
<h2 id="nlp-7_eval_nlp-machine-translation-evaluation">Machine Translation Evaluation<a class="headerlink" href="#nlp-7_eval_nlp-machine-translation-evaluation" title="Permanent link">¶</a></h2>
<p>Machine translation is a structured generation task where outputs are compared against one or more human reference translations.</p>
<h3 id="nlp-7_eval_nlp-1-bleu-bilingual-evaluation-understudy">1. BLEU (Bilingual Evaluation Understudy)<a class="headerlink" href="#nlp-7_eval_nlp-1-bleu-bilingual-evaluation-understudy" title="Permanent link">¶</a></h3>
<p>BLEU is a precision-based metric used to evaluate the quality of machine-translated text by comparing it to one or more human reference translations. It uses modified n-gram precision and a brevity penalty to penalize overly short translations.</p>
<div class="arithmatex">\[
\text{BLEU} = \text{BP} \cdot \exp\left( \sum_{n=1}^{N} w_n \log p_n \right)
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(p_n\)</span> is the modified (clipped) precision for n-grams of size <span class="arithmatex">\(n\)</span></li>
<li><span class="arithmatex">\(w_n\)</span> is the weight for each n-gram order (typically <span class="arithmatex">\(w_n = \frac{1}{N}\)</span>)</li>
<li><span class="arithmatex">\(\text{BP}\)</span> is the brevity penalt
y to penalize translations that are too short
BLEU primarily measures surface-level n-gram overlap and does not directly capture semantic adequacy or fluency.</li>
</ul>
<p>The clipped precision <span class="arithmatex">\(p_n\)</span> is defined as:</p>
<div class="arithmatex">\[
p_n =
\frac{
\sum_{\text{ngram} \in C}
\min\left(
\text{Count}_{\text{clip}}(\text{ngram}),
\text{MaxRefCount}(\text{ngram})
\right)
}{
\sum_{\text{ngram} \in C}
\text{Count}(\text{ngram})
}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(C\)</span> is the set of n-grams of order <span class="arithmatex">\(n\)</span> in the candidate translation</li>
<li><span class="arithmatex">\(\text{Count}(\text{ngram})\)</span> is the number of times an n-gram appears in the candidate</li>
<li><span class="arithmatex">\(\text{MaxRefCount}(\text{ngram})\)</span> is the maximum number of times the n-gram appears in any reference translation</li>
<li><span class="arithmatex">\(\text{Count}_{\text{clip}}(\text{ngram})\)</span> is clipped to avoid overcounting spurious matches</li>
</ul>
<p>Clipping prevents a system from gaining excessive credit by repeating n-grams that appear only a limited number of times in the references.</p>
<p>The brevity penalty (BP) is given by:</p>
<div class="arithmatex">\[
\text{BP} =
\begin{cases}
1 &amp; \text{if } c &gt; r \\
\exp\left(1 - \frac{r}{c}\right) &amp; \text{if } c \le r
\end{cases}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(c\)</span> is the length of the candidate translation</li>
<li><span class="arithmatex">\(r\)</span> is the effective reference length (usually the closest reference length to <span class="arithmatex">\(c\)</span>)</li>
</ul>
<p>Without the brevity penalty, a system could achieve high precision by generating unnaturally short translations.</p>
<p>Use: BLEU is widely used for evaluating machine translation systems and is more reliable at the corpus level than at the sentence level.</p>
<h3 id="nlp-7_eval_nlp-2-rouge-recall-oriented-understudy-for-gisting-evaluation">2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)<a class="headerlink" href="#nlp-7_eval_nlp-2-rouge-recall-oriented-understudy-for-gisting-evaluation" title="Permanent link">¶</a></h3>
<p>Unlike BLEU, which emphasizes precision, ROUGE is recall-oriented and focuses on how much of the reference content is covered by the generated text.</p>
<p>Originally designed for summarization, ROUGE can also be used for machine translation and other generation tasks. It primarily measures recall of overlapping units such as n-grams and longest common subsequences.</p>
<ul>
<li>ROUGE-N: Overlap of n-grams (e.g., ROUGE-1, ROUGE-2)</li>
<li>ROUGE-L: Longest Common Subsequence (LCS)-based recall</li>
<li>ROUGE-S: Skip bigram-based F-measure</li>
</ul>
<div class="arithmatex">\[
\text{ROUGE-N Recall} =
\frac{
\sum_{\text{reference}}
\text{match}_{n\text{-grams}}
}{
\sum_{\text{reference}}
\text{total}_{n\text{-grams}}
}
\]</div>
<p>Use: Especially useful in summarization tasks and adaptable to translation.</p>
<h3 id="nlp-7_eval_nlp-other-metrics-for-mt-and-generation">Other Metrics for MT and Generation<a class="headerlink" href="#nlp-7_eval_nlp-other-metrics-for-mt-and-generation" title="Permanent link">¶</a></h3>
<p>Recent evaluation metrics aim to move beyond surface overlap by incorporating linguistic knowledge or learned semantic representations.</p>
<ul>
<li>METEOR: Considers synonym matching, stemming, and alignment; combines precision and recall using an F-score</li>
<li>chrF: Character n-gram F-score; language-agnostic and effective for morphologically rich languages</li>
<li>BERTScore: Uses contextual embeddings from BERT to compute similarity between candidate and reference</li>
<li>COMET / BLEURT: Learned metrics trained on human judgment data; state-of-the-art for semantic machine translation evaluation</li>
</ul>
<p>While these metrics correlate better with human judgments, they depend on pretrained models and training data biases.</p>
<h2 id="nlp-7_eval_nlp-comprehensive-evaluation-table-for-nlp-tasks">Comprehensive Evaluation Table for NLP Tasks<a class="headerlink" href="#nlp-7_eval_nlp-comprehensive-evaluation-table-for-nlp-tasks" title="Permanent link">¶</a></h2>
<p>Table below summarizes commonly used metrics across major NLP tasks, highlighting typical evaluation practices rather than exhaustive standards.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Common Metrics</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Part-of-Speech Tagging</td>
<td>Accuracy</td>
<td>Fraction of correctly predicted tags per token</td>
</tr>
<tr>
<td>Named Entity Recognition</td>
<td>Precision, Recall, F1-score (token/span-level)</td>
<td>Evaluates boundary and entity type correctness using strict span or loose token overlap</td>
</tr>
<tr>
<td>Sentiment Analysis</td>
<td>Accuracy, F1-score, MCC</td>
<td>Binary or multi-class classification; class imbalance is often a concern</td>
</tr>
<tr>
<td>Text Classification</td>
<td>Accuracy, Precision, Recall, F1-score</td>
<td>General-purpose classification; often multi-label or hierarchical</td>
</tr>
<tr>
<td>Machine Translation</td>
<td>BLEU, ROUGE, METEOR, chrF, COMET, BERTScore</td>
<td>BLEU is standard; newer metrics better reflect semantic adequacy</td>
</tr>
<tr>
<td>Summarization</td>
<td>ROUGE-1/2/L, BERTScore, BLEURT</td>
<td>ROUGE-L captures sequence overlap; embedding-based metrics assess meaning</td>
</tr>
<tr>
<td>Question Answering</td>
<td>EM, F1-score</td>
<td>Used for span-based QA; F1 accounts for partial overlap</td>
</tr>
<tr>
<td>Text Generation</td>
<td>BLEU, ROUGE, BERTScore, MAUVE, Human evaluation</td>
<td>Open-ended generation requires fluency and diversity metrics</td>
</tr>
<tr>
<td>Dialogue Systems</td>
<td>USR, BLEU, METEOR, BERTScore, Human evaluation</td>
<td>Measures relevance, coherence, and engagement</td>
</tr>
<tr>
<td>Text Simplification</td>
<td>SARI, FKGL, BLEU</td>
<td>SARI evaluates beneficial edits; BLEU may not correlate with simplicity</td>
</tr>
<tr>
<td>Coreference Resolution</td>
<td>MUC, B<span class="arithmatex">\(^3\)</span>, CEAF</td>
<td>Measures clustering of entity mentions; metrics are often used together</td>
</tr>
<tr>
<td>Language Modeling</td>
<td>Perplexity, Cross-Entropy</td>
<td>Evaluates next-token prediction; lower is better</td>
</tr>
<tr>
<td>Paraphrase Detection / Text Similarity</td>
<td>Accuracy, Cosine similarity, Pearson/Spearman correlation</td>
<td>Used in semantic textual similarity and entailment tasks</td>
</tr>
<tr>
<td>Information Retrieval / Ranking</td>
<td>MRR, MAP, Recall@k, nDCG</td>
<td>Measures ranking quality for documents or passages</td>
</tr>
</tbody>
</table>
<p>In practice, reliable evaluation often combines automatic metrics with human judgment, especially for open-ended generation tasks. Metric selection should be guided by task objectives, data characteristics, and known limitations of each evaluation method.</p></body></html></section><section class="print-page" id="nlp-8_attention_seq2seq" heading-number="2.8"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="8-attention-mechanism-in-sequence-to-sequence-models">8. Attention Mechanism in Sequence-to-Sequence Models<a class="headerlink" href="#nlp-8_attention_seq2seq-8-attention-mechanism-in-sequence-to-sequence-models" title="Permanent link">¶</a></h1>
<p>Traditional sequence-to-sequence (seq2seq) models suffer from a bottleneck where the entire source sentence is compressed into a single fixed-size vector (i.e., the final encoder hidden state). This poses challenges, particularly for long or information-rich inputs, as important details may be lost. This fixed-length bottleneck becomes increasingly problematic as input sequences grow longer or contain multiple salient elements.</p>
<p>The attention mechanism alleviates this issue by allowing the decoder to dynamically attend to different parts of the input sequence at each time step. Rather than depending solely on a single context vector, the decoder computes a weighted combination of all encoder hidden states, tailored for each output step. Intuitively, attention allows the decoder to query the encoder for the most relevant information at each decoding step.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/attention_seq_t_seq.png" data-desc-position="bottom"><img alt="Attention - Seq-to-seq" src="../nlp/images/attention_seq_t_seq.png"></a></p>
<h3 id="nlp-8_attention_seq2seq-mathematical-formulation">Mathematical Formulation<a class="headerlink" href="#nlp-8_attention_seq2seq-mathematical-formulation" title="Permanent link">¶</a></h3>
<p>Assume the encoder outputs a sequence of hidden states. The encoder hidden states represent the source sequence, while the decoder hidden state represents the current generation context.</p>
<div class="arithmatex">\[
h_1, h_2, \dots, h_N \in \mathbb{R}^h
\]</div>
<p>and the decoder has a hidden state at time step <span class="arithmatex">\(t\)</span>:</p>
<div class="arithmatex">\[
s_t \in \mathbb{R}^h
\]</div>
<p>The attention mechanism proceeds as follows:</p>
<ol>
<li>
<p>Score computation: Compute unnormalized attention scores between the decoder hidden state and each encoder hidden state:</p>
<div class="arithmatex">\[
e^t = [s_t^\top h_1, \dots, s_t^\top h_N] \in \mathbb{R}^N
\]</div>
<p>Each score measures the compatibility between the current decoder state and a specific encoder state.</p>
</li>
<li>
<p>Attention weights: Apply the softmax function to normalize these scores and obtain a probability distribution over the input positions:</p>
<div class="arithmatex">\[
\alpha^t = \text{softmax}(e^t) \in \mathbb{R}^N
\]</div>
<p>The resulting weights form a probability distribution over input positions.</p>
</li>
<li>
<p>Context vector: Compute the attention output <span class="arithmatex">\(a_t\)</span> as the weighted sum of the encoder hidden states:</p>
<div class="arithmatex">\[
a_t = \sum_{i=1}^N \alpha_i^t h_i \in \mathbb{R}^h
\]</div>
<p>This context vector changes at every decoding step, enabling dynamic focus over the input sequence.</p>
</li>
<li>
<p>Final decoder input: Concatenate the attention vector <span class="arithmatex">\(a_t\)</span> with the decoder hidden state <span class="arithmatex">\(s_t\)</span> to form a rich context vector for output generation:</p>
<div class="arithmatex">\[
[a_t; s_t] \in \mathbb{R}^{2h}
\]</div>
</li>
</ol>
<p>This combined vector is typically passed through a feedforward layer or used directly for predicting the output token.</p>
<h2 id="nlp-8_attention_seq2seq-interpretability-and-alignment">Interpretability and Alignment<a class="headerlink" href="#nlp-8_attention_seq2seq-interpretability-and-alignment" title="Permanent link">¶</a></h2>
<p>One of the key advantages of attention is that it provides a degree of interpretability to the model. Specifically:</p>
<ul>
<li>Alignment visualization: The attention weights <span class="arithmatex">\(\alpha^t\)</span> at each decoder step can be visualized to see which parts of the input the model is focusing on.</li>
<li>Soft alignment: Attention naturally yields a soft alignment between source and target tokens without explicit supervision. The network learns this alignment purely from end-to-end training.</li>
<li>Insight into model behavior: These alignments allow us to debug, explain, and understand the model's translation or generation decisions.</li>
</ul>
<p>This emergent alignment capability is one of the reasons attention mechanisms are considered both powerful and elegant.</p>
<h2 id="nlp-8_attention_seq2seq-variants-of-attention-mechanisms">Variants of Attention Mechanisms<a class="headerlink" href="#nlp-8_attention_seq2seq-variants-of-attention-mechanisms" title="Permanent link">¶</a></h2>
<p>While the basic attention mechanism uses a simple dot product between decoder and encoder hidden states, there are several alternative formulations that aim to improve the flexibility or expressiveness of the attention scoring function. Let <span class="arithmatex">\(h_i \in \mathbb{R}^{d_1}\)</span> denote the encoder hidden state at position <span class="arithmatex">\(i\)</span>, and <span class="arithmatex">\(s \in \mathbb{R}^{d_2}\)</span> be the current decoder hidden state.</p>
<h3 id="nlp-8_attention_seq2seq-1-dot-product-attention-luong-et-al">1. Dot-Product Attention (Luong et al.)<a class="headerlink" href="#nlp-8_attention_seq2seq-1-dot-product-attention-luong-et-al" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
e_i = s^\top h_i \in \mathbb{R}
\]</div>
<p>This is the simplest form of attention, assuming that <span class="arithmatex">\(d_1 = d_2\)</span>. This method is efficient and often works well in practice, but lacks trainable parameters in the scoring function. Its simplicity makes it computationally efficient and well suited for large-scale models.</p>
<h3 id="nlp-8_attention_seq2seq-2-multiplicative-bilinear-attention">2. Multiplicative (Bilinear) Attention<a class="headerlink" href="#nlp-8_attention_seq2seq-2-multiplicative-bilinear-attention" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
e_i = s^\top W h_i \in \mathbb{R}
\]</div>
<p>where <span class="arithmatex">\(W \in \mathbb{R}^{d_2 \times d_1}\)</span> is a learned weight matrix. This adds more expressiveness to the attention mechanism, enabling a trainable compatibility function between <span class="arithmatex">\(s\)</span> and <span class="arithmatex">\(h_i\)</span>. </p>
<h3 id="nlp-8_attention_seq2seq-3-reduced-rank-multiplicative-attention">3. Reduced-Rank Multiplicative Attention<a class="headerlink" href="#nlp-8_attention_seq2seq-3-reduced-rank-multiplicative-attention" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
e_i = s^\top (U^\top V) h_i = (Us)^\top (Vh_i)
\]</div>
<p>with <span class="arithmatex">\(U \in \mathbb{R}^{k \times d_2}\)</span> and <span class="arithmatex">\(V \in \mathbb{R}^{k \times d_1}\)</span>, where <span class="arithmatex">\(k \ll d_1, d_2\)</span>. This low-rank factorization reduces the number of parameters and computations and is conceptually related to self-attention in Transformers. This formulation trades expressiveness for efficiency by constraining the attention interaction to a lower-dimensional subspace.</p>
<h3 id="nlp-8_attention_seq2seq-4-additive-bahdanau-attention">4. Additive (Bahdanau) Attention<a class="headerlink" href="#nlp-8_attention_seq2seq-4-additive-bahdanau-attention" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[
e_i = v^\top \tanh(W_1 h_i + W_2 s) \in \mathbb{R}
\]</div>
<p>where <span class="arithmatex">\(W_1 \in \mathbb{R}^{d_3 \times d_1}\)</span>, <span class="arithmatex">\(W_2 \in \mathbb{R}^{d_3 \times d_2}\)</span> are learned weight matrices, and <span class="arithmatex">\(v \in \mathbb{R}^{d_3}\)</span> is a learned vector. Here, <span class="arithmatex">\(d_3\)</span> is a tunable dimensionality (sometimes called the attention dimension). Despite its name, this formulation involves a nonlinearity and trainable layers, making it functionally a feedforward neural network scoring mechanism. This form of attention was used in early neural machine translation systems and performs well when encoder and decoder dimensions differ.</p>
<p>Attention mechanisms eliminate the fixed-length context bottleneck and form the foundation for modern architectures such as Transformers, which rely entirely on attention for sequence modeling.</p></body></html></section><section class="print-page" id="nlp-9_selfattention" heading-number="2.9"><h1 id="nlp-9_selfattention-nlp-9_selfattention">9. Self- Attention</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="self-attention-and-transformers">Self-Attention and Transformers<a class="headerlink" href="#nlp-9_selfattention-self-attention-and-transformers" title="Permanent link">¶</a></h2>
<p>This chapter explains how self-attention addresses the core limitations of recurrent architectures and forms the foundation of Transformer models.</p>
<h2 id="nlp-9_selfattention-limitations-of-recurrent-models-linear-interaction-distance">Limitations of Recurrent Models: Linear Interaction Distance<a class="headerlink" href="#nlp-9_selfattention-limitations-of-recurrent-models-linear-interaction-distance" title="Permanent link">¶</a></h2>
<p>Recurrent neural networks (RNNs) process input sequences sequentially, typically from left to right. This enforces a form of linear locality, which reflects the useful heuristic that nearby words often influence each other's meanings. However, this sequential structure introduces several key limitations:</p>
<ul>
<li>
<p>Inefficient long-range dependencies: For two tokens separated by <span class="arithmatex">\(n\)</span> positions, RNNs require <span class="arithmatex">\(O(n)\)</span> steps for information to propagate between them. This makes it difficult to model long-distance dependencies, as the gradient signal must traverse many time steps, leading to vanishing or exploding gradients. Even in gated variants such as LSTMs and GRUs, this remains a practical bottleneck.</p>
</li>
<li>
<p>Hardcoded sequential bias: The linear order of token processing is baked into the architecture. However, natural language often exhibits hierarchical or non-sequential structure that is not well captured by strictly left-to-right modeling.</p>
</li>
<li>
<p>Limited parallelization: The computation of each hidden state depends on the previous one. Both forward and backward passes involve <span class="arithmatex">\(O(n)\)</span> inherently sequential operations, preventing full utilization of modern parallel computing hardware such as GPUs.</p>
</li>
<li>
<p>Scalability challenges: Due to the sequential nature of training, RNNs are slower to train on large datasets and harder to scale to deep architectures.</p>
</li>
</ul>
<p>These issues stem from the requirement that information must pass through a chain of intermediate states. These limitations motivate the development of alternative architectures. In particular, self-attention mechanisms, as introduced in the Transformer model, allow each token to attend to all others directly in a single logical step, enabling efficient modeling of long-range dependencies and highly parallelizable computation.</p>
<h2 id="nlp-9_selfattention-attention-mechanisms">Attention Mechanisms<a class="headerlink" href="#nlp-9_selfattention-attention-mechanisms" title="Permanent link">¶</a></h2>
<p>Attention mechanisms enable each word in a sentence to dynamically incorporate information from other words. At a high level, attention treats each word’s representation as a query that retrieves and integrates information from a set of key-value pairs derived from the same or another sequence. In self-attention, queries, keys, and values are all derived from the same input sequence.</p>
<p>Unlike recurrent models, attention does not rely on sequential processing. This leads to two key computational benefits:</p>
<ul>
<li>
<p>Constant logical interaction distance: Any token can attend to any other token within a sentence in a single logical step, making the maximum dependency path <span class="arithmatex">\(O(1)\)</span>. However, the actual computational cost remains <span class="arithmatex">\(O(n^2)\)</span> due to pairwise interactions.</p>
</li>
<li>
<p>Fully parallelizable: Since all token interactions can be computed simultaneously, the number of sequential (unparallelizable) operations does not grow with sequence length.</p>
</li>
</ul>
<p>From encoder-decoder to self-attention.<br>
In earlier encoder-decoder models (e.g., for machine translation), attention was applied from the decoder to the encoder, allowing the decoder to selectively focus on parts of the input sequence. In self-attention, we apply the same mechanism within a single sequence, allowing each word to attend to every other word in the same sequence.</p>
<p>Attention as soft lookup.<br>
Conceptually, attention can be viewed as a soft, weighted lookup in a key-value store. Each query computes a similarity score with every key, producing a weight (typically via softmax). The output is a weighted average of the values:</p>
<div class="arithmatex">\[
\text{output} = \operatorname{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\]</div>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/attention_lookup.png" data-desc-position="bottom"><img alt="Attention - Soft Lookup" src="../nlp/images/attention_lookup.png"></a></p>
<h2 id="nlp-9_selfattention-self-attention-computing-contextual-representations">Self-Attention: Computing Contextual Representations<a class="headerlink" href="#nlp-9_selfattention-self-attention-computing-contextual-representations" title="Permanent link">¶</a></h2>
<p>Let <span class="arithmatex">\(\mathbf{w}_{1:n}\)</span> be a sequence of token indices. Each token <span class="arithmatex">\(\mathbf{w}_i\)</span> is mapped to an embedding vector <span class="arithmatex">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> using an embedding matrix <span class="arithmatex">\(E \in \mathbb{R}^{d \times |V|}\)</span>.</p>
<p>For each input vector <span class="arithmatex">\(\mathbf{x}_i\)</span>, we compute three linear projections:</p>
<div class="arithmatex">\[
\mathbf{q}_i = Q \mathbf{x}_i, \quad
\mathbf{k}_i = K \mathbf{x}_i, \quad
\mathbf{v}_i = V \mathbf{x}_i
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(Q, K, V \in \mathbb{R}^{d_a \times d}\)</span> are learnable projection matrices  </li>
<li><span class="arithmatex">\(d\)</span> is the input embedding dimension  </li>
<li><span class="arithmatex">\(d_a\)</span> is the dimension of the attention subspace (often <span class="arithmatex">\(d_a = d / h\)</span> for <span class="arithmatex">\(h\)</span> heads)</li>
</ul>
<p>The dot product of <span class="arithmatex">\(\mathbf{q}_i\)</span> and <span class="arithmatex">\(\mathbf{k}_j\)</span> determines how much token <span class="arithmatex">\(i\)</span> attends to token <span class="arithmatex">\(j\)</span>:</p>
<div class="arithmatex">\[
e_{ij} = \frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_a}},
\quad
\alpha_{ij} = \text{softmax}_j(e_{ij})
\]</div>
<p>The scaling factor <span class="arithmatex">\(\sqrt{d_a}\)</span> stabilizes gradients by preventing dot products from growing too large.</p>
<div class="arithmatex">\[
\mathbf{o}_i = \sum_{j=1}^n \alpha_{ij} \mathbf{v}_j
\]</div>
<p>Each token’s output <span class="arithmatex">\(\mathbf{o}_i\)</span> is a weighted sum of the value vectors from all tokens, where the weights are determined by how similar their keys are to the query. Each output vector therefore represents the token in the context of the entire sequence.</p>
<p>Multi-head attention.<br>
Instead of one set of <span class="arithmatex">\(Q, K, V\)</span>, Transformers use <span class="arithmatex">\(h\)</span> sets to learn diverse patterns. Each head uses its own set of <span class="arithmatex">\(Q_h, K_h, V_h \in \mathbb{R}^{d_h \times d}\)</span> with <span class="arithmatex">\(d_h = d / h\)</span>, and their outputs are concatenated:</p>
<div class="arithmatex">\[
\text{MultiHead}(X)
=
\text{Concat}(\mathbf{o}_i^{(1)}, \dots, \mathbf{o}_i^{(h)}) W^O,
\quad
W^O \in \mathbb{R}^{d \times d}
\]</div>
<p>Using multiple heads allows the model to attend to different types of relationships, such as syntax, coreference, or positional patterns.</p>
<h2 id="nlp-9_selfattention-nonlinearities-and-the-role-of-feed-forward-networks">Nonlinearities and the Role of Feed-Forward Networks<a class="headerlink" href="#nlp-9_selfattention-nonlinearities-and-the-role-of-feed-forward-networks" title="Permanent link">¶</a></h2>
<p>Attention mixes information across tokens, while feed-forward networks transform information within each token. Note that the self-attention mechanism itself is linear with respect to the input embeddings. There are no activation functions inside the attention computation. Stacking more self-attention layers only re-averages and mixes value vectors.</p>
<p>To introduce nonlinearity and enable richer representations, each Transformer block includes a position-wise feed-forward network applied independently to each token:</p>
<div class="arithmatex">\[
\text{FFN}(\mathbf{o}_i)
=
W_2 \cdot \text{ReLU}(W_1 \cdot \mathbf{o}_i + \mathbf{b}_1) + \mathbf{b}_2
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(W_1 \in \mathbb{R}^{d_{\text{ff}} \times d}\)</span> and <span class="arithmatex">\(W_2 \in \mathbb{R}^{d \times d_{\text{ff}}}\)</span> are learnable parameters  </li>
<li><span class="arithmatex">\(d_{\text{ff}}\)</span> is typically larger than <span class="arithmatex">\(d\)</span></li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/attention_nonlinear_ff.png" data-desc-position="bottom"><img alt="Attention Nonlinear FFN" src="../nlp/images/attention_nonlinear_ff.png"></a></p>
<p>This nonlinear transformation enhances the model's capacity and expressiveness.</p>
<h2 id="nlp-9_selfattention-masking-the-future-in-self-attention">Masking the Future in Self-Attention<a class="headerlink" href="#nlp-9_selfattention-masking-the-future-in-self-attention" title="Permanent link">¶</a></h2>
<p>In autoregressive tasks such as language modeling or machine translation decoding, the model must not attend to future tokens.</p>
<p>Naive approach (inefficient).<br>
At each time step <span class="arithmatex">\(i\)</span>, compute attention using only tokens <span class="arithmatex">\(1\)</span> through <span class="arithmatex">\(i\)</span>. This requires sequential recomputation and defeats parallelization.</p>
<p>Efficient approach: causal masking.<br>
Full attention scores are computed, but future tokens are masked by setting their logits to <span class="arithmatex">\(-\infty\)</span>:</p>
<div class="arithmatex">\[
e_{ij} =
\begin{cases}
\frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_a}}, &amp; j \le i \\
-\infty, &amp; j &gt; i
\end{cases}
\]</div>
<p>This causes <span class="arithmatex">\(\alpha_{ij} = 0\)</span> for future positions while preserving parallel computation.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/attention_mask.png" data-desc-position="bottom"><img alt="Attention Mask" src="../nlp/images/attention_mask.png"></a></p>
<p>Effect: Prevents information from future tokens from leaking into past representations while enabling efficient training.</p>
<h2 id="nlp-9_selfattention-positional-encoding-in-self-attention">Positional Encoding in Self-Attention<a class="headerlink" href="#nlp-9_selfattention-positional-encoding-in-self-attention" title="Permanent link">¶</a></h2>
<p>Since attention is content-based, it is insensitive to token positions unless position information is added.</p>
<p>Absolute positional encoding.<br>
Positional vectors <span class="arithmatex">\(\mathbf{p}_i\)</span> are added to the input embeddings:</p>
<div class="arithmatex">\[
\tilde{\mathbf{x}}_i = \mathbf{x}_i + \mathbf{p}_i
\]</div>
<p>Sinusoidal (fixed) vs learned (trainable):</p>
<ul>
<li>
<p>Sinusoidal:
<script type="math/tex; mode=display">
\mathbf{p}_i^{(2k)} = \sin(i / 10000^{2k/d}),
\quad
\mathbf{p}_i^{(2k+1)} = \cos(i / 10000^{2k/d})
</script>
</p>
</li>
<li>
<p>Learned: Each position has a trainable vector</p>
</li>
</ul>
<p>Relative and rotary positional encodings:</p>
<ul>
<li>Relative: Represent relative distance between tokens  </li>
<li>Rotary (RoPE): Rotate queries and keys in complex space to encode position  </li>
</ul>
<p>These methods improve generalization to longer contexts and are used in modern architectures such as LLaMA and Transformer-XL.</p>
<h2 id="nlp-9_selfattention-necessities-for-a-self-attention-building-block">Necessities for a Self-Attention Building Block<a class="headerlink" href="#nlp-9_selfattention-necessities-for-a-self-attention-building-block" title="Permanent link">¶</a></h2>
<ul>
<li>Self-attention: the core mechanism  </li>
<li>Position representations: specify sequence order since self-attention is permutation-invariant  </li>
<li>Nonlinearities: applied after self-attention, typically via feed-forward networks  </li>
<li>Masking: prevents future information leakage while enabling parallel computation  </li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/attention_basic.png" data-desc-position="bottom"><img alt="Attention Basics" src="../nlp/images/attention_basic.png"></a></p>
<p>Together, these components define a complete and scalable self-attention building block.</p>
<p>Stacking these blocks yields the Transformer architecture, which has become the dominant model for modern natural language processing.</p>
<h2 id="nlp-9_selfattention-sequence-stacked-and-multi-headed-attention">Sequence-Stacked and Multi-Headed Attention<a class="headerlink" href="#nlp-9_selfattention-sequence-stacked-and-multi-headed-attention" title="Permanent link">¶</a></h2>
<p>This section reformulates self-attention in matrix form and extends it to multi-head attention, which is the core computational unit of Transformer layers.</p>
<h3 id="nlp-9_selfattention-matrix-formulation-of-self-attention">Matrix formulation of self-attention<a class="headerlink" href="#nlp-9_selfattention-matrix-formulation-of-self-attention" title="Permanent link">¶</a></h3>
<p>Self-attention allows each token in a sequence to attend to every other token, enabling contextual representation learning. Let <span class="arithmatex">\(X = [x_1, \dots, x_n] \in \mathbb{R}^{n \times d}\)</span> be the input sequence of <span class="arithmatex">\(n\)</span> token embeddings, where <span class="arithmatex">\(d\)</span> is the embedding dimension. Each row <span class="arithmatex">\(x_i \in \mathbb{R}^d\)</span> corresponds to the <span class="arithmatex">\(i\)</span>-th token. We compute projections of the input to obtain the query, key, and value matrices:</p>
<div class="arithmatex">\[
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
\]</div>
<p>where <span class="arithmatex">\(W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}\)</span> are learned projection matrices, and<br>
<span class="arithmatex">\(Q, K, V \in \mathbb{R}^{n \times d_k}\)</span> are the resulting matrices of projected queries, keys, and values.</p>
<p>This matrix formulation allows attention to be computed for all tokens simultaneously using efficient linear algebra operations.</p>
<p>Interpretation of dimensions:</p>
<ul>
<li><span class="arithmatex">\(n\)</span>: number of tokens in the input sequence  </li>
<li><span class="arithmatex">\(d\)</span>: original token embedding dimension  </li>
<li><span class="arithmatex">\(d_k\)</span>: dimensionality of projected query, key, and value vectors  </li>
<li><span class="arithmatex">\(Q_{i\cdot}, K_{j\cdot} \in \mathbb{R}^{d_k}\)</span>: the query vector for token <span class="arithmatex">\(i\)</span> and the key vector for token <span class="arithmatex">\(j\)</span>  </li>
</ul>
<h3 id="nlp-9_selfattention-scaled-dot-product-attention">Scaled dot-product attention<a class="headerlink" href="#nlp-9_selfattention-scaled-dot-product-attention" title="Permanent link">¶</a></h3>
<p>Attention scores are computed using scaled dot-products between queries and keys:</p>
<div class="arithmatex">\[
S = \frac{Q K^\top}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}
\]</div>
<p>The matrix <span class="arithmatex">\(S\)</span> therefore contains all pairwise token interactions in the sequence. Each entry <span class="arithmatex">\(S_{ij}\)</span> represents the unnormalized attention score from token <span class="arithmatex">\(i\)</span> to token <span class="arithmatex">\(j\)</span>. The division by <span class="arithmatex">\(\sqrt{d_k}\)</span> stabilizes gradients by preventing dot products from becoming too large in high dimensions. A row-wise softmax is applied to <span class="arithmatex">\(S\)</span> to produce attention weights, which are then used to compute a weighted sum of the value vectors:</p>
<div class="arithmatex">\[
\text{Attention}(Q, K, V)
=
\text{softmax}\!\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
\in \mathbb{R}^{n \times d_k}
\]</div>
<p>This formulation enables fully parallelizable all-to-all interactions between tokens in a single matrix operation. </p>
<p>While single-head attention can model dependencies, it is limited to a single representation subspace.</p>
<h3 id="nlp-9_selfattention-multi-head-self-attention">Multi-head self-attention.<a class="headerlink" href="#nlp-9_selfattention-multi-head-self-attention" title="Permanent link">¶</a></h3>
<p>Single-head attention captures a single type of interaction. To disentangle different kinds of linguistic relationships such as syntactic structure, semantic similarity, or positional alignment, Transformers use multi-head attention, which computes multiple attention distributions in parallel.</p>
<p>Let <span class="arithmatex">\(h\)</span> be the number of attention heads. For each head <span class="arithmatex">\(\ell \in \{1, \dots, h\}\)</span>, the input<br>
<span class="arithmatex">\(X \in \mathbb{R}^{n \times d}\)</span> is projected into separate subspaces using head-specific learned matrices:</p>
<div class="arithmatex">\[
Q_\ell = X W_{\ell Q}, \quad
K_\ell = X W_{\ell K}, \quad
V_\ell = X W_{\ell V},
\]</div>
<p>where <span class="arithmatex">\(W_{\ell Q}, W_{\ell K}, W_{\ell V} \in \mathbb{R}^{d \times d_h}\)</span> and <span class="arithmatex">\(d_h = d / h\)</span>, ensuring that total computational cost remains comparable to single-head attention.</p>
<p>Each head computes its own scaled dot-product attention:</p>
<div class="arithmatex">\[
\text{head}_\ell
=
\text{softmax}\!\left( \frac{Q_\ell K_\ell^\top}{\sqrt{d_h}} \right) V_\ell
\in \mathbb{R}^{n \times d_h}
\]</div>
<p>The outputs of all heads are then concatenated and linearly transformed to produce the final result:</p>
<div class="arithmatex">\[
\text{MultiHead}(X)
=
\text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O,
\quad
W^O \in \mathbb{R}^{d \times d}
\]</div>
<p>The output combines multiple perspectives on the sequence, each capturing different relational patterns.</p>
<h3 id="nlp-9_selfattention-implementation-efficiency">Implementation efficiency<a class="headerlink" href="#nlp-9_selfattention-implementation-efficiency" title="Permanent link">¶</a></h3>
<p>Despite computing multiple attention heads, multi-head attention is efficient in practice. Once the projections <span class="arithmatex">\(Q\)</span>, <span class="arithmatex">\(K\)</span>, and <span class="arithmatex">\(V\)</span> are computed jointly for all heads, the resulting tensors are reshaped into <span class="arithmatex">\((n, h, d_h)\)</span> and then transposed to <span class="arithmatex">\((h, n, d_h)\)</span> to enable batched attention computation across heads. Treating the head index as an additional batch dimension allows highly optimized parallel execution.</p>
<p>Summary: Multi-head attention provides rich modeling capacity by enabling the network to focus on different parts of the sequence using separate learned subspaces. Each head attends to different contextual signals, and their combination offers a composite, expressive representation that is crucial for modeling complex dependencies in natural language.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/multihead.png" data-desc-position="bottom"><img alt="Multi-head Self-attention" src="../nlp/images/multihead.png"></a></p>
<h2 id="nlp-9_selfattention-optimization-tricks-add-norm">Optimization Tricks – Add &amp; Norm<a class="headerlink" href="#nlp-9_selfattention-optimization-tricks-add-norm" title="Permanent link">¶</a></h2>
<p>The Transformer uses the multi-head self-attention mechanism introduced earlier. To ensure better training dynamics and convergence, it incorporates two critical optimization components in each block:</p>
<ul>
<li>Residual connections  </li>
<li>Layer normalization  </li>
</ul>
<p>These operations are often summarized as Add &amp; Norm, since they occur together at every sublayer.</p>
<h3 id="nlp-9_selfattention-residual-connections">Residual Connections<a class="headerlink" href="#nlp-9_selfattention-residual-connections" title="Permanent link">¶</a></h3>
<p>Residual (or skip) connections help mitigate the vanishing gradient problem and make deep networks easier to train.  Instead of applying a transformation layer directly to the input:</p>
<div class="arithmatex">\[
\mathbf{x}^{(i)} = \text{Layer}(\mathbf{x}^{(i-1)})
\]</div>
<p>we instead compute:</p>
<div class="arithmatex">\[
\mathbf{x}^{(i)} = \mathbf{x}^{(i-1)} + \text{Layer}(\mathbf{x}^{(i-1)})
\]</div>
<p>This formulation learns only the residual between the input and the output, which often makes optimization easier. Key benefits include:</p>
<ul>
<li>Stable gradients: the gradient through the residual path is exactly 1, which improves backpropagation through deep networks  </li>
<li>Bias toward the identity function: encourages the model to retain information from earlier layers when deeper transformations are unnecessary  </li>
</ul>
<p>In Transformers, residual connections are applied around both the self-attention sublayer and the feed-forward sublayer.</p>
<h3 id="nlp-9_selfattention-layer-normalization">Layer Normalization<a class="headerlink" href="#nlp-9_selfattention-layer-normalization" title="Permanent link">¶</a></h3>
<p>Layer normalization improves training speed and stability by standardizing intermediate representations. Given a vector <span class="arithmatex">\(\mathbf{x} \in \mathbb{R}^d\)</span>, layer normalization computes:</p>
<div class="arithmatex">\[
\mu = \frac{1}{d} \sum_{j=1}^{d} x_j,
\quad
\sigma = \sqrt{ \frac{1}{d} \sum_{j=1}^{d} (x_j - \mu)^2 }
\]</div>
<p>The normalized output is then:</p>
<div class="arithmatex">\[
\text{LayerNorm}(\mathbf{x})
=
\frac{\mathbf{x} - \mu}{\sigma + \epsilon} \odot \gamma + \beta
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\gamma \in \mathbb{R}^d\)</span> is a learnable gain parameter  </li>
<li><span class="arithmatex">\(\beta \in \mathbb{R}^d\)</span> is a learnable bias parameter  </li>
<li><span class="arithmatex">\(\epsilon\)</span> is a small constant for numerical stability  </li>
</ul>
<p>The normalization is applied along the feature dimension independently for each token. This reduces internal covariate shift and stabilizes training.</p>
<p>Unlike batch normalization, layer normalization does not depend on batch statistics and is therefore well suited for sequence models.</p>
<p>Together, multi-head attention, residual connections, and layer normalization form the backbone of modern Transformer architectures.</p></body></html></section><section class="print-page" id="nlp-10_archi" heading-number="2.10"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="transformer-architectures">Transformer Architectures<a class="headerlink" href="#nlp-10_archi-transformer-architectures" title="Permanent link">¶</a></h1>
<p>Transformer architectures can be categorized based on how attention is applied to input and output sequences, leading to encoder-only, decoder-only, and encoder–decoder designs.</p>
<h2 id="nlp-10_archi-decoder">Decoder<a class="headerlink" href="#nlp-10_archi-decoder" title="Permanent link">¶</a></h2>
<p>The Transformer Decoder consists of a stack of identical Transformer Decoder Blocks. Each block contains the following components in sequence:</p>
<ul>
<li>Masked self-attention: Allows each position to attend only to previous positions in the sequence, enforcing auto-regressive generation  </li>
<li>Add and norm: Residual connection followed by layer normalization  </li>
<li>Feed-forward network (FFN): A position-wise fully connected feed-forward network  </li>
<li>Add and norm: Another residual connection and layer normalization  </li>
</ul>
<p>This structure ensures that the decoder generates tokens autoregressively, relying solely on previously generated outputs.</p>
<p>Decoder models use only the decoder of a Transformer model. At each stage, for a given word, the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.</p>
<p>The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p>
<p>These models are best suited for tasks involving text generation.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/decoder.png" data-desc-position="bottom"><img alt="Decoder" src="../nlp/images/decoder.png"></a></p>
<h2 id="nlp-10_archi-encoder">Encoder<a class="headerlink" href="#nlp-10_archi-encoder" title="Permanent link">¶</a></h2>
<p>The Transformer Encoder differs by enabling bidirectional context, analogous to bidirectional RNNs. This is achieved by removing the causal masking in the self-attention mechanism, allowing each token to attend to all tokens in the sequence.</p>
<p>Each encoder block consists of:</p>
<ul>
<li>Self-attention (unmasked): Enables full bidirectional context  </li>
<li>Add and norm  </li>
<li>Feed-forward network  </li>
<li>Add and norm  </li>
</ul>
<p>This allows the encoder to build comprehensive contextual representations of the input sequence.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/encoder.png" data-desc-position="bottom"><img alt="Encoder" src="../nlp/images/encoder.png"></a></p>
<p>The pretraining of these models usually revolves around corrupting a given sentence (for instance, by masking random words) and tasking the model with reconstructing the original sentence.</p>
<p>Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.</p>
<h2 id="nlp-10_archi-encoder-decoder">Encoder-Decoder<a class="headerlink" href="#nlp-10_archi-encoder-decoder" title="Permanent link">¶</a></h2>
<p>For sequence-to-sequence tasks such as machine translation, the source sentence is encoded with a bidirectional model, while the target sentence is generated with a unidirectional model. The Transformer encoder-decoder architecture formalizes this approach by combining:</p>
<ul>
<li>A standard Transformer encoder to process the entire source sequence  </li>
<li>A modified Transformer decoder that, in addition to masked self-attention, performs cross-attention over the encoder outputs  </li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/encdec.png" data-desc-position="bottom"><img alt="Encoder-Decoder" src="../nlp/images/encdec.png"></a></p>
<h3 id="nlp-10_archi-cross-attention">Cross-attention.<a class="headerlink" href="#nlp-10_archi-cross-attention" title="Permanent link">¶</a></h3>
<p>Cross-attention differs from self-attention in that the keys and values come from the encoder output, while the queries come from the decoder input states.</p>
<p>Formally, let</p>
<div class="arithmatex">\[
h_1, \ldots, h_n \in \mathbb{R}^d
\]</div>
<p>be the encoder output vectors, and</p>
<div class="arithmatex">\[
z_1, \ldots, z_m \in \mathbb{R}^d
\]</div>
<p>be the decoder input vectors. Then the keys and values drawn from the encoder are</p>
<div class="arithmatex">\[
K = W_k h_i, \quad V = W_h h_i
\]</div>
<p>and the queries are</p>
<div class="arithmatex">\[
Q = W_z z_j
\]</div>
<p>where <span class="arithmatex">\(W_k\)</span>, <span class="arithmatex">\(W_h\)</span>, and <span class="arithmatex">\(W_z\)</span> are learned projection matrices.</p>
<p>Cross-attention allows each decoder state to selectively retrieve information from the entire encoded source sequence.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/crossattention.png" data-desc-position="bottom"><img alt="Cross Attention" src="../nlp/images/crossattention.png"></a></p>
<p>The decoder uses these queries to attend to the encoder memory via the cross-attention mechanism, enabling selective focus on relevant parts of the source sequence when generating each target token.</p>
<p>The pretraining of these models can take different forms, but it often involves reconstructing a sentence for which the input has been corrupted. For example, the pretraining of the T5 model replaces random spans of text (which may contain multiple words) with a single mask token, and the task is to predict the text that the mask token replaces.</p>
<p>Sequence-to-sequence models are best suited for tasks involving generating new sentences conditioned on an input, such as summarization, translation, or generative question answering.</p>
<h2 id="nlp-10_archi-limitations-and-areas-for-improvement-in-the-transformer">Limitations and Areas for Improvement in the Transformer<a class="headerlink" href="#nlp-10_archi-limitations-and-areas-for-improvement-in-the-transformer" title="Permanent link">¶</a></h2>
<p>Despite its success, the Transformer architecture has some notable limitations that motivate ongoing research:</p>
<ul>
<li>
<p>Quadratic computational complexity in self-attention:   The self-attention mechanism requires computing interactions between all pairs of tokens in the input sequence, resulting in computation and memory complexity of <span class="arithmatex">\(O(n^2 d)\)</span>, where <span class="arithmatex">\(n\)</span> is the sequence length and <span class="arithmatex">\(d\)</span> is the dimensionality of the embeddings. Specifically, the attention matrix <span class="arithmatex">\(QK^\top \in \mathbb{R}^{n \times n}\)</span> encodes all pairwise interactions, which becomes prohibitive for very long sequences.For example, with <span class="arithmatex">\(n = 512\)</span>, the number of pairwise interactions exceeds <span class="arithmatex">\(262{,}000\)</span>. For sequences with <span class="arithmatex">\(n \ge 50{,}000\)</span>, this quadratic growth becomes infeasible. In contrast, recurrent models have computational complexity that grows linearly with sequence length.</p>
</li>
<li>
<p>Position representations:  Transformers typically use simple absolute positional encodings, such as sinusoidal embeddings, to inject information about token order. However, it remains an open question whether absolute indices are the optimal way to represent positional information. Alternatives such as relative position encodings and syntax-aware position embeddings have been proposed to capture dependencies more naturally and improve model performance.</p>
</li>
</ul></body></html></section><section class="print-page" id="nlp-11_tokens" heading-number="2.11"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="11-tokenization-and-tokens">11. Tokenization and Tokens<a class="headerlink" href="#nlp-11_tokens-11-tokenization-and-tokens" title="Permanent link">¶</a></h1>
<p>Tokenization defines the basic units over which a language model reasons and therefore strongly influences model capacity, efficiency, and generalization. Raw text is typically represented as Unicode strings, for example:</p>
<div class="arithmatex">\[
\texttt{string = "Hello, How are you?"}
\]</div>
<p>Language models operate over sequences of tokens, which are usually represented as integer indices. For example:</p>
<div class="arithmatex">\[
\texttt{indices = [15496, 11, 995, 0]}
\]</div>
<p>These integer indices serve as inputs to embedding layers, which map discrete tokens into continuous vector representations.</p>
<ul>
<li>A tokenizer is a class that implements two main functions:</li>
<li>encode: Converts a string into tokens (integers)</li>
<li>decode: Converts tokens back into a string</li>
<li>The vocabulary size refers to the number of distinct tokens in the model's dictionary.</li>
</ul>
<p>In practice, tokenizers are deterministic and shared between training and inference.</p>
<p>For interactive exploration, try:  <a href="https://tiktokenizer.vercel.app/?encoder=gpt2">Link</a></p>
<ul>
<li>
<p>Character-based tokenization: Character-based tokenizers split the input text into individual characters rather than words or subwords. This results in a much smaller and fixed vocabulary (e.g., all letters, digits, p. unctuation, and special characters). Every possible word can be represented, eliminating most out-of-vocabulary (OOV) issues</p>
<ul>
<li>
<p>Pros:</p>
<ul>
<li>Extremely small vocabulary size</li>
<li>Fully lossless representation — any text can be encoded and decoded perfectly</li>
<li>Robust to typos and unknown words — e.g., <code>"xylocopter"</code> can still be tokenized, even if it is a made-up word</li>
</ul>
</li>
<li>
<p>Cons:</p>
<ul>
<li>Characters are semantically weak — individual letters carry little meaning in Latin-based languages</li>
<li>Sequence lengths become much longer. For example, the word <code>"tokenization"</code> becomes 13 tokens instead of 1</li>
<li>More computationally expensive due to increased sequence length</li>
<li>May struggle to capture meaningful patterns unless large contexts are modeled</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>As a result, character-level models typically require deeper architectures or longer contexts to learn meaningful structure.</p>
<ul>
<li>
<p>Word-based tokenization:Splits text into words and punctuation using regular expressions.Each word is treated as a distinct token. For example, <code>"dog"</code> and <code>"dogs"</code> are different tokens even though they share semantic roots. Vocabulary size depends on the number of unique words in the corpus and is often extremely large. Word-based tokenization was the dominant approach in early statistical NLP systems.</p>
<ul>
<li>Pros: Shorter sequences and semantically intuitive tokens</li>
<li>Cons:<ul>
<li>Very large and variable vocabulary size</li>
<li>Morphologically similar words (e.g., <code>dog</code> vs <code>dogs</code>) are treated as unrelated tokens</li>
<li>Rare or unseen words require a special <code>[UNK]</code> (unknown) token</li>
<li>Vocabulary sparsity makes learning harder, especially for morphologically rich languages
These limitations are especially pronounced in languages with rich morphology or productive word formation.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="nlp-11_tokens-subword-modeling">Subword Modeling<a class="headerlink" href="#nlp-11_tokens-subword-modeling" title="Permanent link">¶</a></h2>
<h3 id="nlp-11_tokens-motivation-and-linguistic-foundations">Motivation and Linguistic Foundations<a class="headerlink" href="#nlp-11_tokens-motivation-and-linguistic-foundations" title="Permanent link">¶</a></h3>
<p>Subword modeling strikes a compromise between character-level and word-level tokenization.</p>
<p>Traditional NLP systems often assume a fixed-size vocabulary constructed from the training corpus, typically comprising tens or hundreds of thousands of words. This approach assigns a unique token such as <code>[UNK]</code> to all out-of-vocabulary (OOV) words encountered at inference time, introducing brittleness and data sparsity, especially for morphologically rich languages.</p>
<p>Many natural languages exhibit complex morphology, where individual words encode multiple grammatical or semantic features. For example, in Swahili, a single verb such as <em>ambia</em> (“to tell”) can yield hundreds of morphologically inflected forms, incorporating tense, aspect, negation, mood, definiteness, and object agreement. Relying on full-word tokenization under such conditions results in large vocabularies with sparse frequency distributions, undermining the statistical efficiency of learned models.</p>
<h3 id="nlp-11_tokens-subword-units-as-a-solution">Subword Units as a Solution<a class="headerlink" href="#nlp-11_tokens-subword-units-as-a-solution" title="Permanent link">¶</a></h3>
<p>Subword modeling addresses the limitations of fixed-vocabulary word-based tokenization by decomposing words into smaller, more frequent units such as morphemes, syllables, or even individual characters. This approach enables:</p>
<ul>
<li>Generalization to unseen words via composition of known subword units</li>
<li>Mitigation of the OOV problem by modeling open vocabularies</li>
<li>Better cross-linguistic applicability, especially for agglutinative and polysynthetic languages</li>
<li>More compact and data-efficient language representations</li>
</ul>
<h3 id="nlp-11_tokens-byte-pair-encoding-bpe">Byte-Pair Encoding (BPE)<a class="headerlink" href="#nlp-11_tokens-byte-pair-encoding-bpe" title="Permanent link">¶</a></h3>
<p>A prominent algorithm for data-driven subword segmentation is Byte-Pair Encoding (BPE), originally adapted from data compression and later introduced to NLP in neural machine translation. The core idea is to iteratively learn a vocabulary of subword units by merging the most frequent pairs of adjacent symbols in a corpus. BPE constructs subword units by greedily merging frequent symbol pairs.</p>
<p>The BPE algorithm operates as follows:</p>
<ol>
<li>Initialize the vocabulary with all individual characters and a special end-of-word symbol (e.g., <code>_</code>)</li>
<li>Count all symbol pairs in the corpus and identify the most frequent pair</li>
<li>Merge the most frequent pair into a new symbol and update the corpus accordingly</li>
<li>Repeat steps 2–3 until the desired vocabulary size is reached</li>
</ol>
<p>This procedure yields a deterministic and greedy segmentation strategy, allowing both training and inference to tokenize consistently using the learned merge rules. BPE produces variable-length subword units that often correspond to linguistically meaningful morphemes (e.g., prefixes, stems, suffixes), while maintaining robustness across diverse scripts and languages.</p>
<p>To illustrate the utility of BPE in handling both common and rare or noisy words, consider the following tokenization outcomes using a hypothetical learned subword vocabulary:</p>
<table>
<thead>
<tr>
<th>Input Word</th>
<th>BPE Tokenization</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>hat</code></td>
<td><code>hat</code></td>
</tr>
<tr>
<td><code>learn</code></td>
<td><code>learn</code></td>
</tr>
<tr>
<td><code>taaaaasty</code></td>
<td><code>taa## aaa## sty</code></td>
</tr>
<tr>
<td><code>laern</code></td>
<td><code>la## ern</code></td>
</tr>
<tr>
<td><code>Transformerify</code></td>
<td><code>Transformer##ify</code></td>
</tr>
</tbody>
</table>
<p>The delimiter <code>##</code> indicates that a subword is not a standalone word but a continuation of a preceding segment. This example demonstrates several key advantages of subword modeling:</p>
<ul>
<li>Common words such as <code>hat</code> and <code>learn</code> are represented as single units</li>
<li>Misspellings or informal variants (e.g., <code>laern</code>, <code>taaaaasty</code>) can still be tokenized into interpretable components using existing subword units</li>
<li>Rare or novel words (e.g., <code>Transformerify</code>) are decomposed into known subwords, preserving semantic hints for downstream models</li>
</ul>
<h3 id="nlp-11_tokens-variants-and-modern-usage">Variants and Modern Usage<a class="headerlink" href="#nlp-11_tokens-variants-and-modern-usage" title="Permanent link">¶</a></h3>
<p>BPE has inspired several modern variants such as WordPiece and Unigram Language Model tokenization, which are widely used in state-of-the-art pretrained language models including BERT, GPT, and T5. These methods differ in how they select subword units, for example using likelihood-based criteria or probabilistic segmentation, but share the same underlying goal: modeling language compositionally below the word level.</p>
<p>Subword modeling constitutes a crucial innovation in modern NLP pipelines, striking a balance between granularity, efficiency, and linguistic fidelity.</p></body></html></section><section class="print-page" id="nlp-12_pretraining" heading-number="2.12"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="pre-training">Pre-training<a class="headerlink" href="#nlp-12_pretraining-pre-training" title="Permanent link">¶</a></h1>
<p>Pretraining refers to learning general-purpose language representations from large amounts of unlabeled text before adapting models to specific tasks. This approach decoupled lexical representation learning from task-specific modeling.</p>
<h2 id="nlp-12_pretraining-from-pretrained-embeddings-to-pretrained-models">From Pretrained Embeddings to Pretrained Models<a class="headerlink" href="#nlp-12_pretraining-from-pretrained-embeddings-to-pretrained-models" title="Permanent link">¶</a></h2>
<p>Before the rise of large-scale language models, pretraining in NLP primarily focused on static word embeddings such as Word2Vec, GloVe, or FastText. These embeddings provided fixed, context-independent vector representations for each word in the vocabulary, which were then used as input features for downstream models.</p>
<h3 id="nlp-12_pretraining-circa-2017-word-embeddings-contextual-models">Circa 2017: Word embeddings + contextual models<a class="headerlink" href="#nlp-12_pretraining-circa-2017-word-embeddings-contextual-models" title="Permanent link">¶</a></h3>
<p>A common pipeline during this period combined pretrained embeddings with supervised task-specific models.</p>
<ul>
<li>Begin with pretrained word embeddings (context-agnostic)</li>
<li>Learn contextualization during supervised training on a downstream task (e.g., sentiment analysis, question answering)</li>
<li>Recurrent architectures (e.g., LSTMs) or early Transformer variants were used to build task-specific context</li>
<li>Limitations:<ul>
<li>Downstream data must be large and diverse enough to teach the model language understanding</li>
<li>Most model parameters are randomly initialized and trained from scratch</li>
<li>Inefficient reuse of linguistic knowledge across tasks</li>
</ul>
</li>
</ul>
<p>As a result, much of the burden of language understanding was placed on downstream supervision.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/pre_old.png" data-desc-position="bottom"><img alt="Pretrained Embeddings" src="../nlp/images/pre_old.png"></a></p>
<h2 id="nlp-12_pretraining-modern-paradigm-pretraining-whole-models">Modern Paradigm: Pretraining Whole Models<a class="headerlink" href="#nlp-12_pretraining-modern-paradigm-pretraining-whole-models" title="Permanent link">¶</a></h2>
<p>In modern NLP systems, the dominant paradigm has shifted from pretraining isolated components (e.g., word embeddings) to pretraining entire models on unsupervised or self-supervised objectives over massive corpora. These pretrained models are then fine-tuned on specific tasks, often with limited labeled data.</p>
<p>Key characteristics of whole-model pretraining</p>
<ul>
<li>Nearly all model parameters are initialized using large-scale pretraining</li>
<li>The model is trained on unlabeled text by corrupting the input and optimizing for its reconstruction. </li>
<li>Common objectives: Different pretraining objectives reflect different modeling assumptions and downstream use cases.<ul>
<li>Masked language modeling (MLM): Randomly mask tokens and train the model to predict them (e.g., BERT)</li>
<li>Causal language modeling (CLM): Predict the next token given previous ones (e.g., GPT)</li>
<li>Permutation-based objectives: Learn to reason over non-sequential token orders (e.g., XLNet)</li>
</ul>
</li>
<li>Models learn:<ul>
<li>Deep, contextualized representations of language structure and semantics</li>
<li>Strong priors for downstream task learning via transfer</li>
<li>Coherent probability distributions over sequences, enabling sampling and generation. These learned representations can be rapidly adapted to new tasks through fine-tuning or prompting.</li>
</ul>
</li>
</ul>
<h2 id="nlp-12_pretraining-benefits-of-pretraining">Benefits of Pretraining<a class="headerlink" href="#nlp-12_pretraining-benefits-of-pretraining" title="Permanent link">¶</a></h2>
<p>Whole-model pretraining offers several practical and theoretical advantages.</p>
<ul>
<li>Data efficiency: Fine-tuning requires significantly fewer task-specific labeled examples</li>
<li>Robust generalization: Pretrained models capture syntactic, semantic, and world knowledge</li>
<li>Unified architecture: A single pretrained backbone can be reused across a wide range of tasks, reducing task-specific engineering</li>
<li>Sampling and generation: Language models trained with causal objectives can be used to generate fluent, coherent text</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/pre_new.png" data-desc-position="bottom"><img alt="Pretraining" src="../nlp/images/pre_new.png"></a></p>
<h2 id="nlp-12_pretraining-summary">Summary<a class="headerlink" href="#nlp-12_pretraining-summary" title="Permanent link">¶</a></h2>
<p>Pretraining has revolutionized NLP by shifting the burden of language understanding from downstream task data to large-scale, general-purpose models. This transition from static embeddings to pretrained Transformers has enabled rapid progress across virtually every NLP benchmark, making pretraining the foundation of modern language understanding and generation systems.</p></body></html></section><section class="print-page" id="nlp-13_pretrain_strat" heading-number="2.13"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="13-the-pretraining-strategies">13. The Pretraining Strategies<a class="headerlink" href="#nlp-13_pretrain_strat-13-the-pretraining-strategies" title="Permanent link">¶</a></h1>
<p>This section describes how pretraining objectives differ across encoder, decoder, and encoder–decoder architectures, and why these differences matter.</p>
<p>The pretraining/fine-tuning paradigm has become the dominant approach in modern NLP. Instead of training models from scratch for every task, we first train a general-purpose language model on a large, unlabeled corpus, and then adapt it to a specific task using a smaller labeled dataset. This approach enables models to transfer general linguistic knowledge learned during pretraining into a wide variety of downstream applications.</p>
<ul>
<li>Step 1: Pretraining — Train on a large-scale unsupervised objective such as language modeling. The model learns broad statistical regularities, syntax, semantics, and even some world knowledge from raw text</li>
<li>Step 2: Fine-tuning — Adapt the pretrained model to a specific supervised task (e.g., sentiment classification, question answering, named entity recognition) using a smaller, labeled dataset</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/pre_vs_fine.png" data-desc-position="bottom"><img alt="Pretraining and Fine Tuning" src="../nlp/images/pre_vs_fine.png"></a></p>
<p>These two stages decouple language understanding from task-specific supervision. This two-stage procedure has been remarkably effective, especially when labeled data is scarce, as the pretrained model already encodes a strong inductive bias about language.</p>
<h2 id="nlp-13_pretrain_strat-why-does-this-work-an-optimization-view">Why Does This Work? An Optimization View<a class="headerlink" href="#nlp-13_pretrain_strat-why-does-this-work-an-optimization-view" title="Permanent link">¶</a></h2>
<p>From the perspective of training neural networks with stochastic gradient descent, pretraining provides a highly informative initialization for model parameters.</p>
<p>Let <span class="arithmatex">\(\mathcal{L}_{\text{pretrain}}(\theta)\)</span> denote the pretraining loss, and let <span class="arithmatex">\(\hat{\theta}\)</span> be the parameters obtained by minimizing this loss:</p>
<div class="arithmatex">\[
\hat{\theta} \approx \arg\min_\theta \mathcal{L}_{\text{pretrain}}(\theta)
\]</div>
<p>During fine-tuning, we optimize a new task-specific loss <span class="arithmatex">\(\mathcal{L}_{\text{finetune}}(\theta)\)</span>, starting from the pretrained parameters:</p>
<div class="arithmatex">\[
\theta^* = \arg\min_\theta \mathcal{L}_{\text{finetune}}(\theta),
\quad \text{initialized at } \theta = \hat{\theta}
\]</div>
<p>This setup offers two complementary advantages:</p>
<ol>
<li>
<p>Good starting point:  The pretrained parameters <span class="arithmatex">\(\hat{\theta}\)</span> already encode general linguistic knowledge, meaning that gradient-based optimization during fine-tuning is more likely to converge quickly and to a good local minimum</p>
</li>
<li>
<p>Better generalization: Due to the geometry of the loss landscape, stochastic gradient descent tends to stay relatively close to <span class="arithmatex">\(\hat{\theta}\)</span>. If the local minima around <span class="arithmatex">\(\hat{\theta}\)</span> are well-aligned with generalization, then the fine-tuned model is more likely to perform well on unseen data</p>
</li>
</ol>
<h2 id="nlp-13_pretrain_strat-intuition-behind-gradient-propagation">Intuition Behind Gradient Propagation<a class="headerlink" href="#nlp-13_pretrain_strat-intuition-behind-gradient-propagation" title="Permanent link">¶</a></h2>
<p>Another benefit is that the gradients of the fine-tuning loss <span class="arithmatex">\(\nabla \mathcal{L}_{\text{finetune}}(\theta)\)</span> often propagate more effectively when <span class="arithmatex">\(\theta\)</span> is initialized at <span class="arithmatex">\(\hat{\theta}\)</span>. Pretraining shapes the model’s representations such that downstream gradients flow through semantically meaningful feature spaces, improving both optimization stability and sample efficiency.</p>
<h2 id="nlp-13_pretrain_strat-summary">Summary<a class="headerlink" href="#nlp-13_pretrain_strat-summary" title="Permanent link">¶</a></h2>
<p>The pretraining/fine-tuning paradigm represents a powerful instantiation of transfer learning in NLP. It leverages large-scale unsupervised data to produce models with rich linguistic priors and uses supervised fine-tuning to tailor those priors to task-specific objectives. From both empirical and theoretical perspectives, this strategy enables better generalization, faster convergence, and higher performance across nearly all areas of natural language understanding and generation.</p>
<h2 id="nlp-13_pretrain_strat-pretraining-encoder-architectures">Pretraining Encoder Architectures<a class="headerlink" href="#nlp-13_pretrain_strat-pretraining-encoder-architectures" title="Permanent link">¶</a></h2>
<p>Transformer-based encoder models, particularly those following the BERT architecture, are pretrained using objectives that leverage the bidirectional nature of attention. Unlike autoregressive language models, which condition only on past tokens, encoder architectures benefit from full left-and-right context, enabling richer and more globally informed token representations.</p>
<h2 id="nlp-13_pretrain_strat-masked-language-modeling-mlm">Masked Language Modeling (MLM)<a class="headerlink" href="#nlp-13_pretrain_strat-masked-language-modeling-mlm" title="Permanent link">¶</a></h2>
<p>The core idea behind encoder pretraining is the masked language modeling objective. In this setup, a fraction of the input tokens is replaced with a special <code>[MASK]</code> token. The model is then trained to predict the original tokens at these masked positions, conditioning on the surrounding unmasked context. Formally, given an input sequence  <span class="arithmatex">\(x = (w_1, w_2, \dots, w_T)\)</span><br>
and its corrupted version <span class="arithmatex">\(\tilde{x}\)</span>, the model learns parameters <span class="arithmatex">\(\theta\)</span> that maximize the likelihood <span class="arithmatex">\(p_\theta(x \mid \tilde{x})\)</span>.</p>
<div class="arithmatex">\[
\mathbf{h}_1, \dots, \mathbf{h}_T = \text{Encoder}(w_1, \dots, w_T)
\]</div>
<div class="arithmatex">\[
\hat{y}_i \sim \text{softmax}(A \mathbf{h}_i + b),
\quad \text{for masked positions } i
\]</div>
<p>where <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(b\)</span> are learned projection parameters.</p>
<p>The BERT pretraining procedure masks 15% of tokens according to the following strategy:</p>
<ul>
<li>80% of the time, the token is replaced with <code>[MASK]</code></li>
<li>10% of the time, it is replaced with a random token</li>
<li>10% of the time, it is left unchanged but still predicted</li>
</ul>
<p>This scheme prevents the model from overfitting to the presence of <code>[MASK]</code> tokens and encourages robust representations even for unmasked inputs.</p>
<h2 id="nlp-13_pretrain_strat-next-sentence-prediction-nsp">Next Sentence Prediction (NSP)<a class="headerlink" href="#nlp-13_pretrain_strat-next-sentence-prediction-nsp" title="Permanent link">¶</a></h2>
<p>In addition to masked language modeling, BERT was originally trained with a binary classification task known as next sentence prediction. Given two input segments, the model predicts whether the second segment follows the first in the original corpus or was randomly sampled. Subsequent studies such as RoBERTa showed that removing NSP can improve downstream performance, suggesting that NSP is not essential.</p>
<h2 id="nlp-13_pretrain_strat-advancements-and-variants">Advancements and Variants<a class="headerlink" href="#nlp-13_pretrain_strat-advancements-and-variants" title="Permanent link">¶</a></h2>
<p>Numerous refinements to the BERT pretraining methodology have been proposed:</p>
<ul>
<li>
<p>RoBERTa:   Trains longer, on more data, removes NSP, uses dynamic masking and larger batch sizes</p>
</li>
<li>
<p>SpanBERT:  Masks contiguous spans of tokens instead of individual ones, promoting span-level representations</p>
</li>
</ul>
<h2 id="nlp-13_pretrain_strat-limitations-of-encoder-only-pretraining">Limitations of Encoder-Only Pretraining<a class="headerlink" href="#nlp-13_pretrain_strat-limitations-of-encoder-only-pretraining" title="Permanent link">¶</a></h2>
<p>While encoder models like BERT excel at understanding and classification tasks (e.g., sentiment analysis, QA), they are not directly suited for sequence generation due to their non-autoregressive architecture. For tasks requiring fluent text generation (e.g., summarization, translation), decoder-based or encoder-decoder models are more appropriate. Nonetheless, pretrained encoders remain foundational across a wide array of NLP applications due to their strong contextual representations and adaptability to fine-tuning for downstream tasks.</p>
<h2 id="nlp-13_pretrain_strat-pretraining-encoderdecoder-architectures">Pretraining Encoder–Decoder Architectures<a class="headerlink" href="#nlp-13_pretrain_strat-pretraining-encoderdecoder-architectures" title="Permanent link">¶</a></h2>
<p>Encoder–decoder models combine the strengths of both architectures: the encoder produces rich, bidirectional representations of input sequences, while the decoder performs autoregressive generation conditioned on these representations. This architecture is particularly well-suited for sequence-to-sequence tasks such as machine translation, summarization, and question answering.</p>
<h2 id="nlp-13_pretrain_strat-why-use-encoderdecoder-models">Why Use Encoder–Decoder Models?<a class="headerlink" href="#nlp-13_pretrain_strat-why-use-encoderdecoder-models" title="Permanent link">¶</a></h2>
<ul>
<li>Encoders build contextual representations using bidirectional attention</li>
<li>Decoders enable autoregressive generation conditioned on encoded input and past outputs</li>
<li>Encoder–decoder models enable powerful conditional generation while leveraging deep understanding of the input.</li>
</ul>
<h2 id="nlp-13_pretrain_strat-pretraining-strategy-language-modeling-with-encoders">Pretraining Strategy: Language Modeling with Encoders<a class="headerlink" href="#nlp-13_pretrain_strat-pretraining-strategy-language-modeling-with-encoders" title="Permanent link">¶</a></h2>
<p>A naive extension of language modeling to encoder–decoder architectures splits the input sequence:</p>
<ul>
<li>A prefix of the input (e.g., tokens <span class="arithmatex">\(w_1, \dots, w_T\)</span>) is passed to the encoder.</li>
<li>The decoder autoregressively generates the continuation</li>
</ul>
<div class="arithmatex">\[
\mathbf{h}_{1:T} = \text{Encoder}(w_{1:T}), \quad
\mathbf{h}_{T+1:T'} = \text{Decoder}(w_{1:T'}, \mathbf{h}_{1:T})
\]</div>
<div class="arithmatex">\[
P(y_i) \sim \text{softmax}(\mathbf{A}\mathbf{h}_i + \mathbf{b}), \quad i &gt; T
\]</div>
<p>This allows the encoder to learn bidirectional features while enabling the decoder to condition on them during generation. However, more specialized objectives have shown better performance.</p>
<h2 id="nlp-13_pretrain_strat-span-corruption-the-t5-objective">Span Corruption: The T5 Objective<a class="headerlink" href="#nlp-13_pretrain_strat-span-corruption-the-t5-objective" title="Permanent link">¶</a></h2>
<p>Span corruption is the core pretraining objective of the T5 model. Random spans of text are removed from the input and replaced with unique sentinel tokens such as <code>&lt;extra_id_0&gt;</code>. The decoder is trained to reconstruct the missing spans.</p>
<ul>
<li>Random spans of text are removed from the input</li>
<li>Each removed span is replaced with a unique sentinel token (e.g., <code>&lt;extra_id_0&gt;</code>).</li>
<li>The model is trained to reconstruct the missing spans from these placeholders.</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/span_corruption.png" data-desc-position="bottom"><img alt="Span Corruption" src="../nlp/images/span_corruption.png"></a></p>
<p>Example:</p>
<ul>
<li>Input: <code>The quick &lt;extra_id_0&gt; fox jumps &lt;extra_id_1&gt; the lazy dog.</code></li>
<li>Target: <code>&lt;extra_id_0&gt; brown &lt;extra_id_1&gt; over &lt;extra_id_2&gt;</code></li>
</ul>
<p>Advantages:</p>
<ul>
<li>The encoder benefits from full bidirectional context (unlike standard causal models).</li>
<li>The decoder is trained autoregressively, generating spans conditioned on encoder output.</li>
<li>The objective is implemented via input preprocessing; the model learns a language modeling task at the decoder side.</li>
</ul>
<h2 id="nlp-13_pretrain_strat-summary_1">Summary<a class="headerlink" href="#nlp-13_pretrain_strat-summary_1" title="Permanent link">¶</a></h2>
<p>Encoder–decoder pretraining balances bidirectional representation learning (via the encoder) with generative capability (via the decoder). Span corruption, as implemented in T5, has emerged as a highly effective strategy. It produces models that generalize well across a wide range of NLP tasks and are compatible with the "text-to-text" paradigm.</p>
<h2 id="nlp-13_pretrain_strat-pretraining-decoder-architectures">Pretraining Decoder Architectures<a class="headerlink" href="#nlp-13_pretrain_strat-pretraining-decoder-architectures" title="Permanent link">¶</a></h2>
<p>Decoder-only language models have emerged as a central architecture in modern NLP, particularly for tasks involving natural language generation. These models are pretrained autoregressively to maximize the likelihood of the next token conditioned on previous tokens:</p>
<div class="arithmatex">\[
p_\theta(w_t \mid w_{1:t-1})
\]</div>
<p>typically using causal (left-to-right) self-attention to ensure that each token only attends to its left context. This setup enables decoders to learn rich, context-sensitive representations suitable for both generation and classification tasks.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/decoder_pre.png" data-desc-position="bottom"><img alt="Decoder Pretraining" src="../nlp/images/decoder_pre.png"></a></p>
<h2 id="nlp-13_pretrain_strat-representation-learning-for-classification">Representation Learning for Classification<a class="headerlink" href="#nlp-13_pretrain_strat-representation-learning-for-classification" title="Permanent link">¶</a></h2>
<p>Despite being trained for generation, pretrained decoders can be repurposed for classification tasks by leveraging their hidden representations. A common approach is to use the final token's hidden state <span class="arithmatex">\(h_T\)</span> as a summary of the sequence and apply a linear classifier on top:</p>
<div class="arithmatex">\[
h_1, \dots, h_T = \text{Decoder}(w_1, \dots, w_T)
\]</div>
<div class="arithmatex">\[
y \sim \text{softmax}(A h_T + b)
\]</div>
<p>where <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(b\)</span> are task-specific parameters initialized randomly and optimized during finetuning. Importantly, gradients are backpropagated through the entire decoder, enabling the model to adapt its internal representations to the downstream task.</p>
<h2 id="nlp-13_pretrain_strat-sequence-generation-via-pretrained-decoders">Sequence Generation via Pretrained Decoders<a class="headerlink" href="#nlp-13_pretrain_strat-sequence-generation-via-pretrained-decoders" title="Permanent link">¶</a></h2>
<p>In generation settings, decoder models are used directly in the autoregressive manner in which they were pretrained. At each timestep, the decoder produces a distribution over the next token given the previously generated tokens:</p>
<div class="arithmatex">\[
h_1, \dots, h_{t-1} = \text{Decoder}(w_1, \dots, w_{t-1})
\]</div>
<div class="arithmatex">\[
w_t \sim \text{softmax}(A h_{t-1} + b)
\]</div>
<p>Here, <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(b\)</span> represent the output projection matrix and bias that were learned during pretraining. In many applications, these parameters are reused without modification; however, they may also be further finetuned alongside the decoder, depending on the task and data availability.</p>
<p>This generation paradigm is particularly effective for tasks such as:</p>
<ul>
<li>Dialogue modeling: where the decoder generates a response conditioned on the prior dialogue history.</li>
<li>Summarization: where the decoder generates a summary conditioned on a document input.</li>
</ul>
<h2 id="nlp-13_pretrain_strat-architectural-context-and-transferability">Architectural Context and Transferability<a class="headerlink" href="#nlp-13_pretrain_strat-architectural-context-and-transferability" title="Permanent link">¶</a></h2>
<p>Decoder-only models differ from encoder-decoder architectures (e.g., T5) in that they do not encode the input with a separate encoder. Instead, all information is processed through the decoder itself. This simplicity enables direct reuse of the pretrained decoder for diverse tasks with minimal architectural modification.</p>
<p>Pretraining provides:</p>
<ul>
<li>Transferability: Learned representations encode rich syntactic, semantic, and factual knowledge that generalize well across tasks.</li>
<li>Architectural reuse: The same pretrained model can support both classification and generation, reducing deployment complexity.</li>
<li>Scalability: Autoregressive training benefits from large-scale data and scales effectively with model size.</li>
</ul>
<p>Pretrained decoder architectures, as used in models such as GPT-2, GPT-3, and LLaMA, have demonstrated remarkable performance across a wide range of NLP benchmarks, confirming the utility of this simple yet powerful design.</p></body></html></section><section class="print-page" id="nlp-14_finetuning" heading-number="2.14"><h1 id="nlp-14_finetuning-nlp-14_finetuning">14. Fine-tuning</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="fine-tuning-and-parameter-efficient-adaptation">Fine-tuning and Parameter-Efficient Adaptation<a class="headerlink" href="#nlp-14_finetuning-fine-tuning-and-parameter-efficient-adaptation" title="Permanent link">¶</a></h2>
<p>The standard adaptation strategy for pretrained language models involves updating all model parameters on a task-specific dataset. This approach is typically effective but computationally expensive.</p>
<ul>
<li>Method: Unfreeze and update all weights during training</li>
<li>Pros:<ul>
<li>Strong performance across tasks</li>
<li>Deep integration of task-specific signal</li>
</ul>
</li>
<li>Cons:<ul>
<li>High memory and compute cost</li>
<li>Risk of overfitting, especially in low-resource settings</li>
<li>Each task or domain requires storing a full model copy</li>
</ul>
</li>
</ul>
<h2 id="nlp-14_finetuning-parameter-efficient-fine-tuning-peft">Parameter-Efficient Fine-tuning (PEFT)<a class="headerlink" href="#nlp-14_finetuning-parameter-efficient-fine-tuning-peft" title="Permanent link">¶</a></h2>
<p>PEFT methods aim to adapt large language models by updating only a small fraction of the parameters, or by introducing new lightweight modules, while keeping the backbone frozen.</p>
<ul>
<li>Reduce training time and GPU memory usage</li>
<li>Support multi-task and multi-user deployment</li>
<li>Preserve generalization by avoiding full weight drift</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/pfet.png" data-desc-position="bottom"><img alt="PEFT" src="../nlp/images/pfet.png"></a></p>
<h2 id="nlp-14_finetuning-prompt-tuning">Prompt Tuning<a class="headerlink" href="#nlp-14_finetuning-prompt-tuning" title="Permanent link">¶</a></h2>
<p>Prompt tuning learns a set of continuous embeddings (soft prompts) that are prepended to the input. The rest of the model remains frozen.</p>
<ul>
<li>Method: Optimize only a small set of input-level vectors</li>
<li>Pros:<ul>
<li>Extremely lightweight (less than 0.1% of total parameters)</li>
<li>Easy to implement</li>
</ul>
</li>
<li>Cons:<ul>
<li>Unstable for small models</li>
<li>Performance gap compared to other PEFT methods on many tasks</li>
</ul>
</li>
</ul>
<h2 id="nlp-14_finetuning-prefix-tuning">Prefix Tuning<a class="headerlink" href="#nlp-14_finetuning-prefix-tuning" title="Permanent link">¶</a></h2>
<p>Prefix tuning prepends trainable vectors to the key and value matrices of each Transformer layer, guiding attention.</p>
<ul>
<li>Method: Learn prefix vectors while freezing the pretrained model</li>
<li>Pros:<ul>
<li>Efficient (approximately 1–2% of total parameters)</li>
<li>Stronger performance than prompt tuning on many tasks</li>
<li>Task-specific prefixes allow batched inference across tasks</li>
</ul>
</li>
<li>Cons:<ul>
<li>More intrusive implementation than prompt tuning</li>
</ul>
</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/pretraining_1.png" data-desc-position="bottom"><img alt="Prefix Tuning" src="../nlp/images/pretraining_1.png"></a></p>
<h2 id="nlp-14_finetuning-adapter-tuning">Adapter Tuning<a class="headerlink" href="#nlp-14_finetuning-adapter-tuning" title="Permanent link">¶</a></h2>
<p>Adapters are small bottleneck layers inserted within each Transformer block. Only adapter parameters are trained.</p>
<ul>
<li>Method: Add adapter layers after attention and after the feed-forward network, while freezing the base model</li>
<li>Pros:<ul>
<li>Competitive performance on many NLP tasks</li>
<li>Modular design allows composition for multi-task learning</li>
</ul>
</li>
<li>Cons:<ul>
<li>Slightly larger parameter footprint than other PEFT methods</li>
<li>Requires architecture modification</li>
</ul>
</li>
</ul>
<h2 id="nlp-14_finetuning-low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)<a class="headerlink" href="#nlp-14_finetuning-low-rank-adaptation-lora" title="Permanent link">¶</a></h2>
<p>LoRA learns a low-rank decomposition of the difference between pretrained and fine-tuned weights.</p>
<ul>
<li>Method: For a linear layer <span class="arithmatex">\(W \in \mathbb{R}^{d \times k}\)</span>, learn <span class="arithmatex">\(\Delta W = A B\)</span> where <span class="arithmatex">\(A \in \mathbb{R}^{d \times r}\)</span> and <span class="arithmatex">\(B \in \mathbb{R}^{r \times k}\)</span>, with <span class="arithmatex">\(r \ll \min(d, k)\)</span></li>
<li>Pros:<ul>
<li>Highly parameter-efficient (typically less than 1%)</li>
<li>Stable training and strong task performance</li>
<li>Easy to integrate into existing models</li>
</ul>
</li>
<li>Cons:<ul>
<li>Adds a small compute overhead at inference time</li>
</ul>
</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/pretraining_2.png" data-desc-position="bottom"><img alt="LoRA" src="../nlp/images/pretraining_2.png"></a></p>
<p>Location of LoRA:</p>
<ul>
<li>Originally proposed in masked multi-head attention matrices.</li>
<li>Nowadays recommended in the feed-forward layers (after attention)</li>
</ul>
<p>Empirical observations with LoRA:</p>
<ul>
<li>LoRA needs a higher learning rate than full fine-tuning</li>
<li>LoRA does poorly on large batch size compared to full fine-tuning</li>
</ul>
<h3 id="nlp-14_finetuning-quantized-lora">Quantized LoRA<a class="headerlink" href="#nlp-14_finetuning-quantized-lora" title="Permanent link">¶</a></h3>
<p>QLoRA trains low-rank adapters (LoRA) on top of a 4-bit quantized frozen LLM, keeping the base weights memory-efficient while still allowing learning. The implementation stores the base model in 4-bit NF4 precision and only updates small <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span> matrices (LoRA matrices) in full precision. </p>
<p>Intuition: freeze most of the model to save memory, and let tiny trainable adapters learn task-specific changes without touching the huge pretrained weights.</p>
<h2 id="nlp-14_finetuning-empirical-comparison-of-fine-tuning-methods">Empirical Comparison of Fine-tuning Methods<a class="headerlink" href="#nlp-14_finetuning-empirical-comparison-of-fine-tuning-methods" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Trainable Params</th>
<th>GPU Memory</th>
<th>Performance</th>
<th>Stability</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full Fine-tuning</td>
<td>100%</td>
<td>High</td>
<td>Strong</td>
<td>Stable</td>
</tr>
<tr>
<td>Prompt Tuning</td>
<td>&lt;0.1%</td>
<td>Very Low</td>
<td>Moderate–Low</td>
<td>Unstable (small models)</td>
</tr>
<tr>
<td>Prefix Tuning</td>
<td>~1–2%</td>
<td>Low</td>
<td>Moderate–High</td>
<td>Stable</td>
</tr>
<tr>
<td>Adapter Tuning</td>
<td>~1–5%</td>
<td>Moderate</td>
<td>High</td>
<td>Stable</td>
</tr>
<tr>
<td>LoRA</td>
<td>~0.1–1%</td>
<td>Low</td>
<td>High</td>
<td>Stable</td>
</tr>
</tbody>
</table>
<p>Parameter-efficient fine-tuning strategies provide practical tools for adapting large language models without retraining or duplicating the full network. LoRA, adapters, and prefix tuning are currently among the most effective approaches, with trade-offs between memory usage, ease of integration, and task performance. As the size and scope of language models continue to grow, PEFT methods will be essential for scaling personalization, domain adaptation, and multi-task inference.</p></body></html></section><section class="print-page" id="nlp-15_cot" heading-number="2.15"><h1 id="nlp-15_cot-nlp-15_cot">15. Chain of Thought Reasoning</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="chain-of-thought-reasoning">Chain of Thought Reasoning<a class="headerlink" href="#nlp-15_cot-chain-of-thought-reasoning" title="Permanent link">¶</a></h2>
<p>While decoder-only models are powerful text generators, they can also exhibit complex reasoning behavior when prompted effectively. One prominent prompting technique is chain of thought (CoT) prompting, which encourages the model to generate intermediate reasoning steps before arriving at a final answer.</p>
<p>Unlike direct answer prompting, CoT decomposes complex problems into step-by-step explanations, aligning better with how the model internally represents procedural and logical information. This is particularly beneficial in tasks such as arithmetic reasoning, commonsense inference, and symbolic manipulation.</p>
<h3 id="nlp-15_cot-standard-prompt-vs-chain-of-thought-prompt">Standard Prompt vs. Chain of Thought Prompt<a class="headerlink" href="#nlp-15_cot-standard-prompt-vs-chain-of-thought-prompt" title="Permanent link">¶</a></h3>
<p>Consider the task of solving a multi-step math problem. A standard prompt might look like:</p>
<blockquote>
<p>Q: If there are 3 cars and each car has 4 wheels, how many wheels are there in total?<br>
A:</p>
</blockquote>
<p>A model may correctly answer with <code>12</code>, but it might also fail if it tries to answer without reasoning explicitly.</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/cot.png" data-desc-position="bottom"><img alt="Chain of Thought" src="../nlp/images/cot.png"></a></p>
<p>In contrast, a chain of thought prompt elicits intermediate steps:</p>
<blockquote>
<p>Q: If there are 3 cars and each car has 4 wheels, how many wheels are there in total?<br>
A: Each car has 4 wheels. There are 3 cars. So the total number of wheels is <span class="arithmatex">\(3 \times 4 = 12\)</span>. The answer is 12.</p>
</blockquote>
<h3 id="nlp-15_cot-benefits-of-chain-of-thought-prompting">Benefits of Chain of Thought Prompting<a class="headerlink" href="#nlp-15_cot-benefits-of-chain-of-thought-prompting" title="Permanent link">¶</a></h3>
<ul>
<li>Improved accuracy: CoT often leads to significantly higher accuracy on reasoning-heavy tasks, particularly in few-shot or zero-shot settings  </li>
<li>Interpretable outputs: Intermediate steps provide insights into model behavior, making it easier to debug or assess alignment  </li>
<li>Alignment with human reasoning: CoT mimics how humans solve problems, which can be useful in educational or assistive applications  </li>
</ul>
<h3 id="nlp-15_cot-model-compatibility">Model Compatibility<a class="headerlink" href="#nlp-15_cot-model-compatibility" title="Permanent link">¶</a></h3>
<p>Chain of thought prompting has shown the most benefit in large decoder-only models such as GPT-2 and GPT-4. These models have sufficient capacity to maintain coherent multistep reasoning traces when prompted appropriately.</p></body></html></section><section class="print-page" id="nlp-16_nlg" heading-number="2.16"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="natural-language-generation">Natural Language Generation<a class="headerlink" href="#nlp-16_nlg-natural-language-generation" title="Permanent link">¶</a></h1>
<p>Natural Language Generation (NLG) is one of the two primary components of Natural Language Processing (NLP), the other being Natural Language Understanding (NLU). In simple terms:</p>
<div class="arithmatex">\[
\text{NLP} = \text{NLU} + \text{NLG}
\]</div>
<p>While NLU focuses on interpreting and extracting meaning from human language, NLG is concerned with generating coherent, fluent, and contextually appropriate text intended for human readers.</p>
<p>Recent advances in deep learning have significantly improved the quality and capabilities of NLG systems, enabling them to produce natural-sounding language across a wide range of applications.</p>
<h2 id="nlp-16_nlg-applications-of-natural-language-generation">Applications of Natural Language Generation<a class="headerlink" href="#nlp-16_nlg-applications-of-natural-language-generation" title="Permanent link">¶</a></h2>
<p>NLG plays a central role in many real-world applications, including:</p>
<ul>
<li>
<p>Machine translation (MT):</p>
<ul>
<li>Input: Text in a source language</li>
<li>Output: Translated text in a target language</li>
</ul>
</li>
<li>
<p>Dialogue systems / digital assistants:</p>
<ul>
<li>Input: Dialogue history or user queries</li>
<li>Output: Text that appropriately continues the conversation</li>
</ul>
</li>
<li>
<p>Text summarization:</p>
<ul>
<li>Input: Long-form documents (e.g., research papers, emails, meeting transcripts)</li>
<li>Output: Concise and coherent summaries</li>
</ul>
</li>
<li>
<p>Creative writing and story generation</p>
</li>
<li>
<p>Data-to-text generation:</p>
<ul>
<li>Converts structured data (e.g., tables, databases) into natural language reports or descriptions</li>
</ul>
</li>
<li>
<p>Image and video captioning:</p>
<ul>
<li>Generates natural language descriptions based on visual input</li>
</ul>
</li>
</ul>
<h2 id="nlp-16_nlg-basics-of-natural-language-generation">Basics of Natural Language Generation<a class="headerlink" href="#nlp-16_nlg-basics-of-natural-language-generation" title="Permanent link">¶</a></h2>
<p>Most modern NLG systems rely on autoregressive language models, which generate text one token at a time. At each time step <span class="arithmatex">\(t\)</span>, the model receives the preceding sequence of tokens <span class="arithmatex">\(x_{&lt;t}\)</span> and predicts the next token <span class="arithmatex">\(x_t\)</span>.</p>
<p>Let <span class="arithmatex">\(f(\cdot)\)</span> denote the model and <span class="arithmatex">\(V\)</span> be the vocabulary. For a given context <span class="arithmatex">\(x_{&lt;t}\)</span>, the model computes a score <span class="arithmatex">\(s_v = f(x_{&lt;t}, v) \in \mathbb{R}\)</span> for each token <span class="arithmatex">\(v \in V\)</span>. These scores are converted into probabilities via the softmax function:</p>
<div class="arithmatex">\[
P(x_t = v \mid x_{&lt;t})
=
\frac{\exp(s_v)}{\sum_{v' \in V} \exp(s_{v'})}
\]</div>
<h3 id="nlp-16_nlg-architectures">Architectures<a class="headerlink" href="#nlp-16_nlg-architectures" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Non-open-ended tasks (e.g., machine translation):<br>
  Typically use an encoder-decoder architecture:</p>
<ul>
<li>The encoder (often bidirectional) processes the input</li>
<li>The decoder (autoregressive) generates output one token at a time</li>
</ul>
</li>
<li>
<p>Open-ended tasks (e.g., story generation):<br>
  May rely solely on an autoregressive decoder without a separate encoder</p>
</li>
</ul>
<h3 id="nlp-16_nlg-training-objective">Training Objective<a class="headerlink" href="#nlp-16_nlg-training-objective" title="Permanent link">¶</a></h3>
<p>Models are trained using maximum likelihood estimation (MLE), which aims to maximize the probability of each token in the training sequence given its preceding context.</p>
<p>This is a token-level classification problem and is often trained using a method known as teacher forcing, where at each time step the model receives the true previous token instead of its own prediction. This aligns training conditions with the ground truth.</p></body></html></section><section class="print-page" id="nlp-17_imp_nlg" heading-number="2.17"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="improving-training-and-generation-for-nlg">Improving Training and Generation for NLG<a class="headerlink" href="#nlp-17_imp_nlg-improving-training-and-generation-for-nlg" title="Permanent link">¶</a></h1>
<h2 id="nlp-17_imp_nlg-improving-decoding-in-language-generation">Improving Decoding in Language Generation<a class="headerlink" href="#nlp-17_imp_nlg-improving-decoding-in-language-generation" title="Permanent link">¶</a></h2>
<p>Decoding strategies play a crucial role in transforming a model’s token probabilities into coherent and high-quality output. Below are commonly used techniques and strategies to improve decoding.</p>
<h3 id="nlp-17_imp_nlg-greedy-decoding">Greedy Decoding<a class="headerlink" href="#nlp-17_imp_nlg-greedy-decoding" title="Permanent link">¶</a></h3>
<ul>
<li>At each time step, select the token with the highest probability:
<script type="math/tex; mode=display">
\hat{y}_t = \arg\max_{w \in V} P(w \mid y_{<t})
</script>
</li>
<li>Simple and fast, but may yield repetitive or suboptimal text due to lack of exploration.</li>
</ul>
<h3 id="nlp-17_imp_nlg-beam-search">Beam Search<a class="headerlink" href="#nlp-17_imp_nlg-beam-search" title="Permanent link">¶</a></h3>
<ul>
<li>Keeps the top-<span class="arithmatex">\(k\)</span> most likely partial sequences (beams) at each time step.</li>
<li>Aims to maximize the overall sequence log-probability:
<script type="math/tex; mode=display">
\text{Score}(\hat{y}_{1:t}) = \sum_{i=1}^{t} \log P(\hat{y}_i \mid \hat{y}_{<i})
</script>
</li>
<li>Length normalization is often applied to prevent a bias toward shorter sequences:
<script type="math/tex; mode=display">
\text{Score}_{\text{norm}}(\hat{y}_{1:t}) =
\frac{1}{t^\alpha} \sum_{i=1}^{t} \log P(\hat{y}_i \mid \hat{y}_{<i}),
\quad \alpha \in [0, 1]
</script>
</li>
</ul>
<h3 id="nlp-17_imp_nlg-sampling-based-methods">Sampling-Based Methods<a class="headerlink" href="#nlp-17_imp_nlg-sampling-based-methods" title="Permanent link">¶</a></h3>
<h4 id="nlp-17_imp_nlg-top-k-sampling">Top-<span class="arithmatex">\(k\)</span> Sampling<a class="headerlink" href="#nlp-17_imp_nlg-top-k-sampling" title="Permanent link">¶</a></h4>
<ul>
<li>At each time step, restrict sampling to the top-<span class="arithmatex">\(k\)</span> tokens in the probability distribution.</li>
<li>Higher <span class="arithmatex">\(k\)</span> increases output diversity but may introduce incoherence.</li>
<li>Lower <span class="arithmatex">\(k\)</span> leads to safer, more predictable outputs.</li>
</ul>
<h4 id="nlp-17_imp_nlg-top-p-nucleus-sampling">Top-<span class="arithmatex">\(p\)</span> (Nucleus) Sampling<a class="headerlink" href="#nlp-17_imp_nlg-top-p-nucleus-sampling" title="Permanent link">¶</a></h4>
<ul>
<li>Dynamically selects the smallest set of tokens whose cumulative probability exceeds a threshold <span class="arithmatex">\(p\)</span>:
<script type="math/tex; mode=display">
\sum_{w \in V_p} P(w \mid y_{<t}) \geq p
</script>
</li>
<li>More adaptive than top-<span class="arithmatex">\(k\)</span>, especially when token probability mass is unevenly distributed.</li>
</ul>
<h3 id="nlp-17_imp_nlg-temperature-scaling">Temperature Scaling<a class="headerlink" href="#nlp-17_imp_nlg-temperature-scaling" title="Permanent link">¶</a></h3>
<ul>
<li>Temperature controls the sharpness of the softmax distribution:
<script type="math/tex; mode=display">
P_{\tau}(y_t = w)
=
\frac{\exp(s_w / \tau)}{\sum_{w' \in V} \exp(s_{w'} / \tau)}
</script>
</li>
<li><span class="arithmatex">\(\tau &gt; 1\)</span>: flattens the distribution, increasing randomness and diversity.</li>
<li><span class="arithmatex">\(\tau &lt; 1\)</span>: sharpens the distribution, making outputs more deterministic.</li>
</ul>
<h3 id="nlp-17_imp_nlg-re-ranking-generated-sequences">Re-ranking Generated Sequences<a class="headerlink" href="#nlp-17_imp_nlg-re-ranking-generated-sequences" title="Permanent link">¶</a></h3>
<ul>
<li>Decode multiple candidate sequences (e.g., 10), then rank them by a quality metric.</li>
<li>Perplexity is a common metric but often favors generic or repetitive text due to high token likelihoods.</li>
<li>Advanced re-rankers may consider:<ul>
<li>Style</li>
<li>Discourse coherence</li>
<li>Factuality and entailment</li>
<li>Logical consistency</li>
</ul>
</li>
<li>Multiple re-rankers may be combined, but calibration issues can arise.</li>
</ul>
<h2 id="nlp-17_imp_nlg-improving-training-for-natural-language-generation-models">Improving Training for Natural Language Generation Models<a class="headerlink" href="#nlp-17_imp_nlg-improving-training-for-natural-language-generation-models" title="Permanent link">¶</a></h2>
<p>Training natural language generation (NLG) models with maximum likelihood estimation (MLE) using teacher forcing is effective but suffers from critical limitations. This section outlines the central issues, mitigation techniques, and advanced strategies used in both academic and production systems.</p>
<h3 id="nlp-17_imp_nlg-exposure-bias">Exposure Bias<a class="headerlink" href="#nlp-17_imp_nlg-exposure-bias" title="Permanent link">¶</a></h3>
<ul>
<li>During training, the model conditions on ground-truth tokens <span class="arithmatex">\(y_{&lt;t}^*\)</span>; at test time, it conditions on its own previous predictions <span class="arithmatex">\(\hat{y}_{&lt;t}\)</span>.</li>
<li>This mismatch can cause errors to compound—early mistakes can derail generation.</li>
</ul>
<p>The standard teacher forcing objective is:
<script type="math/tex; mode=display">
\mathcal{L}_{\text{MLE}} =
- \sum_{t=1}^{T} \log P_\theta(y_t^* \mid y_{<t}^*)
</script>
</p>
<h3 id="nlp-17_imp_nlg-scheduled-sampling">Scheduled Sampling<a class="headerlink" href="#nlp-17_imp_nlg-scheduled-sampling" title="Permanent link">¶</a></h3>
<ul>
<li>With probability <span class="arithmatex">\(p\)</span>, the model uses its own previous prediction instead of the ground-truth token.</li>
<li><span class="arithmatex">\(p\)</span> increases over training to gradually expose the model to its own distribution.</li>
<li>However, this violates the assumption that training inputs are independent of predictions, leading to inconsistent gradients and potential instability</li>
</ul>
<h3 id="nlp-17_imp_nlg-dataset-aggregation">Dataset Aggregation<a class="headerlink" href="#nlp-17_imp_nlg-dataset-aggregation" title="Permanent link">¶</a></h3>
<ul>
<li>Periodically generate sequences using the current model.</li>
<li>Add these self-generated sequences to the training set.</li>
<li>The model learns to correct its own outputs over time.</li>
</ul>
<h3 id="nlp-17_imp_nlg-retrieval-augmented-generation">Retrieval-Augmented Generation<a class="headerlink" href="#nlp-17_imp_nlg-retrieval-augmented-generation" title="Permanent link">¶</a></h3>
<ul>
<li>Retrieve prototype sentences from a corpus.</li>
<li>Two common paradigms:<ul>
<li>Edit-based generation: learn to modify retrieved examples.</li>
<li>Search-and-generate: use retrieval as a prompt (e.g., KNN-LM, RAG, RETRO).</li>
</ul>
</li>
<li>Grounding generation in human-written text improves fluency and informativeness.</li>
</ul>
<h3 id="nlp-17_imp_nlg-reinforcement-learning-for-text-generation">Reinforcement Learning for Text Generation<a class="headerlink" href="#nlp-17_imp_nlg-reinforcement-learning-for-text-generation" title="Permanent link">¶</a></h3>
<ul>
<li>Model generation as a Markov Decision Process:<ul>
<li>States: current context representation</li>
<li>Actions: next-token choices</li>
<li>Policy: decoder distribution</li>
<li>Rewards: external scalar signal (e.g., BLEU)</li>
</ul>
</li>
<li>Objective: maximize expected total reward:
<script type="math/tex; mode=display">
\mathbb{E}_{\pi_\theta} \left[ \sum_{t=1}^{T} r_t \right]
</script>
</li>
<li>Methods include REINFORCE, actor-critic, and policy gradients.</li>
</ul>
<h4 id="nlp-17_imp_nlg-limitations-of-reinforcement-learning">Limitations of Reinforcement Learning<a class="headerlink" href="#nlp-17_imp_nlg-limitations-of-reinforcement-learning" title="Permanent link">¶</a></h4>
<ul>
<li>Sample inefficiency and high gradient variance.</li>
<li>Difficult credit assignment over long sequences.</li>
<li>Risk of overfitting the reward metric rather than true quality.</li>
</ul>
<h3 id="nlp-17_imp_nlg-defining-reward-functions">Defining Reward Functions<a class="headerlink" href="#nlp-17_imp_nlg-defining-reward-functions" title="Permanent link">¶</a></h3>
<p>Common automatic metrics include:
- BLEU for machine translation
- ROUGE for summarization
- CIDEr and SPIDEr for image captioning</p>
<p>These metrics are proxies and may correlate poorly with human judgment.</p>
<h3 id="nlp-17_imp_nlg-rewarding-specific-behaviors">Rewarding Specific Behaviors<a class="headerlink" href="#nlp-17_imp_nlg-rewarding-specific-behaviors" title="Permanent link">¶</a></h3>
<p>Reinforcement learning can target fine-grained objectives:
- Politeness
- Simplicity
- Temporal coherence
- Cross-modality alignment
- Formality
- Human preference via reinforcement learning from human feedback (RLHF)</p>
<h3 id="nlp-17_imp_nlg-training-takeaways">Training Takeaways<a class="headerlink" href="#nlp-17_imp_nlg-training-takeaways" title="Permanent link">¶</a></h3>
<ul>
<li>Teacher forcing is standard but introduces exposure bias.</li>
<li>Mitigation strategies include scheduled sampling, dataset aggregation, retrieval augmentation, and reinforcement learning.</li>
<li>Reinforcement learning is powerful but expensive and should be used when desired behaviors are not captured by likelihood-based training.</li>
<li>Human feedback and reward modeling are central to aligning modern language models with user preferences.</li>
</ul></body></html></section><section class="print-page" id="nlp-18_eval_nlu" heading-number="2.18"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="evaluating-nlu-systems">Evaluating NLU Systems<a class="headerlink" href="#nlp-18_eval_nlu-evaluating-nlu-systems" title="Permanent link">¶</a></h1>
<p>Evaluating the performance of Natural Language Understanding (NLU) systems requires standardized benchmarks and carefully chosen evaluation metrics. Unlike open-ended generation tasks, NLU tasks are typically close-ended, meaning they have a limited and well-defined set of correct outputs—such as binary, categorical, or span-based labels.</p>
<p>These tasks lend themselves to automatic evaluation using standard supervised learning techniques, making them particularly well-suited for large-scale benchmarking. Their constrained output space allows for clear-cut metrics such as accuracy, precision, recall, and F1-score, enabling reproducible and objective comparisons across models.</p>
<h3 id="nlp-18_eval_nlu-common-close-ended-nlp-tasks-and-benchmarks">Common Close-ended NLP Tasks and Benchmarks<a class="headerlink" href="#nlp-18_eval_nlu-common-close-ended-nlp-tasks-and-benchmarks" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Sentiment Analysis: Classify text by sentiment polarity (positive, negative, neutral).</p>
<ul>
<li>Benchmarks: SST-2, IMDB, Yelp Reviews</li>
</ul>
</li>
<li>
<p>Natural Language Inference (NLI): Determine whether a hypothesis is entailed, contradicted by, or neutral to a premise.</p>
<ul>
<li>Benchmarks: SNLI, MultiNLI</li>
</ul>
</li>
<li>
<p>Named Entity Recognition (NER): Identify and classify named entities (e.g., people, organizations, locations) in text.</p>
<ul>
<li>Benchmark: CoNLL-2003</li>
</ul>
</li>
<li>
<p>Part-of-Speech (POS) Tagging: Assign syntactic categories to each token in a sentence.</p>
<ul>
<li>Benchmark: Penn Treebank (PTB)</li>
</ul>
</li>
<li>
<p>Coreference Resolution: Identify mentions in text that refer to the same entity.</p>
<ul>
<li>Benchmark: Winograd Schema Challenge (WSC)</li>
</ul>
</li>
<li>
<p>Question Answering (QA): Answer factual questions based on a passage of text.</p>
<ul>
<li>Benchmark: SQuAD v2.0 (includes unanswerable questions)</li>
</ul>
</li>
<li>
<p>Multi-task Benchmarking:</p>
<ul>
<li>SuperGLUE is a prominent multi-task benchmark for close-ended evaluation. It extends GLUE by including harder tasks with a focus on deeper linguistic and reasoning skills.</li>
</ul>
</li>
</ul>
<h3 id="nlp-18_eval_nlu-evaluation-metrics">Evaluation Metrics<a class="headerlink" href="#nlp-18_eval_nlu-evaluation-metrics" title="Permanent link">¶</a></h3>
<p>The choice of evaluation metric depends on the nature of the task and class balance:</p>
<ul>
<li>
<p>Accuracy: Proportion of correct predictions. Suitable for balanced datasets with a single label per instance.</p>
</li>
<li>
<p>Precision / Recall / F1-score: Useful in imbalanced settings.</p>
<ul>
<li>Precision: Ratio of true positives to all predicted positives.</li>
<li>Recall: Ratio of true positives to all actual positives.</li>
<li>F1-score: Harmonic mean of precision and recall.</li>
</ul>
</li>
<li>
<p>ROC-AUC: Area under the receiver operating characteristic curve. Appropriate for binary classifiers, especially in imbalanced settings.</p>
</li>
</ul>
<p>Aggregating Performance Across Tasks or Metrics:<br>
For multi-task evaluations (e.g., SuperGLUE), scores across tasks may be aggregated by computing a macro-average or weighted average of individual task metrics. However, task heterogeneity can make aggregate scoring misleading without task-level inspection.</p>
<h2 id="nlp-18_eval_nlu-evaluation-challenges">Evaluation Challenges<a class="headerlink" href="#nlp-18_eval_nlu-evaluation-challenges" title="Permanent link">¶</a></h2>
<ul>
<li>
<p>Source of Labels: Many benchmarks rely on crowd-sourced annotations, which can introduce label noise and inconsistencies. It's important to consider inter-annotator agreement and annotation protocols.</p>
</li>
<li>
<p>Spurious Correlations: Models often exploit superficial patterns in training data (e.g., specific lexical cues) that do not generalize to out-of-distribution examples. This can result in overestimated performance on test sets that share similar artifacts.</p>
</li>
<li>
<p>Benchmark Saturation: Many established benchmarks (e.g., GLUE, SQuAD) have been nearly saturated by large models. Performance gains on these datasets may no longer reflect true improvements in generalization or reasoning.</p>
</li>
<li>
<p>Generalization vs Memorization: Close-ended evaluations often do not measure generalization under distribution shift. It is critical to supplement them with robustness tests or adversarial datasets.</p>
</li>
</ul>
<h2 id="nlp-18_eval_nlu-summary">Summary<a class="headerlink" href="#nlp-18_eval_nlu-summary" title="Permanent link">¶</a></h2>
<p>Close-ended evaluations remain a cornerstone of NLP model assessment due to their scalability and reliability. They are particularly useful for low-level tasks (e.g., tagging, classification) and for benchmarking multi-task generalization. However, their automatic nature can obscure deeper limitations in model reasoning, generalization, and alignment with real-world goals. Careful metric selection, robust dataset construction, and task-specific error analysis remain essential.</p></body></html></section><section class="print-page" id="nlp-19_eval_nlg" heading-number="2.19"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="evaluating-nlg-systems">Evaluating NLG Systems<a class="headerlink" href="#nlp-19_eval_nlg-evaluating-nlg-systems" title="Permanent link">¶</a></h1>
<p>Evaluation of Natural Language Generation (NLG) systems presents unique challenges due to the open-ended nature of their outputs. Unlike NLU tasks, where there is typically a single correct answer or label, NLG tasks often involve generating long-form text with many plausible outputs. As a result, standard machine learning metrics like accuracy or F1-score are not directly applicable.</p>
<p>In open-ended tasks, the quality of a generation cannot be assessed solely based on correctness, but rather on a spectrum of qualitative dimensions such as fluency, coherence, relevance, and factuality. Multiple responses may be acceptable to varying degrees, making evaluation inherently more subjective.</p>
<p>Common NLG tasks include:</p>
<ul>
<li>Summarization: CNN/DailyMail, Gigaword  </li>
<li>Machine Translation: WMT (Workshop on Machine Translation)  </li>
<li>Instruction-following and dialogue: Chatbot Arena, AlpacaEval, MT-Bench  </li>
</ul>
<p>Evaluation of Natural Language Generation (NLG) systems is challenging due to the diverse forms and goals of generated text. Evaluation methods can be categorized into three primary types:</p>
<ol>
<li>Content Overlap Metrics  </li>
<li>Model-Based Metrics  </li>
<li>Human Evaluations  </li>
</ol>
<h2 id="nlp-19_eval_nlg-1-content-overlap-metrics">1. Content Overlap Metrics<a class="headerlink" href="#nlp-19_eval_nlg-1-content-overlap-metrics" title="Permanent link">¶</a></h2>
<p>These metrics assess the lexical similarity between generated text and reference (gold-standard) outputs. They rely on <span class="arithmatex">\(n\)</span>-gram overlap and are fast, easy to compute, and widely used.</p>
<p>Common Metrics:</p>
<ul>
<li>BLEU (Precisiong) - MT  </li>
<li>ROUGE (Recall) - Summarization  </li>
<li>METEOR  </li>
<li>CIDEr  </li>
</ul>
<p>Limitations:</p>
<ul>
<li>Do not account for semantic similarity or paraphrasing.  </li>
<li>Declining effectiveness with more open-ended tasks:<ul>
<li>Summarization: Sensitive to lexical variation in longer texts.  </li>
<li>Dialogue: Penalizes legitimate, diverse responses.  </li>
<li>Story generation: May report high scores due to long-sequence overlap without true quality.  </li>
</ul>
</li>
</ul>
<h2 id="nlp-19_eval_nlg-2-model-based-metrics">2. Model-Based Metrics<a class="headerlink" href="#nlp-19_eval_nlg-2-model-based-metrics" title="Permanent link">¶</a></h2>
<p>These leverage pretrained neural representations (learned representation) to assess semantic similarity, often with better correlation to human judgments.</p>
<h3 id="nlp-19_eval_nlg-a-embedding-based-metrics">(a) Embedding-Based Metrics<a class="headerlink" href="#nlp-19_eval_nlg-a-embedding-based-metrics" title="Permanent link">¶</a></h3>
<ul>
<li>Embedding Average  </li>
<li>Vector Extrema  </li>
<li>Word Mover's Distance (WMD)  </li>
<li>MEANT  </li>
<li>YISI  </li>
</ul>
<h3 id="nlp-19_eval_nlg-b-contextual-embedding-metrics">(b) Contextual Embedding Metrics<a class="headerlink" href="#nlp-19_eval_nlg-b-contextual-embedding-metrics" title="Permanent link">¶</a></h3>
<ul>
<li>BERTScore: Uses contextual embeddings from BERT to compute word-level cosine similarity.</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/bertscore.png" data-desc-position="bottom"><img alt="BERTScore" src="../nlp/images/bertscore.png"></a></p>
<ul>
<li>BLEURT: BERT-based regression model fine-tuned to predict human judgment.  </li>
<li>Sentence Mover's Similarity: Extends WMD to sentence-level embeddings for evaluating coherence.  </li>
</ul>
<h3 id="nlp-19_eval_nlg-c-distributional-metrics">(c) Distributional Metrics<a class="headerlink" href="#nlp-19_eval_nlg-c-distributional-metrics" title="Permanent link">¶</a></h3>
<ul>
<li>MAUVE: Computes divergence between the distribution of generated and reference texts in embedding space, interpolating between them. Suited for open-ended tasks.  </li>
</ul>
<h2 id="nlp-19_eval_nlg-3-human-evaluations">3. Human Evaluations<a class="headerlink" href="#nlp-19_eval_nlg-3-human-evaluations" title="Permanent link">¶</a></h2>
<p>Automatic metrics often fail to fully capture generation quality. Human evaluations remain the most reliable and nuanced method.</p>
<p>Evaluation Dimensions:</p>
<ul>
<li>Fluency  </li>
<li>Coherence / Consistency  </li>
<li>Factual Accuracy  </li>
<li>Commonsense Reasoning  </li>
<li>Grammaticality  </li>
<li>Style and Formality  </li>
<li>Redundancy  </li>
<li>Typicality / Appropriateness  </li>
</ul>
<p>Challenges:</p>
<ul>
<li>Expensive and time-consuming  </li>
<li>Variability across annotators  </li>
<li>Inconsistent or subjective judgments  </li>
<li>Human only evaluate precision not recall.  </li>
</ul>
<p>Best Practices:</p>
<ul>
<li>Use standardized scales (e.g., Likert)  </li>
<li>Report inter-rater agreement  </li>
<li>Combine with automatic metrics  </li>
</ul>
<h2 id="nlp-19_eval_nlg-emerging-trend-llm-as-evaluator">Emerging Trend: LLM-as-Evaluator<a class="headerlink" href="#nlp-19_eval_nlg-emerging-trend-llm-as-evaluator" title="Permanent link">¶</a></h2>
<h3 id="nlp-19_eval_nlg-reference-based-evaluation">Reference-based Evaluation<a class="headerlink" href="#nlp-19_eval_nlg-reference-based-evaluation" title="Permanent link">¶</a></h3>
<p>This traditional approach compares model-generated text to one or more human-written reference outputs. It assumes the existence of gold-standard references and uses lexical or semantic similarity metrics to assess quality. While effective for tasks like summarization and translation, it struggles with open-ended generation where multiple valid outputs exist.</p>
<ul>
<li>Widely used in early NLP benchmarks.  </li>
<li>Examples: BLEU, ROUGE, METEOR, BERTScore.  </li>
</ul>
<h3 id="nlp-19_eval_nlg-reference-free-evaluation">Reference-free Evaluation<a class="headerlink" href="#nlp-19_eval_nlg-reference-free-evaluation" title="Permanent link">¶</a></h3>
<p>In this approach, model outputs are evaluated without gold references—typically by using another model (or the same model) to assign scores based on perceived quality. This has gained traction with the advent of large instruction-tuned models like GPT-4, which can perform relatively consistent evaluations aligned with human preferences.</p>
<ul>
<li>Especially useful for instruction-following or multi-turn dialogue.  </li>
<li>Examples: AlpacaEval, MT-Bench, Chatbot Arena.  </li>
</ul>
<p>Recent approaches employ large language models to act as automated evaluators (e.g., G-Eval, GPT-4-as-a-judge), showing promising alignment with human judgments while offering scale and speed.</p>
<h2 id="nlp-19_eval_nlg-comparison-of-evaluation-metrics">Comparison of Evaluation Metrics<a class="headerlink" href="#nlp-19_eval_nlg-comparison-of-evaluation-metrics" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Type</th>
<th>Semantic Awareness</th>
<th>Best for</th>
</tr>
</thead>
<tbody>
<tr>
<td>BLEU</td>
<td>Content overlap</td>
<td>No</td>
<td>Translation</td>
</tr>
<tr>
<td>ROUGE</td>
<td>Content overlap</td>
<td>Partial</td>
<td>Summarization</td>
</tr>
<tr>
<td>METEOR</td>
<td>Content overlap</td>
<td>Partial</td>
<td>Translation</td>
</tr>
<tr>
<td>CIDEr</td>
<td>Content overlap</td>
<td>No</td>
<td>Image captioning</td>
</tr>
<tr>
<td>BERTScore</td>
<td>Model-based</td>
<td>Yes</td>
<td>Summarization, QA</td>
</tr>
<tr>
<td>BLEURT</td>
<td>Model-based</td>
<td>Yes</td>
<td>Open-domain gen</td>
</tr>
<tr>
<td>MAUVE</td>
<td>Model-based (dist.)</td>
<td>Yes</td>
<td>Long-form gen</td>
</tr>
<tr>
<td>Human Eval</td>
<td>Human</td>
<td>Yes</td>
<td>All tasks</td>
</tr>
</tbody>
</table>
<h2 id="nlp-19_eval_nlg-takeaways">Takeaways<a class="headerlink" href="#nlp-19_eval_nlg-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>Content overlap metrics are fast but insufficient for evaluating meaning or diversity.  </li>
<li>Model-based metrics offer semantic insight but can be opaque.  </li>
<li>Human judgments are indispensable, despite their cost and variability.  </li>
<li>Inspect model outputs manually—don’t rely on metrics alone.  </li>
<li>Publicly release model outputs to promote reproducibility and transparency.  </li>
</ul></body></html></section><section class="print-page" id="nlp-20_post_training" heading-number="2.20"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="post-trainings-prompting-instruction-finetuning-and-dporlhf">Post-trainings: Prompting, Instruction Finetuning, and DPO/RLHF<a class="headerlink" href="#nlp-20_post_training-post-trainings-prompting-instruction-finetuning-and-dporlhf" title="Permanent link">¶</a></h1>
<h2 id="nlp-20_post_training-zero-shot-and-few-shot-in-context-learning">Zero-Shot and Few-Shot In-Context Learning<a class="headerlink" href="#nlp-20_post_training-zero-shot-and-few-shot-in-context-learning" title="Permanent link">¶</a></h2>
<h3 id="nlp-20_post_training-zero-shot-in-context-learning">Zero-Shot In-Context Learning<a class="headerlink" href="#nlp-20_post_training-zero-shot-in-context-learning" title="Permanent link">¶</a></h3>
<p>Large language models (LLMs), beginning with GPT-2 and especially prominent in GPT-3 and beyond, exhibit a remarkable emergent ability: zero-shot learning. In this setting, a model can perform a task without any task-specific gradient updates or labeled examples—just by conditioning on the right input format.</p>
<h4 id="nlp-20_post_training-example-qa-as-language-modeling">Example (QA as Language Modeling):<a class="headerlink" href="#nlp-20_post_training-example-qa-as-language-modeling" title="Permanent link">¶</a></h4>
<blockquote>
<p>Passage: Tom Brady is an American football player born in San Mateo, California.<br>
Q: Where was Tom Brady born? A:</p>
</blockquote>
<p>The model simply continues the sequence by generating the most probable completion (“San Mateo, California”).</p>
<h4 id="nlp-20_post_training-example-winograd-schema">Example (Winograd Schema):<a class="headerlink" href="#nlp-20_post_training-example-winograd-schema" title="Permanent link">¶</a></h4>
<p>The model can also answer commonsense questions by comparing the likelihood of different completions:</p>
<blockquote>
<p>The cat couldn't fit into the hat because it was too big.</p>
</blockquote>
<p>Here, the model disambiguates the pronoun “it” by comparing:
<script type="math/tex; mode=display">
P(\text{...because the cat was too big}) \quad \text{vs.} \quad P(\text{...because the hat was too big})
</script>
A higher probability assigned to the second option indicates the model's preference for “hat” as the correct referent, aligning with human intuition.</p>
<h3 id="nlp-20_post_training-few-shot-in-context-learning">Few-Shot (In-Context) Learning<a class="headerlink" href="#nlp-20_post_training-few-shot-in-context-learning" title="Permanent link">¶</a></h3>
<p>In few-shot in-context learning, the model is given a small number of input-output examples as context, followed by a new input to predict. No parameter updates occur—learning happens purely via conditioning on the prompt.</p>
<h4 id="nlp-20_post_training-example-sentiment-classification">Example (Sentiment Classification):<a class="headerlink" href="#nlp-20_post_training-example-sentiment-classification" title="Permanent link">¶</a></h4>
<blockquote>
<p>Review: This movie was amazing. Sentiment: Positive<br>
Review: The plot was dull and slow. Sentiment: Negative<br>
Review: The acting was brilliant. Sentiment:</p>
</blockquote>
<p>The model is expected to complete the pattern with <code>Positive</code>.</p>
<h4 id="nlp-20_post_training-terminology-note">Terminology Note:<a class="headerlink" href="#nlp-20_post_training-terminology-note" title="Permanent link">¶</a></h4>
<p>This is often referred to as in-context learning (ICL) to distinguish it from traditional few-shot learning involving optimization-based methods.</p>
<h4 id="nlp-20_post_training-limitations-of-prompting">Limitations of Prompting<a class="headerlink" href="#nlp-20_post_training-limitations-of-prompting" title="Permanent link">¶</a></h4>
<p>Despite the versatility of prompting, in-context learning has limitations:</p>
<ul>
<li>Context window size: Prompt length is bounded by the model's maximum context window, limiting how many examples or instructions can be included.</li>
<li>Reasoning complexity: Some tasks, especially those involving multi-hop or logical reasoning, remain difficult even for very large models using sophisticated prompts.</li>
<li>Interpretability and control: Prompting alone provides limited control over internal reasoning steps or modular behavior.</li>
</ul>
<h4 id="nlp-20_post_training-summary">Summary<a class="headerlink" href="#nlp-20_post_training-summary" title="Permanent link">¶</a></h4>
<ul>
<li>In-context learning enables task generalization without gradient updates.</li>
<li>Prompt design (e.g., Chain-of-Thought, few-shot examples) significantly influences performance.</li>
<li>However, for complex reasoning tasks, prompting may not be sufficient—gradient-based finetuning or hybrid techniques (e.g., tool use, scratchpads) may be necessary.</li>
</ul>
<h3 id="nlp-20_post_training-instruction-finetuning">Instruction Finetuning<a class="headerlink" href="#nlp-20_post_training-instruction-finetuning" title="Permanent link">¶</a></h3>
<p>Instruction finetuning is a paradigm in which a pretrained language model is adapted using a diverse set of (instruction, output) pairs across a wide range of natural language tasks. The goal is to teach the model to follow explicit instructions phrased in natural language, enabling it to generalize to unseen tasks at inference time without further finetuning.</p>
<h4 id="nlp-20_post_training-core-methodology">Core Methodology<a class="headerlink" href="#nlp-20_post_training-core-methodology" title="Permanent link">¶</a></h4>
<p>The core idea is to treat language modeling as a multitask learning problem, where each task is presented as a textual instruction, and the model is finetuned to produce the appropriate output. This process involves:</p>
<ul>
<li>Collecting a large, heterogeneous corpus of <span class="arithmatex">\((\text{instruction}, \text{response})\)</span> pairs spanning tasks such as question answering, summarization, classification, translation, commonsense reasoning, etc.</li>
<li>Optionally applying task mixtures (e.g., T0) or task reformulations to promote robustness.</li>
<li>Finetuning the model using a standard language modeling objective on these paired samples.</li>
</ul>
<h4 id="nlp-20_post_training-benefits">Benefits<a class="headerlink" href="#nlp-20_post_training-benefits" title="Permanent link">¶</a></h4>
<ul>
<li>Simplicity: A unified finetuning pipeline can teach the model to perform a wide range of tasks, framed uniformly as instruction following.</li>
<li>Generalization: Models trained this way can often generalize to novel tasks simply by being given a well-phrased prompt—even tasks not seen during training (zero-shot generalization).</li>
<li>Improved usability: End-users do not need to know the model’s internal API or finetune further; they can interact with the model via natural language instructions.</li>
</ul>
<h4 id="nlp-20_post_training-challenges-and-limitations">Challenges and Limitations<a class="headerlink" href="#nlp-20_post_training-challenges-and-limitations" title="Permanent link">¶</a></h4>
<ul>
<li>Data cost: Collecting high-quality demonstrations for hundreds of diverse tasks is resource-intensive and often requires human annotation.</li>
<li>Alignment mismatch: The model is optimized for a maximum likelihood objective, which may not align with human preferences (e.g., helpfulness, harmlessness, or truthfulness), leading to outputs that are technically correct but pragmatically unhelpful.</li>
<li>Ambiguity in instructions: Natural language instructions can be underspecified or ambiguous, requiring either better dataset design or model mechanisms for disambiguation.</li>
</ul>
<h4 id="nlp-20_post_training-representative-models">Representative Models<a class="headerlink" href="#nlp-20_post_training-representative-models" title="Permanent link">¶</a></h4>
<p>Instruction finetuning has been a foundational technique behind several recent LLMs:</p>
<ul>
<li>T5 (Text-to-Text Transfer Transformer): Casts all tasks into a text-to-text format with task-specific prefixes (e.g., “translate English to German: ...”).</li>
<li>FLAN (Finetuned Language Net): Builds on T5 with instruction finetuning across many tasks and task variants.</li>
<li>T0: Trains on prompt collections to enable strong zero-shot task generalization.</li>
<li>InstructGPT: Combines instruction finetuning with reinforcement learning from human feedback (RLHF) to better align with human intent.</li>
</ul>
<h4 id="nlp-20_post_training-outlook">Outlook<a class="headerlink" href="#nlp-20_post_training-outlook" title="Permanent link">¶</a></h4>
<p>Instruction finetuning plays a key role in aligning language models with human goals and making them more broadly useful without requiring users to craft task-specific prompts. However, as models scale and tasks become more complex, instruction finetuning alone may be insufficient, motivating hybrid approaches that integrate tool use, memory, and interaction (e.g., agents or retrievers).</p>
<h3 id="nlp-20_post_training-optimizing-for-human-preferences-rlhf-dpo">Optimizing for Human Preferences (RLHF / DPO)<a class="headerlink" href="#nlp-20_post_training-optimizing-for-human-preferences-rlhf-dpo" title="Permanent link">¶</a></h3>
<p>While instruction finetuning significantly improves a language model's usability by teaching it to follow task instructions, it does not guarantee alignment with human values, preferences, or expectations. To bridge this gap, researchers have developed techniques that explicitly optimize language models based on human preferences. Two leading approaches in this domain are Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO).</p>
<h4 id="nlp-20_post_training-from-instructions-to-rewards">From Instructions to Rewards<a class="headerlink" href="#nlp-20_post_training-from-instructions-to-rewards" title="Permanent link">¶</a></h4>
<p>Instruction tuning is typically the first step in building an aligned model. However, aligning model outputs more precisely with human expectations requires further optimization:</p>
<ul>
<li>Step 1: Instruction finetuning on diverse (instruction, output) pairs.</li>
<li>Step 2: Learning a reward function from human feedback.</li>
<li>Step 3: Optimizing the model to produce outputs that maximize this learned reward.</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/RLHF.png" data-desc-position="bottom"><img alt="RLHF" src="../nlp/images/RLHF.png"></a></p>
<h4 id="nlp-20_post_training-learning-human-preferences">Learning Human Preferences<a class="headerlink" href="#nlp-20_post_training-learning-human-preferences" title="Permanent link">¶</a></h4>
<p>Key challenge: Obtaining human feedback at scale is expensive, inconsistent, and noisy.</p>
<p>Solution: Instead of relying on absolute scores or direct ratings—which are often uncalibrated—use pairwise comparisons, where humans are asked to choose the preferred output among two or more options. These comparisons can then be modeled as a binary classification task.</p>
<p>A commonly used model for interpreting such data is the Bradley–Terry model, which assigns a higher latent score to the preferred (“winning”) output over the non-preferred (“losing”) one.</p>
<h4 id="nlp-20_post_training-reinforcement-learning-from-human-feedback-rlhf">Reinforcement Learning from Human Feedback (RLHF)<a class="headerlink" href="#nlp-20_post_training-reinforcement-learning-from-human-feedback-rlhf" title="Permanent link">¶</a></h4>
<p>RLHF is a three-step pipeline:</p>
<ol>
<li>Supervised Instruction Tuning: Finetune the language model on instruction datasets to teach basic task-following behavior.</li>
<li>Reward Modeling: Collect human preference data in the form of comparisons between model outputs. Train a reward model (RM) to predict human preferences by learning to assign higher scores to preferred responses.</li>
<li>Policy Optimization: Use reinforcement learning (typically Proximal Policy Optimization, PPO) to optimize the language model to produce outputs that maximize the predicted reward, while remaining close to the supervised model (e.g., via a KL-divergence penalty).</li>
</ol>
<p>Strengths:</p>
<ul>
<li>Produces high-quality, aligned outputs when tuned well.</li>
<li>Flexible reward modeling enables fine-grained preference learning.</li>
</ul>
<p>Challenges:</p>
<ul>
<li>RL optimization is unstable, sensitive to reward shaping and hyperparameters.</li>
<li>Computationally expensive: PPO with large models requires extensive resources.</li>
<li>Human preferences are not always consistent, and reward models may overfit artifacts in the training data.</li>
</ul>
<h4 id="nlp-20_post_training-direct-preference-optimization-dpo">Direct Preference Optimization (DPO)<a class="headerlink" href="#nlp-20_post_training-direct-preference-optimization-dpo" title="Permanent link">¶</a></h4>
<p>DPO offers a simpler, stable alternative to RLHF by bypassing the need for reinforcement learning altogether. Instead, it directly finetunes the language model on human preference data using a contrastive objective.</p>
<p>Given a dataset of preferred (“chosen”) and non-preferred (“rejected”) outputs, the model is trained to increase the likelihood of preferred completions relative to rejected ones.</p>
<p>Advantages:</p>
<ul>
<li>No reinforcement learning loop — avoids instability and simplifies training.</li>
<li>Leverages the pretrained model’s probabilities directly.</li>
<li>Comparable performance to RLHF on many benchmarks.</li>
</ul>
<p>Limitations:</p>
<ul>
<li>Assumes high-quality, diverse comparison data.</li>
<li>Does not inherently model long-term or interactive objectives.</li>
</ul>
<h4 id="nlp-20_post_training-summary_1">Summary<a class="headerlink" href="#nlp-20_post_training-summary_1" title="Permanent link">¶</a></h4>
<ul>
<li>Optimizing for human preferences is essential for building safe, aligned LLMs.</li>
<li>RLHF has shown strong results (e.g., InstructGPT, ChatGPT), but is computationally intensive.</li>
<li>DPO is emerging as a lightweight, scalable alternative with competitive results.</li>
<li>Both methods rely on accurate human preference modeling—an inherently noisy and subjective signal.</li>
</ul>
<table>
<thead>
<tr>
<th>Term</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Instruction Fine-Tuning (IFT)</td>
<td>Training a language model to follow user instructions, typically using an autoregressive language modeling loss.</td>
</tr>
<tr>
<td>Supervised Fine-Tuning (SFT)</td>
<td>Training a model on task-specific labeled data to acquire desired capabilities, generally with an autoregressive LM loss.</td>
</tr>
<tr>
<td>Alignment</td>
<td>A broad goal of making models behave according to user intentions or societal values; can be optimized via any suitable loss function.</td>
</tr>
<tr>
<td>Reinforcement Learning from Human Feedback (RLHF)</td>
<td>A specific technique that fine-tunes models using human-generated preference data via reinforcement learning.</td>
</tr>
<tr>
<td>Preference Fine-Tuning</td>
<td>Uses human-labeled preference data to fine-tune models. Can be implemented via RLHF, Direct Preference Optimization (DPO), or learning-to-rank methods.</td>
</tr>
</tbody>
</table></body></html></section><section class="print-page" id="nlp-21_advanced_topics" heading-number="2.21"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="advanced-topics-in-language-modelling">Advanced Topics in Language Modelling<a class="headerlink" href="#nlp-21_advanced_topics-advanced-topics-in-language-modelling" title="Permanent link">¶</a></h1>
<h2 id="nlp-21_advanced_topics-knowledge-distillation-for-language-models">Knowledge Distillation for Language Models<a class="headerlink" href="#nlp-21_advanced_topics-knowledge-distillation-for-language-models" title="Permanent link">¶</a></h2>
<p>Knowledge distillation is a model compression and capability transfer technique in NLP and LLMs that trains a compact student model to approximate a high-capacity teacher model. Instead of learning only from one-hot labels, the student also learns from the teacher’s soft predictive distribution and, optionally, its intermediate reasoning samples, improving both generalization and reliability while reducing inference cost.</p>
<h3 id="nlp-21_advanced_topics-teacher-and-student-distributions">Teacher and Student Distributions<a class="headerlink" href="#nlp-21_advanced_topics-teacher-and-student-distributions" title="Permanent link">¶</a></h3>
<p>Given input <span class="arithmatex">\(x\)</span>, the teacher outputs logits <span class="arithmatex">\(z_t \in \mathbb{R}^{|V|}\)</span> over vocabulary <span class="arithmatex">\(V\)</span>. A temperature-scaled softmax produces softened probabilities:</p>
<div class="arithmatex">\[
p_t^T = softmax(z_t / T), \quad T &gt; 1
\]</div>
<p>The student model generates its own logits <span class="arithmatex">\(z_s\)</span> and corresponding distribution:</p>
<div class="arithmatex">\[
p_s^T = softmax(z_s / T)
\]</div>
<p>When <span class="arithmatex">\(T &gt; 1\)</span>, distributions become smoother, exposing relationships between alternative token choices. This is critical for multi-step reasoning, where each token builds on implicit intermediate deductions. Distillation transfers not only correct answers but also richer decision structure and uncertainty.</p>
<h3 id="nlp-21_advanced_topics-distillation-loss">Distillation Loss<a class="headerlink" href="#nlp-21_advanced_topics-distillation-loss" title="Permanent link">¶</a></h3>
<p>Training minimizes a weighted objective balancing:</p>
<ul>
<li>Correctness using cross-entropy with ground truth <span class="arithmatex">\(y\)</span></li>
<li>Imitation using KL divergence between teacher and student distributions</li>
</ul>
<p>Loss formulation:</p>
<div class="arithmatex">\[
L = \alpha T^2 \, KL(p_t^T \| p_s^T) + (1 - \alpha) \, CE(y, softmax(z_s))
\]</div>
<p>Where:</p>
<div class="arithmatex">\[
KL(p \| q) = \sum_i p_i \log \frac{p_i}{q_i}
\]</div>
<p>The term <span class="arithmatex">\(T^2\)</span> rescales gradients to preserve signal magnitude when using high temperature, preventing vanishing updates.</p>
<h3 id="nlp-21_advanced_topics-representation-alignment-encoder-models">Representation Alignment (Encoder Models)<a class="headerlink" href="#nlp-21_advanced_topics-representation-alignment-encoder-models" title="Permanent link">¶</a></h3>
<p>For encoder models, teacher and student hidden states <span class="arithmatex">\(h_t\)</span> and <span class="arithmatex">\(h_s\)</span> may differ in size. A learned projector <span class="arithmatex">\(W\)</span> aligns them:</p>
<div class="arithmatex">\[
L_{rep} = MSE(h_t W, h_s)
\]</div>
<p>Alternative alignment objectives include cosine similarity or attention map matching across selected layers <span class="arithmatex">\(\mathcal{L}_k\)</span>.</p>
<h3 id="nlp-21_advanced_topics-distillation-algorithm">Distillation Algorithm<a class="headerlink" href="#nlp-21_advanced_topics-distillation-algorithm" title="Permanent link">¶</a></h3>
<h4 id="nlp-21_advanced_topics-initial-setup">Initial Setup<a class="headerlink" href="#nlp-21_advanced_topics-initial-setup" title="Permanent link">¶</a></h4>
<ul>
<li>Teacher Model: Large LM (e.g., 3.7B–175B parameters)</li>
<li>Student Model: Smaller LM (e.g., 125M–1.3B parameters)</li>
<li>Training Inputs: <span class="arithmatex">\(\mathcal{D}_{Train} = \{x_i\}\)</span></li>
<li>Prompt Set: <span class="arithmatex">\(P = \{(x_i, y_i, z_i)\}\)</span> where <span class="arithmatex">\(z_i\)</span> are teacher reasoning samples</li>
</ul>
<h4 id="nlp-21_advanced_topics-sampling-process">Sampling Process<a class="headerlink" href="#nlp-21_advanced_topics-sampling-process" title="Permanent link">¶</a></h4>
<p>For each example <span class="arithmatex">\(x_i\)</span>:</p>
<ol>
<li>Sample <span class="arithmatex">\(N\)</span> teacher reasoning–prediction pairs:
   <script type="math/tex; mode=display">
   (\hat{y}_i, \hat{z}_i) \sim \mathcal{N}_T(y_i, z_i \mid x_i, P)
   </script>
</li>
<li>Construct sample set:
   <script type="math/tex; mode=display">
   P_i = \{(x_i, \hat{z}_i^j, \hat{y}_i^j)\}_{j=1}^N
   </script>
</li>
<li>Typical setting:
   <script type="math/tex; mode=display">
   N = 30
   </script>
</li>
</ol>
<p>Create corpus:</p>
<div class="arithmatex">\[
C = \{(x_i, (\hat{z}_i^j, \hat{y}_i^j))\}_{j=1}^N
\]</div>
<h4 id="nlp-21_advanced_topics-training-process">Training Process<a class="headerlink" href="#nlp-21_advanced_topics-training-process" title="Permanent link">¶</a></h4>
<p>Train student on teacher samples using LM objective:</p>
<div class="arithmatex">\[
L(z_s, y_s \mid C) = CE(\hat{y}_i^j, \hat{z}_i^j \mid x_i)
\]</div>
<p>This objective integrates into the full distillation loss defined earlier.</p>
<h4 id="nlp-21_advanced_topics-evaluation-options">Evaluation Options<a class="headerlink" href="#nlp-21_advanced_topics-evaluation-options" title="Permanent link">¶</a></h4>
<p>After training, evaluate output quality using:</p>
<ul>
<li>
<p>Greedy Decoding
  <script type="math/tex; mode=display">
  (\hat{z}_{test}, \hat{y}_{test}) = argmax_{z_s} \mathcal{S}(y \mid z_t)
  </script>
</p>
</li>
<li>
<p>Self-Consistency
  <script type="math/tex; mode=display">
  \hat{y}_{test} = argmax_y \, \mathbb{E}_{z_s \sim \mathcal{S}_{top-k}} [\mathcal{S}(y \mid z_s, z_{test})]
  </script>
</p>
</li>
</ul>
<h4 id="nlp-21_advanced_topics-optional-generative-extension-sample-and-rank">Optional Generative Extension: Sample-and-Rank<a class="headerlink" href="#nlp-21_advanced_topics-optional-generative-extension-sample-and-rank" title="Permanent link">¶</a></h4>
<ol>
<li>Sample <span class="arithmatex">\(n\)</span> teacher completions per prompt</li>
<li>Score using reward model or teacher log-likelihood</li>
<li>Select top-<span class="arithmatex">\(k\)</span> sequences <span class="arithmatex">\(\mathcal{S}_{top}\)</span></li>
<li>Train student via:</li>
<li><span class="arithmatex">\(CE(\mathcal{S}_{top}, p_{student})\)</span>, or</li>
<li>policy distillation on teacher action distributions</li>
</ol>
<hr>
<h2 id="nlp-21_advanced_topics-mixture-of-experts">Mixture of Experts<a class="headerlink" href="#nlp-21_advanced_topics-mixture-of-experts" title="Permanent link">¶</a></h2>
<p>In LLMs and NLP, mixture of experts (MoE) replaces dense feed-forward layers with a set of expert networks <span class="arithmatex">\(\{E_1, ..., E_N\}\)</span> and a router <span class="arithmatex">\(G\)</span> that selects a sparse subset of experts per token. Given input representation <span class="arithmatex">\(h\)</span>, the router outputs gating logits <span class="arithmatex">\(r = G(h)\)</span> and expert selection is typically top-<span class="arithmatex">\(k\)</span>:</p>
<div class="arithmatex">\[
g_i = softmax(r)_i,\quad \mathcal{I} = topk(r, k)
\]</div>
<p>Only experts in <span class="arithmatex">\(\mathcal{I}\)</span> are executed, producing <span class="arithmatex">\(e_i = E_i(h)\)</span>, and the MoE layer output is:</p>
<div class="arithmatex">\[
y = \sum_{i \in \mathcal{I}} g_i \, e_i
\]</div>
<p>Training minimizes the task loss <span class="arithmatex">\(L_{task}\)</span> (e.g., next-token cross-entropy) plus auxiliary load-balancing and routing regularization to prevent expert collapse and imbalance. A common load-balancing loss uses batch-level expert utilization <span class="arithmatex">\(f_i\)</span> and mean gate probability <span class="arithmatex">\(\bar{g}_i\)</span>:</p>
<div class="arithmatex">\[
L = L_{task} + \lambda \sum_{i=1}^N f_i \cdot \bar{g}_i
\]</div>
<p>Alternative formulations use entropy bonuses on <span class="arithmatex">\(g\)</span>, z-loss on router logits, or differentiable routing approximations.</p>
<p>Algorithm steps:</p>
<ol>
<li>Replace MLP layers with MoE block containing <span class="arithmatex">\(N\)</span> experts and router <span class="arithmatex">\(G\)</span>.</li>
<li>For each token, compute router logits <span class="arithmatex">\(r = G(h)\)</span>.</li>
<li>Select expert indices <span class="arithmatex">\(\mathcal{I} = topk(r, k)\)</span>.</li>
<li>Compute gates <span class="arithmatex">\(g_i = softmax(r)_i\)</span> for <span class="arithmatex">\(i \in \mathcal{I}\)</span>.</li>
<li>Execute selected experts <span class="arithmatex">\(e_i = E_i(h)\)</span>.</li>
<li>Aggregate <span class="arithmatex">\(y = \sum_{i \in \mathcal{I}} g_i \, e_i\)</span>.</li>
<li>Compute loss <span class="arithmatex">\(L = L_{task} + L_{aux}\)</span> and backprop to student and router.</li>
<li>Optionally apply capacity limits per expert, expert dropout, router jitter noise, or switch to single-expert routing (top-1) variants.</li>
</ol>
<p>Efficiency note: MoE increases parameter count while reducing per-token FLOPs via sparse routing, improving scaling at controlled inference cost.</p>
<hr>
<h2 id="nlp-21_advanced_topics-predicting-the-next-token-in-language-models">Predicting the Next Token in Language Models<a class="headerlink" href="#nlp-21_advanced_topics-predicting-the-next-token-in-language-models" title="Permanent link">¶</a></h2>
<ol>
<li>
<p>Greedy Decoding: Greedy decoding selects the token with the highest predicted probability at each step: </p>
<div class="arithmatex">\[x_{t+1} = \arg\max_{x} \, p(x \mid x_{1:t})\]</div>
<p>While simple and efficient, it often produces suboptimal sequences that are repetitive, lack diversity, or fail to capture long-range coherence, since it ignores alternative plausible continuations.</p>
</li>
<li>
<p>Beam Search: Beam search maintains the top-<span class="arithmatex">\(k\)</span> most probable partial sequences (beams) at each timestep. The algorithm expands each beam with all possible next tokens, retains the <span class="arithmatex">\(k\)</span> highest-scoring sequences, and repeats until termination. Benefits include higher likelihood sequences compared to greedy decoding, but beam search increases computational cost and can still lack diversity, often producing deterministic or generic outputs if the beam width is small or length penalties are not applied.</p>
</li>
<li>
<p>Sampling-Based Decoding: Sampling introduces stochasticity to token selection, enabling more diverse and natural outputs. Common strategies include:  </p>
<ul>
<li>
<p>Top-<span class="arithmatex">\(k\)</span> sampling: Restrict the candidate set to the <span class="arithmatex">\(k\)</span> most probable tokens and sample according to their normalized probabilities.  </p>
<div class="arithmatex">\[p'(x \mid x_{1:t}) = \frac{p(x \mid x_{1:t})}{\sum_{i \in topk} p(i \mid x_{1:t})}, \quad x \sim p'\]</div>
</li>
<li>
<p>Top-<span class="arithmatex">\(p\)</span> (nucleus) sampling: Select the smallest set of tokens whose cumulative probability exceeds a threshold <span class="arithmatex">\(p\)</span> and sample from this set.  </p>
<div class="arithmatex">\[\mathcal{V}_p = \min \{V' \mid \sum_{i \in V'} p(i \mid x_{1:t}) \ge p \}, \quad x \sim p(i \mid i \in \mathcal{V}_p)\]</div>
</li>
<li>
<p>Temperature scaling: Adjusts the sharpness of the predicted distribution to control randomness:  </p>
</li>
</ul>
<div class="arithmatex">\[p_T(x \mid x_{1:t}) = softmax \left( \frac{\log p(x \mid x_{1:t})}{T} \right)\]</div>
<p>Low temperatures (<span class="arithmatex">\(T&lt;1\)</span>) make the distribution peakier, favoring high-probability tokens and reducing diversity. High temperatures (<span class="arithmatex">\(T&gt;1\)</span>) flatten the distribution, increasing randomness and creative outputs.</p>
</li>
</ol>
<p>Overall, the choice of decoding strategy involves a tradeoff between likelihood, diversity, and computational efficiency. Greedy and beam search optimize for probability, whereas sampling-based methods enhance diversity and naturalness in generated text.</p>
<hr>
<h2 id="nlp-21_advanced_topics-inference-optimization-in-llms">Inference Optimization in LLMs<a class="headerlink" href="#nlp-21_advanced_topics-inference-optimization-in-llms" title="Permanent link">¶</a></h2>
<h3 id="nlp-21_advanced_topics-kv-caching">KV caching<a class="headerlink" href="#nlp-21_advanced_topics-kv-caching" title="Permanent link">¶</a></h3>
<p>In autoregressive transformers, each new token attends to all prior tokens. Instead of recomputing past key/value projections, we cache them. For token step <span class="arithmatex">\(t\)</span>, the attention computation becomes:</p>
<div class="arithmatex">\[
Q_t = W_Q h_t,\quad K_{1:t} = [K_{cache}; W_K h_t],\quad V_{1:t} = [V_{cache}; W_V h_t]
\]</div>
<div class="arithmatex">\[
A_t = softmax\left(\frac{Q_t K_{1:t}^\top}{\sqrt{d_k}}\right) V_{1:t}
\]</div>
<p>The cache stores <span class="arithmatex">\((K_{1:t-1}, V_{1:t-1})\)</span> from earlier steps. This reduces per-token FLOPs from <span class="arithmatex">\(O(t)\)</span> to <span class="arithmatex">\(O(1)\)</span> for projections, leaving only the attention dot product with cached states. Latency improves significantly for long contexts, at the cost of <span class="arithmatex">\(O(n_{layers} · seq_{len} · d_{kv})\)</span> memory.</p>
<p>Algorithm steps:</p>
<ol>
<li>For each layer, compute <span class="arithmatex">\(K = W_K h, V = W_V h\)</span> for prompt tokens.</li>
<li>Store <span class="arithmatex">\((K, V)\)</span> in cache.</li>
<li>During generation, compute only <span class="arithmatex">\(K_t, V_t\)</span> for the new token.</li>
<li>Append to cache and attend using the full cached <span class="arithmatex">\(K, V\)</span>.</li>
</ol>
<h3 id="nlp-21_advanced_topics-multi-query-attention-mqa-and-grouped-query-attention-gqa">Multi-Query Attention (MQA) and Grouped-Query Attention (GQA)<a class="headerlink" href="#nlp-21_advanced_topics-multi-query-attention-mqa-and-grouped-query-attention-gqa" title="Permanent link">¶</a></h3>
<p>MQA shares a single key/value head across all query heads, reducing memory and bandwidth:</p>
<p>
<script type="math/tex; mode=display">
   K = W_K h,\quad V = W_V h \quad \text{(1 shared head)}
   </script>
</p>
<p>All query heads attend to the same <span class="arithmatex">\(K, V\)</span>. This cuts cache size by <span class="arithmatex">\(h\)</span>× (number of Q heads).<br>
   GQA generalizes this by sharing <span class="arithmatex">\(K, V\)</span> across groups of query heads:</p>
<p>
<script type="math/tex; mode=display">
   \mathcal{G}_j = \{Q_{j,1},...,Q_{j,m}\},\quad K_j, V_j \text{ shared per group}
   </script>
</p>
<p>If there are <span class="arithmatex">\(h_q\)</span> query heads and <span class="arithmatex">\(h_{kv}\)</span> KV heads, each KV head serves <span class="arithmatex">\(h_q / h_{kv}\)</span> query heads. GQA balances quality and efficiency better than full sharing (MQA) while still reducing memory bandwidth and cache footprint.</p>
<p>Benefits:</p>
<ul>
<li>Smaller KV cache (<span class="arithmatex">\(↓\)</span> memory, <span class="arithmatex">\(↓\)</span> GPU bandwidth pressure)</li>
<li>Faster decoding, especially in memory-bound regimes</li>
<li>Better quality than MQA when <span class="arithmatex">\(h_{kv} &gt; 1\)</span></li>
</ul>
<p>Limitations:</p>
<ul>
<li>Some capacity loss from reduced key/value specialization</li>
<li>Requires careful grouping choice to avoid quality drop</li>
</ul>
<h3 id="nlp-21_advanced_topics-pagedattention">PagedAttention<a class="headerlink" href="#nlp-21_advanced_topics-pagedattention" title="Permanent link">¶</a></h3>
<p>Standard KV caches allocate contiguous memory per sequence, causing fragmentation and over-allocation when sequences vary in length. PagedAttention stores KV blocks in fixed-size pages (like virtual memory), allowing non-contiguous storage:</p>
<p>Idea:</p>
<ul>
<li>Preallocate memory into pages of size <span class="arithmatex">\(P\)</span>.</li>
<li>Store <span class="arithmatex">\((K, V)\)</span> in page slots, not one long tensor.</li>
<li>Map logical token positions → physical page addresses.</li>
</ul>
<p>Memory model:</p>
<p>
<script type="math/tex; mode=display">
   (K, V)_{layer} \in \mathbb{R}^{n_{pages} × P × d_{kv}}
   </script>
</p>
<p>Attention reads KV by gathering relevant pages:</p>
<p>
<script type="math/tex; mode=display">
   A_t = softmax\left(\frac{Q_t K_{pages}^\top}{\sqrt{d_k}}\right) V_{pages}
   </script>
</p>
<p>Benefits:</p>
<ul>
<li>Eliminates wasted padding memory</li>
<li>Enables large-batch serving without OOM</li>
<li>Efficient for dynamic and very long contexts</li>
<li>Reduces fragmentation and improves throughput</li>
</ul>
<p>Algorithm steps:</p>
<ol>
<li>Partition prompt tokens into pages of length <span class="arithmatex">\(P\)</span>.</li>
<li>Store each page’s <span class="arithmatex">\(K, V\)</span> into free page slots.</li>
<li>Maintain a page table mapping token index → page slot.</li>
<li>During decoding, gather only required pages for attention.</li>
<li>Append new token KV into next free page.</li>
</ol></body></html></section><section class="print-page" id="nlp-22_llm_training_basics" heading-number="2.22"><h1 id="nlp-22_llm_training_basics-nlp-22_llm_training_basics">22. LLM Training Basics</h1><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h2 id="tensors-and-pytorch">Tensors and PyTorch<a class="headerlink" href="#nlp-22_llm_training_basics-tensors-and-pytorch" title="Permanent link">¶</a></h2>
<p>Tensors are the basic building block for storing everything in deep learning: parameters, gradients, optimizer state, data, and activations. Almost all of these are stored as floating-point numbers.</p>
<p>By default, tensors live in CPU memory. To leverage the massive parallelism of GPUs, we move them to GPU memory via <code>.to(device)</code> or <code>.cuda()</code>.</p>
<h3 id="nlp-22_llm_training_basics-what-is-a-tensor">What is a Tensor?<a class="headerlink" href="#nlp-22_llm_training_basics-what-is-a-tensor" title="Permanent link">¶</a></h3>
<p>PyTorch tensors are pointers into allocated memory, plus metadata describing:</p>
<ul>
<li><code>shape</code></li>
<li><code>stride</code> — how many elements to skip to move along each axis</li>
<li><code>dtype</code></li>
<li><code>device</code></li>
</ul>
<p>Most tensors are created from performing operations on other tensors. Each operation has some memory and compute consequence. Many operations simply provide a different <em>view</em> of the tensor. This does not make a copy, and therefore mutations in one tensor affect the other.</p>
<h3 id="nlp-22_llm_training_basics-optional-math-example">Optional math example<a class="headerlink" href="#nlp-22_llm_training_basics-optional-math-example" title="Permanent link">¶</a></h3>
<p>A tensor <span class="arithmatex">\(x \in \mathbb{R}^{m \times n}\)</span> is stored as a contiguous 1-D block of memory, with metadata describing how to interpret it. The stride vector <span class="arithmatex">\(s\)</span> defines indexing via:</p>
<p>
<script type="math/tex">
x[i, j] = \text{memory}[i \cdot s_0 + j \cdot s_1]
</script>
</p>
<p>If a view operation reshapes <span class="arithmatex">\(x\)</span> without copying, the underlying memory remains the same:</p>
<p>
<script type="math/tex">
x' = \text{view}(x), \quad \text{ptr}(x') = \text{ptr}(x)
</script>
</p>
<h2 id="nlp-22_llm_training_basics-flops-and-performance-in-deep-learning">FLOPs and Performance in Deep Learning<a class="headerlink" href="#nlp-22_llm_training_basics-flops-and-performance-in-deep-learning" title="Permanent link">¶</a></h2>
<p>A FLOP (floating-point operation) is a basic arithmetic operation (e.g., addition or multiplication) on floating-point numbers. It is a standard measure of computational work in deep learning.</p>
<h4 id="nlp-22_llm_training_basics-example-linear-layer-flops">Example: Linear Layer FLOPs<a class="headerlink" href="#nlp-22_llm_training_basics-example-linear-layer-flops" title="Permanent link">¶</a></h4>
<p>Suppose we apply a linear model to a batch of <span class="arithmatex">\(B\)</span> vectors of dimension <span class="arithmatex">\(D\)</span>, mapping to <span class="arithmatex">\(K\)</span> outputs:</p>
<div class="highlight"><pre><span></span><code><span class="n">B</span> <span class="o">=</span> <span class="mi">16384</span>  <span class="c1"># Batch size (e.g., number of tokens)</span>
<span class="n">D</span> <span class="o">=</span> <span class="mi">32768</span>  <span class="c1"># Input dimension</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">8192</span>   <span class="c1"># Output dimension</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">get_device</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
</code></pre></div>
<p>Each output <span class="arithmatex">\(y_{ik} = \sum_j x_{ij} w_{jk}\)</span> requires <span class="arithmatex">\(D\)</span> multiplications
and <span class="arithmatex">\(D - 1\)</span> additions.</p>
<p>Approximate total FLOPs:</p>
<div class="arithmatex">\[
FLOPs = 2  \times B  \times D  \times K
\]</div>
<h3 id="nlp-22_llm_training_basics-other-flops-estimates">Other FLOPs Estimates<a class="headerlink" href="#nlp-22_llm_training_basics-other-flops-estimates" title="Permanent link">¶</a></h3>
<ul>
<li>Elementwise operation (e.g., ReLU on <span class="arithmatex">\(m imes n\)</span> matrix): <span class="arithmatex">\(mn\)</span> FLOPs</li>
<li>Matrix addition (<span class="arithmatex">\(m \times n + m \times n\)</span>): <span class="arithmatex">\(mn\)</span> FLOPs</li>
<li>Normalization layers: <span class="arithmatex">\(O(mn)\)</span> FLOPs</li>
</ul>
<p>In general, matrix multiplications dominate FLOPs in large models like
Transformers.</p>
<h3 id="nlp-22_llm_training_basics-interpreting-flops-in-llms">Interpreting FLOPs in LLMs<a class="headerlink" href="#nlp-22_llm_training_basics-interpreting-flops-in-llms" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(B\)</span> is number of tokens or batch size</li>
<li><span class="arithmatex">\(D \times K\)</span> is number of parameters in a linear layer</li>
<li>Forward pass FLOPs: <span class="arithmatex">\(2 \cdot B \cdot D \cdot K\)</span></li>
<li>Total FLOPs: Sum over all layers, multiplied by batch size and sequence length</li>
</ul>
<h3 id="nlp-22_llm_training_basics-rule-of-thumb">Rule of thumb<a class="headerlink" href="#nlp-22_llm_training_basics-rule-of-thumb" title="Permanent link">¶</a></h3>
<div class="arithmatex">\[FLOPs \approx 2 \cdot    \text{#tokens} \cdot  \text{#parameters (per layer)}\]</div>
<h3 id="nlp-22_llm_training_basics-model-flops-utilization-mfu">Model FLOPs Utilization (MFU)<a class="headerlink" href="#nlp-22_llm_training_basics-model-flops-utilization-mfu" title="Permanent link">¶</a></h3>
<p>Definition:</p>
<div class="arithmatex">\[
MFU = \frac{\text{Actual FLOPs/sec}}{\text{Peak Theoretical FLOPs/sec}}
\]</div>
<p>MFU indicates how closely the training process approaches the maximum
compute capability of the hardware.</p>
<h4 id="nlp-22_llm_training_basics-why-mfu-is-not-always-1">Why MFU is not always 1<a class="headerlink" href="#nlp-22_llm_training_basics-why-mfu-is-not-always-1" title="Permanent link">¶</a></h4>
<ul>
<li>Non-matmul operations (activations, normalization, data movement)
    are often memory-bound</li>
<li>Kernel launch overhead from many small kernels</li>
<li>Memory bandwidth bottlenecks</li>
<li>Irregular workloads (uneven tensor sizes, short sequences)</li>
<li>Communication overhead in multi-GPU setups (sync, data transfer)</li>
</ul>
<h4 id="nlp-22_llm_training_basics-what-is-a-good-mfu">What is a good MFU?<a class="headerlink" href="#nlp-22_llm_training_basics-what-is-a-good-mfu" title="Permanent link">¶</a></h4>
<p>Typically:</p>
<div class="arithmatex">\[
MFU \ge 0.5   \text{ is considered good}
\]</div>
<p>MFU improves when:</p>
<ul>
<li>Matrix multiplications dominate the workload</li>
<li>Batch sizes and sequence lengths are large</li>
<li>Code is optimized for the hardware</li>
</ul>
<h2 id="nlp-22_llm_training_basics-mixed-precision-training">Mixed-Precision Training<a class="headerlink" href="#nlp-22_llm_training_basics-mixed-precision-training" title="Permanent link">¶</a></h2>
<h3 id="nlp-22_llm_training_basics-floating-point-formats-in-gpus">Floating-Point Formats in GPUs<a class="headerlink" href="#nlp-22_llm_training_basics-floating-point-formats-in-gpus" title="Permanent link">¶</a></h3>
<h4 id="nlp-22_llm_training_basics-floating-point-101">Floating-point 101<a class="headerlink" href="#nlp-22_llm_training_basics-floating-point-101" title="Permanent link">¶</a></h4>
<p>A binary floating-point number is encoded as:</p>
<div class="arithmatex">\[\underbrace{\text{sign}}_{\;1\;\text{bit}}\;
\underbrace{\text{exponent}}_{\;e\;\text{bits}}\;
\underbrace{\text{mantissa / significand}}_{\;m\;\text{bits}}
\]</div>
<table>
<thead>
<tr>
<th>Format</th>
<th style="text-align: right;">Bits</th>
<th style="text-align: right;">Exponent</th>
<th style="text-align: right;">Mantissa</th>
<th>Approx. Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32 (single)</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">23</td>
<td><span class="arithmatex">\(10^{-45}\)</span> – <span class="arithmatex">\(10^{38}\)</span></td>
</tr>
<tr>
<td>FP16 (half)</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">10</td>
<td><span class="arithmatex">\(2 \times 10^{-14}\)</span> – <span class="arithmatex">\(2 \times 10^{15}\)</span></td>
</tr>
</tbody>
</table>
<p>Precision vs. range: Reducing the mantissa increases spacing between representable values, so small increments (e.g. <span class="arithmatex">\(1.0001\)</span>) round to <span class="arithmatex">\(1.0\)</span> in FP16.<br>
A narrower exponent field also shrinks dynamic range, increasing overflow/underflow risk.</p>
<h3 id="nlp-22_llm_training_basics-fp32-vs-fp16-in-neural-network-training">FP32 vs. FP16 in Neural-Network Training<a class="headerlink" href="#nlp-22_llm_training_basics-fp32-vs-fp16-in-neural-network-training" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Default FP32 training: Parameters and gradients are stored in FP32. High memory usage <span class="arithmatex">\(\rightarrow\)</span> risk of OOM on large models.</p>
</li>
<li>
<p>Naïve FP16 swap: Reduces memory and bandwidth by 50%, but:</p>
<ul>
<li>Gradients may underflow to zero</li>
<li>Weight updates lose precision <span class="arithmatex">\(\rightarrow\)</span> poor convergence</li>
</ul>
</li>
</ul>
<h3 id="nlp-22_llm_training_basics-mixed-precision-training-core-idea">Mixed-Precision Training: Core Idea<a class="headerlink" href="#nlp-22_llm_training_basics-mixed-precision-training-core-idea" title="Permanent link">¶</a></h3>
<p>Keep critical operations in FP32 while using FP16 where safe:</p>
<ol>
<li>Maintain a set of FP32 master weights</li>
<li>Cast a working copy of the model to FP16; do the forward pass there.</li>
<li>Back-propagate (initial gradients in FP16)</li>
<li>Convert (copy) gradients to FP32</li>
<li>Update the FP32 master weights with your optimizer (SGD/Adam, etc.).</li>
<li>Cast the updated weights back to FP16 for the next forward pass.</li>
</ol>
<h3 id="nlp-22_llm_training_basics-preventing-gradient-underflow-dynamic-loss-scaling">Preventing Gradient Underflow: Dynamic Loss Scaling<a class="headerlink" href="#nlp-22_llm_training_basics-preventing-gradient-underflow-dynamic-loss-scaling" title="Permanent link">¶</a></h3>
<p>Even with mixed precision, tiny gradients can vanish. A practical recipe, adapted from:</p>
<p>Procedure:</p>
<ul>
<li>Choose a loss‑scaling factor <span class="arithmatex">\(S\)</span> (e.g.\ powers of two).\footnote{Modern frameworks adjust <span class="arithmatex">\(S\)</span> automatically (“dynamic” scaling).}</li>
<li>Multiply the computed loss by <span class="arithmatex">\(S\)</span> before back‑propagation.</li>
<li>Compute FP16 gradients of the scaled loss.</li>
<li>Convert gradients to FP32 and divide them by <span class="arithmatex">\(S\)</span>.</li>
<li>Proceed with the usual FP32 weight update \&amp; casting loop above.</li>
</ul>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../nlp/images/mixedprecision.png" data-desc-position="bottom"><img alt="Pytorch - Mixed Precision Training" src="../nlp/images/mixedprecision.png"></a></p>
<h3 id="nlp-22_llm_training_basics-benefits">Benefits<a class="headerlink" href="#nlp-22_llm_training_basics-benefits" title="Permanent link">¶</a></h3>
<ul>
<li><span class="arithmatex">\(\approx 2\times\)</span> faster math on Tensor Cores (Volta/Ampere+)</li>
<li><span class="arithmatex">\(\approx 2\times\)</span> lower memory footprint, enabling larger batch sizes or models.</li>
<li>Same convergence as FP32 when loss scaling is correct</li>
</ul>
<h3 id="nlp-22_llm_training_basics-caveats">Caveats<a class="headerlink" href="#nlp-22_llm_training_basics-caveats" title="Permanent link">¶</a></h3>
<ul>
<li>Verify numerical stability on your model; some niche layers (e.g. custom CUDA kernels) may not be FP16‑safe.</li>
<li>Loss scaling adds minor overhead if done manually; use the built‑in API of your deep‑learning framework whenever possible.</li>
</ul>
<h2 id="nlp-22_llm_training_basics-bfloat16">BFloat16<a class="headerlink" href="#nlp-22_llm_training_basics-bfloat16" title="Permanent link">¶</a></h2>
<p>Google Brain developed bfloat (brain floating point) in 2018 to address this issue. bfloat16 uses the same memory as float16 but has the same dynamic range as float32! The only catch is that the resolution is worse, but this matters less for deep learning.</p>
<p>Core idea: BFloat16 (16-bit float) keeps the same exponent size as FP32, so it has a very wide range but with lower precision (7 mantissa bits).  </p>
<p>This makes it:</p>
<ul>
<li>Fast and memory-efficient like FP16</li>
<li>Avoids gradient underflow</li>
<li>No loss scaling required</li>
</ul>
<h4 id="nlp-22_llm_training_basics-implementation-steps">Implementation steps<a class="headerlink" href="#nlp-22_llm_training_basics-implementation-steps" title="Permanent link">¶</a></h4>
<ol>
<li>Keep a set og FP32 master weights</li>
<li>Cast model to BFloat16 for forward/backward passes(<code>model.to(torch.bfloat16)</code>)</li>
<li>Gradients accumulate in FP32 by default</li>
<li>Update FP32 master weights as usual</li>
<li>Cast updated weights back to BFloat16 for next step</li>
</ol>
<p>No loss scaling required, BFloat16 simplifies mixed-precision training by providing FP32-like range with FP16-like speed and memory savings.</p>
<h2 id="nlp-22_llm_training_basics-multi-gpu-training-from-ddp-to-zero">Multi-GPU Training: From DDP to ZeRO<a class="headerlink" href="#nlp-22_llm_training_basics-multi-gpu-training-from-ddp-to-zero" title="Permanent link">¶</a></h2>
<ol>
<li>
<p>Single-GPU Setup: In a typical mixed-precision setup:</p>
<ul>
<li>Model parameters: Stored in FP16 on GPU VRAM.</li>
<li>Optimizer state (FP32):<ul>
<li>Master weights (for precision-preserving updates)</li>
<li>Momentum buffers (e.g., Adam)</li>
<li>Variance estimates (e.g., Adam)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Distributed Data Parallel (DDP) — Baseline: DDP replicates the entire model and optimizer state across all GPUs and splits the input data:</p>
<ul>
<li>
<p>Each GPU computes a forward and backward pass on its mini-batch.</p>
</li>
<li>
<p>During backpropagation, gradients are synchronized across all GPUs using an AllReduce.</p>
</li>
<li>
<p>Each GPU independently applies the optimizer update to its local copy of the model.</p>
</li>
</ul>
</li>
</ol>
<p>Communication cost: Each gradient (typically in FP32 unless cast to FP16) is sent across GPUs. The bandwidth cost is roughly ∼4 bytes per parameter (or ∼2 bytes with
FP16 compression)</p>
<p>Limitation: Memory overhead scales poorly — each GPU stores a full copy of model parameters and optimizer state.</p>
<h2 id="nlp-22_llm_training_basics-zero-zero-redundancy-optimizer-deepspeed">ZeRO: Zero Redundancy Optimizer (DeepSpeed)<a class="headerlink" href="#nlp-22_llm_training_basics-zero-zero-redundancy-optimizer-deepspeed" title="Permanent link">¶</a></h2>
<p>ZeRO removes the major inefficiency of Distributed Data Parallel (DDP), where every GPU keeps a full copy of model parameters, gradients, and optimizer states. Instead, ZeRO partitions (shards) training states across GPUs so that each device holds only the slice of memory it actually needs, while still participating in full model training.</p>
<h3 id="nlp-22_llm_training_basics-zero-stage-1-optimizer-state-sharding">ZeRO Stage-1: Optimizer State Sharding<a class="headerlink" href="#nlp-22_llm_training_basics-zero-stage-1-optimizer-state-sharding" title="Permanent link">¶</a></h3>
<ul>
<li>Each GPU stores:<ul>
<li>The full FP16 model</li>
<li>Only a shard of the optimizer state</li>
</ul>
</li>
<li>Each GPU computes gradients on its data shard</li>
<li>Gradients are reduce-scattered so each GPU receives only the gradients for its parameter shard</li>
<li>Each GPU updates its own optimizer shard and corresponding parameters</li>
<li>Updated parameters are all-gathered so all GPUs hold a synchronized model</li>
</ul>
<h3 id="nlp-22_llm_training_basics-zero-stage-2-optimizer-state-gradient-sharding">ZeRO Stage-2: Optimizer State + Gradient Sharding<a class="headerlink" href="#nlp-22_llm_training_basics-zero-stage-2-optimizer-state-gradient-sharding" title="Permanent link">¶</a></h3>
<p>Builds upon Stage-1 by also sharding gradients:</p>
<ul>
<li>Gradients are never fully instantiated in memory</li>
<li>During backward pass:<ul>
<li>Each GPU computes gradients for a layer</li>
<li>Immediately reduces gradients to the GPU responsible for that parameter shard</li>
<li>Frees local gradient memory once sent</li>
</ul>
</li>
<li>After backpropagation, each GPU:<ul>
<li>Updates its optimizer and parameter shards locally</li>
<li>Parameters are synchronized via all-gather before the next forward pass</li>
</ul>
</li>
</ul>
<p>Benefit: significantly reduces memory footprint — gradients and optimizer states are distributed.</p>
<h2 id="nlp-22_llm_training_basics-zero-stage-3-full-model-gradient-and-optimizer-sharding">ZeRO Stage-3: Full Model, Gradient, and Optimizer Sharding<a class="headerlink" href="#nlp-22_llm_training_basics-zero-stage-3-full-model-gradient-and-optimizer-sharding" title="Permanent link">¶</a></h2>
<p>In this final stage:</p>
<ul>
<li>Model parameters are sharded across GPUs — no GPU stores the full model</li>
<li>Parameters are materialized just-in-time during forward/backward and deallocated afterward</li>
<li>Training requires orchestration of:<ul>
<li>Parameter gathering</li>
<li>Gradient sharding and reduction</li>
<li>Activation checkpointing (optional but common)</li>
</ul>
</li>
<li>Implemented in PyTorch via <code>torch.distributed.fsdp</code> (Fully Sharded Data Parallel)</li>
</ul>
<p>Use case: enables training of models with hundreds of billions to trillions of parameters on commodity GPU clusters.</p>
<table>
<thead>
<tr>
<th>Training Strategy</th>
<th>Model Sharded</th>
<th>Gradients Sharded</th>
<th>Optimizer Sharded</th>
</tr>
</thead>
<tbody>
<tr>
<td>DDP (Baseline)</td>
<td><span class="arithmatex">\(\times\)</span></td>
<td><span class="arithmatex">\(\times\)</span></td>
<td><span class="arithmatex">\(\times\)</span></td>
</tr>
<tr>
<td>ZeRO Stage-1</td>
<td><span class="arithmatex">\(\times\)</span></td>
<td><span class="arithmatex">\(\times\)</span></td>
<td><span class="arithmatex">\(\checkmark\)</span></td>
</tr>
<tr>
<td>ZeRO Stage-2</td>
<td><span class="arithmatex">\(\times\)</span></td>
<td><span class="arithmatex">\(\checkmark\)</span></td>
<td><span class="arithmatex">\(\checkmark\)</span></td>
</tr>
<tr>
<td>ZeRO Stage-3</td>
<td><span class="arithmatex">\(\checkmark\)</span></td>
<td><span class="arithmatex">\(\checkmark\)</span></td>
<td><span class="arithmatex">\(\checkmark\)</span></td>
</tr>
</tbody>
</table>
<h2 id="nlp-22_llm_training_basics-key-takeaways">Key Takeaways<a class="headerlink" href="#nlp-22_llm_training_basics-key-takeaways" title="Permanent link">¶</a></h2>
<ul>
<li>ZeRO reduces memory consumption linearly with GPU count.</li>
<li>Enables training of large models without sacrificing batch size or needing model parallelism.</li>
<li>Fully Sharded Data Parallel (FSDP) in PyTorch and DeepSpeed provide user-friendly APIs to leverage ZeRO at scale.</li>
</ul></body></html></section><section class="print-page" id="nlp-23_reasoning" heading-number="2.23"><html><head>
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head><body><h1 id="reasoning">Reasoning<a class="headerlink" href="#nlp-23_reasoning-reasoning" title="Permanent link">¶</a></h1>
<h2 id="nlp-23_reasoning-what-do-we-mean-by-reasoning">What Do We Mean by “Reasoning”?<a class="headerlink" href="#nlp-23_reasoning-what-do-we-mean-by-reasoning" title="Permanent link">¶</a></h2>
<p>Reasoning is the disciplined use of facts and logic to reach new conclusions.</p>
<ul>
<li>Deductive reasoning derives conclusions that must be true if the premises are true.</li>
<li>Inductive reasoning generalises from repeated observations to predict what is likely to hold in future.</li>
<li>Abductive reasoning infers the most plausible explanation for an observation (“inference to the best explanation”).</li>
</ul>
<h2 id="nlp-23_reasoning-reasoning-in-large-language-models-llms">Reasoning in Large Language Models (LLMs)<a class="headerlink" href="#nlp-23_reasoning-reasoning-in-large-language-models-llms" title="Permanent link">¶</a></h2>
<p>Large language models excel at predicting the next token given a context — essentially a massively multivariate pattern-completion task. Whether this <em>is</em> reasoning is an open research question, but several techniques reliably elicit reasoning-like behavior.</p>
<h3 id="nlp-23_reasoning-prompt-engineering">Prompt Engineering<a class="headerlink" href="#nlp-23_reasoning-prompt-engineering" title="Permanent link">¶</a></h3>
<ul>
<li>Chain-of-Thought (CoT) prompts append reasoning demonstrations (e.g., "Let's think step-by-step") to encourage the model to reveal latent intermediate states. </li>
<li>Least-to-Most (LtM) decomposition breaks a hard task into a sequence of smaller sub‑problems; the LLM solves each sub‑problem in order, reducing error accumulation. </li>
</ul>
<h3 id="nlp-23_reasoning-counterfactual-probes">Counterfactual Probes<a class="headerlink" href="#nlp-23_reasoning-counterfactual-probes" title="Permanent link">¶</a></h3>
<p>By editing premises and comparing completions, researchers can distinguish memorisation from real generalisation: if the model’s answer tracks the \emph{counterfactual} change, it has formed some causal abstraction rather than merely retrieving training data.</p>
<h3 id="nlp-23_reasoning-limitations">Limitations<a class="headerlink" href="#nlp-23_reasoning-limitations" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Current LLMs have no explicit logical machinery or world model; apparent <code>logic</code> emerges from statistical correlations. </p>
</li>
<li>
<p>Faithfulness is not guaranteed: the model may generate fluent but incorrect reasoning chains (hallucinations).</p>
</li>
<li>
<p>Working memory is finite, so long multi‑step proofs can overflow the context window unless external scratchpads or tool use are provided.</p>
</li>
</ul></body></html></section></section></div><style>.print-site-enumerate-headings #index > h1:before { content: '1 ' }

                .print-site-enumerate-headings #index h2:before { content: '1.' counter(counter-index-2) ' ' }
                .print-site-enumerate-headings #index h2 {  counter-reset: counter-index-3 ;  counter-increment: counter-index-2 }
            
                .print-site-enumerate-headings #index h3:before { content: '1.' counter(counter-index-2) '.' counter(counter-index-3) ' ' }
                .print-site-enumerate-headings #index h3 {  counter-increment: counter-index-3 }
            
.print-site-enumerate-headings #section-2 > h1:before { content: '2 ' }
.print-site-enumerate-headings #nlp-1_intro > h1:before { content: '2.1 ' }

                .print-site-enumerate-headings #nlp-1_intro h2:before { content: '2.1.' counter(counter-nlp-1_intro-2) ' ' }
                .print-site-enumerate-headings #nlp-1_intro h2 {  counter-increment: counter-nlp-1_intro-2 }
            
.print-site-enumerate-headings #nlp-2_lmnp > h1:before { content: '2.2 ' }

                .print-site-enumerate-headings #nlp-2_lmnp h2:before { content: '2.2.' counter(counter-nlp-2_lmnp-2) ' ' }
                .print-site-enumerate-headings #nlp-2_lmnp h2 {  counter-increment: counter-nlp-2_lmnp-2 }
            
.print-site-enumerate-headings #nlp-3_nn > h1:before { content: '2.3 ' }

                .print-site-enumerate-headings #nlp-3_nn h2:before { content: '2.3.' counter(counter-nlp-3_nn-2) ' ' }
                .print-site-enumerate-headings #nlp-3_nn h2 {  counter-increment: counter-nlp-3_nn-2 }
            
.print-site-enumerate-headings #nlp-4_rnn > h1:before { content: '2.4 ' }

                .print-site-enumerate-headings #nlp-4_rnn h2:before { content: '2.4.' counter(counter-nlp-4_rnn-2) ' ' }
                .print-site-enumerate-headings #nlp-4_rnn h2 {  counter-increment: counter-nlp-4_rnn-2 }
            
.print-site-enumerate-headings #nlp-5_lstm > h1:before { content: '2.5 ' }

                .print-site-enumerate-headings #nlp-5_lstm h2:before { content: '2.5.' counter(counter-nlp-5_lstm-2) ' ' }
                .print-site-enumerate-headings #nlp-5_lstm h2 {  counter-increment: counter-nlp-5_lstm-2 }
            
.print-site-enumerate-headings #nlp-6_app_rnn > h1:before { content: '2.6 ' }

                .print-site-enumerate-headings #nlp-6_app_rnn h2:before { content: '2.6.' counter(counter-nlp-6_app_rnn-2) ' ' }
                .print-site-enumerate-headings #nlp-6_app_rnn h2 {  counter-increment: counter-nlp-6_app_rnn-2 }
            
.print-site-enumerate-headings #nlp-7_eval_nlp > h1:before { content: '2.7 ' }

                .print-site-enumerate-headings #nlp-7_eval_nlp h2:before { content: '2.7.' counter(counter-nlp-7_eval_nlp-2) ' ' }
                .print-site-enumerate-headings #nlp-7_eval_nlp h2 {  counter-increment: counter-nlp-7_eval_nlp-2 }
            
.print-site-enumerate-headings #nlp-8_attention_seq2seq > h1:before { content: '2.8 ' }

                .print-site-enumerate-headings #nlp-8_attention_seq2seq h2:before { content: '2.8.' counter(counter-nlp-8_attention_seq2seq-2) ' ' }
                .print-site-enumerate-headings #nlp-8_attention_seq2seq h2 {  counter-increment: counter-nlp-8_attention_seq2seq-2 }
            
.print-site-enumerate-headings #nlp-9_selfattention > h1:before { content: '2.9 ' }

                .print-site-enumerate-headings #nlp-9_selfattention h2:before { content: '2.9.' counter(counter-nlp-9_selfattention-2) ' ' }
                .print-site-enumerate-headings #nlp-9_selfattention h2 {  counter-increment: counter-nlp-9_selfattention-2 }
            
.print-site-enumerate-headings #nlp-10_archi > h1:before { content: '2.10 ' }

                .print-site-enumerate-headings #nlp-10_archi h2:before { content: '2.10.' counter(counter-nlp-10_archi-2) ' ' }
                .print-site-enumerate-headings #nlp-10_archi h2 {  counter-increment: counter-nlp-10_archi-2 }
            
.print-site-enumerate-headings #nlp-11_tokens > h1:before { content: '2.11 ' }

                .print-site-enumerate-headings #nlp-11_tokens h2:before { content: '2.11.' counter(counter-nlp-11_tokens-2) ' ' }
                .print-site-enumerate-headings #nlp-11_tokens h2 {  counter-increment: counter-nlp-11_tokens-2 }
            
.print-site-enumerate-headings #nlp-12_pretraining > h1:before { content: '2.12 ' }

                .print-site-enumerate-headings #nlp-12_pretraining h2:before { content: '2.12.' counter(counter-nlp-12_pretraining-2) ' ' }
                .print-site-enumerate-headings #nlp-12_pretraining h2 {  counter-increment: counter-nlp-12_pretraining-2 }
            
.print-site-enumerate-headings #nlp-13_pretrain_strat > h1:before { content: '2.13 ' }

                .print-site-enumerate-headings #nlp-13_pretrain_strat h2:before { content: '2.13.' counter(counter-nlp-13_pretrain_strat-2) ' ' }
                .print-site-enumerate-headings #nlp-13_pretrain_strat h2 {  counter-increment: counter-nlp-13_pretrain_strat-2 }
            
.print-site-enumerate-headings #nlp-14_finetuning > h1:before { content: '2.14 ' }

                .print-site-enumerate-headings #nlp-14_finetuning h2:before { content: '2.14.' counter(counter-nlp-14_finetuning-2) ' ' }
                .print-site-enumerate-headings #nlp-14_finetuning h2 {  counter-increment: counter-nlp-14_finetuning-2 }
            
.print-site-enumerate-headings #nlp-15_cot > h1:before { content: '2.15 ' }

                .print-site-enumerate-headings #nlp-15_cot h2:before { content: '2.15.' counter(counter-nlp-15_cot-2) ' ' }
                .print-site-enumerate-headings #nlp-15_cot h2 {  counter-increment: counter-nlp-15_cot-2 }
            
.print-site-enumerate-headings #nlp-16_nlg > h1:before { content: '2.16 ' }

                .print-site-enumerate-headings #nlp-16_nlg h2:before { content: '2.16.' counter(counter-nlp-16_nlg-2) ' ' }
                .print-site-enumerate-headings #nlp-16_nlg h2 {  counter-increment: counter-nlp-16_nlg-2 }
            
.print-site-enumerate-headings #nlp-17_imp_nlg > h1:before { content: '2.17 ' }

                .print-site-enumerate-headings #nlp-17_imp_nlg h2:before { content: '2.17.' counter(counter-nlp-17_imp_nlg-2) ' ' }
                .print-site-enumerate-headings #nlp-17_imp_nlg h2 {  counter-increment: counter-nlp-17_imp_nlg-2 }
            
.print-site-enumerate-headings #nlp-18_eval_nlu > h1:before { content: '2.18 ' }

                .print-site-enumerate-headings #nlp-18_eval_nlu h2:before { content: '2.18.' counter(counter-nlp-18_eval_nlu-2) ' ' }
                .print-site-enumerate-headings #nlp-18_eval_nlu h2 {  counter-increment: counter-nlp-18_eval_nlu-2 }
            
.print-site-enumerate-headings #nlp-19_eval_nlg > h1:before { content: '2.19 ' }

                .print-site-enumerate-headings #nlp-19_eval_nlg h2:before { content: '2.19.' counter(counter-nlp-19_eval_nlg-2) ' ' }
                .print-site-enumerate-headings #nlp-19_eval_nlg h2 {  counter-increment: counter-nlp-19_eval_nlg-2 }
            
.print-site-enumerate-headings #nlp-20_post_training > h1:before { content: '2.20 ' }

                .print-site-enumerate-headings #nlp-20_post_training h2:before { content: '2.20.' counter(counter-nlp-20_post_training-2) ' ' }
                .print-site-enumerate-headings #nlp-20_post_training h2 {  counter-increment: counter-nlp-20_post_training-2 }
            
.print-site-enumerate-headings #nlp-21_advanced_topics > h1:before { content: '2.21 ' }

                .print-site-enumerate-headings #nlp-21_advanced_topics h2:before { content: '2.21.' counter(counter-nlp-21_advanced_topics-2) ' ' }
                .print-site-enumerate-headings #nlp-21_advanced_topics h2 {  counter-increment: counter-nlp-21_advanced_topics-2 }
            
.print-site-enumerate-headings #nlp-22_llm_training_basics > h1:before { content: '2.22 ' }

                .print-site-enumerate-headings #nlp-22_llm_training_basics h2:before { content: '2.22.' counter(counter-nlp-22_llm_training_basics-2) ' ' }
                .print-site-enumerate-headings #nlp-22_llm_training_basics h2 {  counter-increment: counter-nlp-22_llm_training_basics-2 }
            
.print-site-enumerate-headings #nlp-23_reasoning > h1:before { content: '2.23 ' }

                .print-site-enumerate-headings #nlp-23_reasoning h2:before { content: '2.23.' counter(counter-nlp-23_reasoning-2) ' ' }
                .print-site-enumerate-headings #nlp-23_reasoning h2 {  counter-increment: counter-nlp-23_reasoning-2 }
            </style>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2025 Salman Khan — Educational Use Only
    </div>
  
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/SalK91/reinforcement_learning" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "/language_modelling/", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>