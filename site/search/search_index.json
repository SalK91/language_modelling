{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#lectures-on-language-modelling","title":"Lectures on Language Modelling","text":"<p>Welcome to Lectures on RLanguage Modelling, a structured set of lecture notes designed to build the mathematical foundations required to understand, analyze, and develop modern language models.</p> <p>These notes are inspired by and draw heavily on material from:</p> <ul> <li>Stanford CS224N: NLP with Deep Learning (Spring 2024)</li> <li>Stanford CME295: Transformers &amp; LLMs (Autumn 2025) </li> </ul> <p>The goal is not to reproduce these courses, but to synthesize their core ideas into a coherent, optimization and mathematics first perspective suitable for practitioners and researchers.</p>"},{"location":"#resume-from","title":"Resume from:","text":"<p>https://cme295.stanford.edu/syllabus/</p> <p>https://www.youtube.com/watch?v=PmW_TMQ3l0I&amp;t=17s</p> <p>https://cs329a.stanford.edu/ https://cseweb.ucsd.edu/~yiying/cse291a-fall25/reading/ https://rdi.berkeley.edu/agentic-ai/f25</p>"},{"location":"nlp/10_archi/","title":"10. Transformer Architectures","text":""},{"location":"nlp/10_archi/#transformer-architectures","title":"Transformer Architectures","text":"<p>Transformer architectures can be categorized based on how attention is applied to input and output sequences, leading to encoder-only, decoder-only, and encoder\u2013decoder designs.</p>"},{"location":"nlp/10_archi/#decoder","title":"Decoder","text":"<p>The Transformer Decoder consists of a stack of identical Transformer Decoder Blocks. Each block contains the following components in sequence:</p> <ul> <li>Masked self-attention: Allows each position to attend only to previous positions in the sequence, enforcing auto-regressive generation  </li> <li>Add and norm: Residual connection followed by layer normalization  </li> <li>Feed-forward network (FFN): A position-wise fully connected feed-forward network  </li> <li>Add and norm: Another residual connection and layer normalization  </li> </ul> <p>This structure ensures that the decoder generates tokens autoregressively, relying solely on previously generated outputs.</p> <p>Decoder models use only the decoder of a Transformer model. At each stage, for a given word, the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.</p> <p>The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p> <p>These models are best suited for tasks involving text generation.</p> <p></p>"},{"location":"nlp/10_archi/#encoder","title":"Encoder","text":"<p>The Transformer Encoder differs by enabling bidirectional context, analogous to bidirectional RNNs. This is achieved by removing the causal masking in the self-attention mechanism, allowing each token to attend to all tokens in the sequence.</p> <p>Each encoder block consists of:</p> <ul> <li>Self-attention (unmasked): Enables full bidirectional context  </li> <li>Add and norm  </li> <li>Feed-forward network  </li> <li>Add and norm  </li> </ul> <p>This allows the encoder to build comprehensive contextual representations of the input sequence.</p> <p></p> <p>The pretraining of these models usually revolves around corrupting a given sentence (for instance, by masking random words) and tasking the model with reconstructing the original sentence.</p> <p>Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.</p>"},{"location":"nlp/10_archi/#encoder-decoder","title":"Encoder-Decoder","text":"<p>For sequence-to-sequence tasks such as machine translation, the source sentence is encoded with a bidirectional model, while the target sentence is generated with a unidirectional model. The Transformer encoder-decoder architecture formalizes this approach by combining:</p> <ul> <li>A standard Transformer encoder to process the entire source sequence  </li> <li>A modified Transformer decoder that, in addition to masked self-attention, performs cross-attention over the encoder outputs  </li> </ul> <p></p>"},{"location":"nlp/10_archi/#cross-attention","title":"Cross-attention.","text":"<p>Cross-attention differs from self-attention in that the keys and values come from the encoder output, while the queries come from the decoder input states.</p> <p>Formally, let</p> \\[ h_1, \\ldots, h_n \\in \\mathbb{R}^d \\] <p>be the encoder output vectors, and</p> \\[ z_1, \\ldots, z_m \\in \\mathbb{R}^d \\] <p>be the decoder input vectors. Then the keys and values drawn from the encoder are</p> \\[ K = W_k h_i, \\quad V = W_h h_i \\] <p>and the queries are</p> \\[ Q = W_z z_j \\] <p>where \\(W_k\\), \\(W_h\\), and \\(W_z\\) are learned projection matrices.</p> <p>Cross-attention allows each decoder state to selectively retrieve information from the entire encoded source sequence.</p> <p></p> <p>The decoder uses these queries to attend to the encoder memory via the cross-attention mechanism, enabling selective focus on relevant parts of the source sequence when generating each target token.</p> <p>The pretraining of these models can take different forms, but it often involves reconstructing a sentence for which the input has been corrupted. For example, the pretraining of the T5 model replaces random spans of text (which may contain multiple words) with a single mask token, and the task is to predict the text that the mask token replaces.</p> <p>Sequence-to-sequence models are best suited for tasks involving generating new sentences conditioned on an input, such as summarization, translation, or generative question answering.</p>"},{"location":"nlp/10_archi/#limitations-and-areas-for-improvement-in-the-transformer","title":"Limitations and Areas for Improvement in the Transformer","text":"<p>Despite its success, the Transformer architecture has some notable limitations that motivate ongoing research:</p> <ul> <li> <p>Quadratic computational complexity in self-attention:   The self-attention mechanism requires computing interactions between all pairs of tokens in the input sequence, resulting in computation and memory complexity of \\(O(n^2 d)\\), where \\(n\\) is the sequence length and \\(d\\) is the dimensionality of the embeddings. Specifically, the attention matrix \\(QK^\\top \\in \\mathbb{R}^{n \\times n}\\) encodes all pairwise interactions, which becomes prohibitive for very long sequences.For example, with \\(n = 512\\), the number of pairwise interactions exceeds \\(262{,}000\\). For sequences with \\(n \\ge 50{,}000\\), this quadratic growth becomes infeasible. In contrast, recurrent models have computational complexity that grows linearly with sequence length.</p> </li> <li> <p>Position representations:  Transformers typically use simple absolute positional encodings, such as sinusoidal embeddings, to inject information about token order. However, it remains an open question whether absolute indices are the optimal way to represent positional information. Alternatives such as relative position encodings and syntax-aware position embeddings have been proposed to capture dependencies more naturally and improve model performance.</p> </li> </ul>"},{"location":"nlp/11_tokens/","title":"11. Tokenization","text":""},{"location":"nlp/11_tokens/#11-tokenization-and-tokens","title":"11. Tokenization and Tokens","text":"<p>Tokenization defines the basic units over which a language model reasons and therefore strongly influences model capacity, efficiency, and generalization. Raw text is typically represented as Unicode strings, for example:</p> \\[ \\texttt{string = \"Hello, How are you?\"} \\] <p>Language models operate over sequences of tokens, which are usually represented as integer indices. For example:</p> \\[ \\texttt{indices = [15496, 11, 995, 0]} \\] <p>These integer indices serve as inputs to embedding layers, which map discrete tokens into continuous vector representations.</p> <ul> <li>A tokenizer is a class that implements two main functions:</li> <li>encode: Converts a string into tokens (integers)</li> <li>decode: Converts tokens back into a string</li> <li>The vocabulary size refers to the number of distinct tokens in the model's dictionary.</li> </ul> <p>In practice, tokenizers are deterministic and shared between training and inference.</p> <p>For interactive exploration, try:  Link</p> <ul> <li> <p>Character-based tokenization: Character-based tokenizers split the input text into individual characters rather than words or subwords. This results in a much smaller and fixed vocabulary (e.g., all letters, digits, p. unctuation, and special characters). Every possible word can be represented, eliminating most out-of-vocabulary (OOV) issues</p> <ul> <li> <p>Pros:</p> <ul> <li>Extremely small vocabulary size</li> <li>Fully lossless representation \u2014 any text can be encoded and decoded perfectly</li> <li>Robust to typos and unknown words \u2014 e.g., <code>\"xylocopter\"</code> can still be tokenized, even if it is a made-up word</li> </ul> </li> <li> <p>Cons:</p> <ul> <li>Characters are semantically weak \u2014 individual letters carry little meaning in Latin-based languages</li> <li>Sequence lengths become much longer. For example, the word <code>\"tokenization\"</code> becomes 13 tokens instead of 1</li> <li>More computationally expensive due to increased sequence length</li> <li>May struggle to capture meaningful patterns unless large contexts are modeled</li> </ul> </li> </ul> </li> </ul> <p>As a result, character-level models typically require deeper architectures or longer contexts to learn meaningful structure.</p> <ul> <li> <p>Word-based tokenization:Splits text into words and punctuation using regular expressions.Each word is treated as a distinct token. For example, <code>\"dog\"</code> and <code>\"dogs\"</code> are different tokens even though they share semantic roots. Vocabulary size depends on the number of unique words in the corpus and is often extremely large. Word-based tokenization was the dominant approach in early statistical NLP systems.</p> <ul> <li>Pros: Shorter sequences and semantically intuitive tokens</li> <li>Cons:<ul> <li>Very large and variable vocabulary size</li> <li>Morphologically similar words (e.g., <code>dog</code> vs <code>dogs</code>) are treated as unrelated tokens</li> <li>Rare or unseen words require a special <code>[UNK]</code> (unknown) token</li> <li>Vocabulary sparsity makes learning harder, especially for morphologically rich languages These limitations are especially pronounced in languages with rich morphology or productive word formation.</li> </ul> </li> </ul> </li> </ul>"},{"location":"nlp/11_tokens/#subword-modeling","title":"Subword Modeling","text":""},{"location":"nlp/11_tokens/#motivation-and-linguistic-foundations","title":"Motivation and Linguistic Foundations","text":"<p>Subword modeling strikes a compromise between character-level and word-level tokenization.</p> <p>Traditional NLP systems often assume a fixed-size vocabulary constructed from the training corpus, typically comprising tens or hundreds of thousands of words. This approach assigns a unique token such as <code>[UNK]</code> to all out-of-vocabulary (OOV) words encountered at inference time, introducing brittleness and data sparsity, especially for morphologically rich languages.</p> <p>Many natural languages exhibit complex morphology, where individual words encode multiple grammatical or semantic features. For example, in Swahili, a single verb such as ambia (\u201cto tell\u201d) can yield hundreds of morphologically inflected forms, incorporating tense, aspect, negation, mood, definiteness, and object agreement. Relying on full-word tokenization under such conditions results in large vocabularies with sparse frequency distributions, undermining the statistical efficiency of learned models.</p>"},{"location":"nlp/11_tokens/#subword-units-as-a-solution","title":"Subword Units as a Solution","text":"<p>Subword modeling addresses the limitations of fixed-vocabulary word-based tokenization by decomposing words into smaller, more frequent units such as morphemes, syllables, or even individual characters. This approach enables:</p> <ul> <li>Generalization to unseen words via composition of known subword units</li> <li>Mitigation of the OOV problem by modeling open vocabularies</li> <li>Better cross-linguistic applicability, especially for agglutinative and polysynthetic languages</li> <li>More compact and data-efficient language representations</li> </ul>"},{"location":"nlp/11_tokens/#byte-pair-encoding-bpe","title":"Byte-Pair Encoding (BPE)","text":"<p>A prominent algorithm for data-driven subword segmentation is Byte-Pair Encoding (BPE), originally adapted from data compression and later introduced to NLP in neural machine translation. The core idea is to iteratively learn a vocabulary of subword units by merging the most frequent pairs of adjacent symbols in a corpus. BPE constructs subword units by greedily merging frequent symbol pairs.</p> <p>The BPE algorithm operates as follows:</p> <ol> <li>Initialize the vocabulary with all individual characters and a special end-of-word symbol (e.g., <code>_</code>)</li> <li>Count all symbol pairs in the corpus and identify the most frequent pair</li> <li>Merge the most frequent pair into a new symbol and update the corpus accordingly</li> <li>Repeat steps 2\u20133 until the desired vocabulary size is reached</li> </ol> <p>This procedure yields a deterministic and greedy segmentation strategy, allowing both training and inference to tokenize consistently using the learned merge rules. BPE produces variable-length subword units that often correspond to linguistically meaningful morphemes (e.g., prefixes, stems, suffixes), while maintaining robustness across diverse scripts and languages.</p> <p>To illustrate the utility of BPE in handling both common and rare or noisy words, consider the following tokenization outcomes using a hypothetical learned subword vocabulary:</p> Input Word BPE Tokenization <code>hat</code> <code>hat</code> <code>learn</code> <code>learn</code> <code>taaaaasty</code> <code>taa## aaa## sty</code> <code>laern</code> <code>la## ern</code> <code>Transformerify</code> <code>Transformer##ify</code> <p>The delimiter <code>##</code> indicates that a subword is not a standalone word but a continuation of a preceding segment. This example demonstrates several key advantages of subword modeling:</p> <ul> <li>Common words such as <code>hat</code> and <code>learn</code> are represented as single units</li> <li>Misspellings or informal variants (e.g., <code>laern</code>, <code>taaaaasty</code>) can still be tokenized into interpretable components using existing subword units</li> <li>Rare or novel words (e.g., <code>Transformerify</code>) are decomposed into known subwords, preserving semantic hints for downstream models</li> </ul>"},{"location":"nlp/11_tokens/#variants-and-modern-usage","title":"Variants and Modern Usage","text":"<p>BPE has inspired several modern variants such as WordPiece and Unigram Language Model tokenization, which are widely used in state-of-the-art pretrained language models including BERT, GPT, and T5. These methods differ in how they select subword units, for example using likelihood-based criteria or probabilistic segmentation, but share the same underlying goal: modeling language compositionally below the word level.</p> <p>Subword modeling constitutes a crucial innovation in modern NLP pipelines, striking a balance between granularity, efficiency, and linguistic fidelity.</p>"},{"location":"nlp/12_pretraining/","title":"12. Pre-training","text":""},{"location":"nlp/12_pretraining/#pre-training","title":"Pre-training","text":"<p>Pretraining refers to learning general-purpose language representations from large amounts of unlabeled text before adapting models to specific tasks. This approach decoupled lexical representation learning from task-specific modeling.</p>"},{"location":"nlp/12_pretraining/#from-pretrained-embeddings-to-pretrained-models","title":"From Pretrained Embeddings to Pretrained Models","text":"<p>Before the rise of large-scale language models, pretraining in NLP primarily focused on static word embeddings such as Word2Vec, GloVe, or FastText. These embeddings provided fixed, context-independent vector representations for each word in the vocabulary, which were then used as input features for downstream models.</p>"},{"location":"nlp/12_pretraining/#circa-2017-word-embeddings-contextual-models","title":"Circa 2017: Word embeddings + contextual models","text":"<p>A common pipeline during this period combined pretrained embeddings with supervised task-specific models.</p> <ul> <li>Begin with pretrained word embeddings (context-agnostic)</li> <li>Learn contextualization during supervised training on a downstream task (e.g., sentiment analysis, question answering)</li> <li>Recurrent architectures (e.g., LSTMs) or early Transformer variants were used to build task-specific context</li> <li>Limitations:<ul> <li>Downstream data must be large and diverse enough to teach the model language understanding</li> <li>Most model parameters are randomly initialized and trained from scratch</li> <li>Inefficient reuse of linguistic knowledge across tasks</li> </ul> </li> </ul> <p>As a result, much of the burden of language understanding was placed on downstream supervision.</p> <p></p>"},{"location":"nlp/12_pretraining/#modern-paradigm-pretraining-whole-models","title":"Modern Paradigm: Pretraining Whole Models","text":"<p>In modern NLP systems, the dominant paradigm has shifted from pretraining isolated components (e.g., word embeddings) to pretraining entire models on unsupervised or self-supervised objectives over massive corpora. These pretrained models are then fine-tuned on specific tasks, often with limited labeled data.</p> <p>Key characteristics of whole-model pretraining</p> <ul> <li>Nearly all model parameters are initialized using large-scale pretraining</li> <li>The model is trained on unlabeled text by corrupting the input and optimizing for its reconstruction. </li> <li>Common objectives: Different pretraining objectives reflect different modeling assumptions and downstream use cases.<ul> <li>Masked language modeling (MLM): Randomly mask tokens and train the model to predict them (e.g., BERT)</li> <li>Causal language modeling (CLM): Predict the next token given previous ones (e.g., GPT)</li> <li>Permutation-based objectives: Learn to reason over non-sequential token orders (e.g., XLNet)</li> </ul> </li> <li>Models learn:<ul> <li>Deep, contextualized representations of language structure and semantics</li> <li>Strong priors for downstream task learning via transfer</li> <li>Coherent probability distributions over sequences, enabling sampling and generation. These learned representations can be rapidly adapted to new tasks through fine-tuning or prompting.</li> </ul> </li> </ul>"},{"location":"nlp/12_pretraining/#benefits-of-pretraining","title":"Benefits of Pretraining","text":"<p>Whole-model pretraining offers several practical and theoretical advantages.</p> <ul> <li>Data efficiency: Fine-tuning requires significantly fewer task-specific labeled examples</li> <li>Robust generalization: Pretrained models capture syntactic, semantic, and world knowledge</li> <li>Unified architecture: A single pretrained backbone can be reused across a wide range of tasks, reducing task-specific engineering</li> <li>Sampling and generation: Language models trained with causal objectives can be used to generate fluent, coherent text</li> </ul> <p></p>"},{"location":"nlp/12_pretraining/#summary","title":"Summary","text":"<p>Pretraining has revolutionized NLP by shifting the burden of language understanding from downstream task data to large-scale, general-purpose models. This transition from static embeddings to pretrained Transformers has enabled rapid progress across virtually every NLP benchmark, making pretraining the foundation of modern language understanding and generation systems.</p>"},{"location":"nlp/13_pretrain_strat/","title":"13. Pre-training Strategies","text":""},{"location":"nlp/13_pretrain_strat/#13-the-pretraining-strategies","title":"13. The Pretraining Strategies","text":"<p>This section describes how pretraining objectives differ across encoder, decoder, and encoder\u2013decoder architectures, and why these differences matter.</p> <p>The pretraining/fine-tuning paradigm has become the dominant approach in modern NLP. Instead of training models from scratch for every task, we first train a general-purpose language model on a large, unlabeled corpus, and then adapt it to a specific task using a smaller labeled dataset. This approach enables models to transfer general linguistic knowledge learned during pretraining into a wide variety of downstream applications.</p> <ul> <li>Step 1: Pretraining \u2014 Train on a large-scale unsupervised objective such as language modeling. The model learns broad statistical regularities, syntax, semantics, and even some world knowledge from raw text</li> <li>Step 2: Fine-tuning \u2014 Adapt the pretrained model to a specific supervised task (e.g., sentiment classification, question answering, named entity recognition) using a smaller, labeled dataset</li> </ul> <p></p> <p>These two stages decouple language understanding from task-specific supervision. This two-stage procedure has been remarkably effective, especially when labeled data is scarce, as the pretrained model already encodes a strong inductive bias about language.</p>"},{"location":"nlp/13_pretrain_strat/#why-does-this-work-an-optimization-view","title":"Why Does This Work? An Optimization View","text":"<p>From the perspective of training neural networks with stochastic gradient descent, pretraining provides a highly informative initialization for model parameters.</p> <p>Let \\(\\mathcal{L}_{\\text{pretrain}}(\\theta)\\) denote the pretraining loss, and let \\(\\hat{\\theta}\\) be the parameters obtained by minimizing this loss:</p> \\[ \\hat{\\theta} \\approx \\arg\\min_\\theta \\mathcal{L}_{\\text{pretrain}}(\\theta) \\] <p>During fine-tuning, we optimize a new task-specific loss \\(\\mathcal{L}_{\\text{finetune}}(\\theta)\\), starting from the pretrained parameters:</p> \\[ \\theta^* = \\arg\\min_\\theta \\mathcal{L}_{\\text{finetune}}(\\theta), \\quad \\text{initialized at } \\theta = \\hat{\\theta} \\] <p>This setup offers two complementary advantages:</p> <ol> <li> <p>Good starting point:  The pretrained parameters \\(\\hat{\\theta}\\) already encode general linguistic knowledge, meaning that gradient-based optimization during fine-tuning is more likely to converge quickly and to a good local minimum</p> </li> <li> <p>Better generalization: Due to the geometry of the loss landscape, stochastic gradient descent tends to stay relatively close to \\(\\hat{\\theta}\\). If the local minima around \\(\\hat{\\theta}\\) are well-aligned with generalization, then the fine-tuned model is more likely to perform well on unseen data</p> </li> </ol>"},{"location":"nlp/13_pretrain_strat/#intuition-behind-gradient-propagation","title":"Intuition Behind Gradient Propagation","text":"<p>Another benefit is that the gradients of the fine-tuning loss \\(\\nabla \\mathcal{L}_{\\text{finetune}}(\\theta)\\) often propagate more effectively when \\(\\theta\\) is initialized at \\(\\hat{\\theta}\\). Pretraining shapes the model\u2019s representations such that downstream gradients flow through semantically meaningful feature spaces, improving both optimization stability and sample efficiency.</p>"},{"location":"nlp/13_pretrain_strat/#summary","title":"Summary","text":"<p>The pretraining/fine-tuning paradigm represents a powerful instantiation of transfer learning in NLP. It leverages large-scale unsupervised data to produce models with rich linguistic priors and uses supervised fine-tuning to tailor those priors to task-specific objectives. From both empirical and theoretical perspectives, this strategy enables better generalization, faster convergence, and higher performance across nearly all areas of natural language understanding and generation.</p>"},{"location":"nlp/13_pretrain_strat/#pretraining-encoder-architectures","title":"Pretraining Encoder Architectures","text":"<p>Transformer-based encoder models, particularly those following the BERT architecture, are pretrained using objectives that leverage the bidirectional nature of attention. Unlike autoregressive language models, which condition only on past tokens, encoder architectures benefit from full left-and-right context, enabling richer and more globally informed token representations.</p>"},{"location":"nlp/13_pretrain_strat/#masked-language-modeling-mlm","title":"Masked Language Modeling (MLM)","text":"<p>The core idea behind encoder pretraining is the masked language modeling objective. In this setup, a fraction of the input tokens is replaced with a special <code>[MASK]</code> token. The model is then trained to predict the original tokens at these masked positions, conditioning on the surrounding unmasked context. Formally, given an input sequence  \\(x = (w_1, w_2, \\dots, w_T)\\) and its corrupted version \\(\\tilde{x}\\), the model learns parameters \\(\\theta\\) that maximize the likelihood \\(p_\\theta(x \\mid \\tilde{x})\\).</p> \\[ \\mathbf{h}_1, \\dots, \\mathbf{h}_T = \\text{Encoder}(w_1, \\dots, w_T) \\] \\[ \\hat{y}_i \\sim \\text{softmax}(A \\mathbf{h}_i + b), \\quad \\text{for masked positions } i \\] <p>where \\(A\\) and \\(b\\) are learned projection parameters.</p> <p>The BERT pretraining procedure masks 15% of tokens according to the following strategy:</p> <ul> <li>80% of the time, the token is replaced with <code>[MASK]</code></li> <li>10% of the time, it is replaced with a random token</li> <li>10% of the time, it is left unchanged but still predicted</li> </ul> <p>This scheme prevents the model from overfitting to the presence of <code>[MASK]</code> tokens and encourages robust representations even for unmasked inputs.</p>"},{"location":"nlp/13_pretrain_strat/#next-sentence-prediction-nsp","title":"Next Sentence Prediction (NSP)","text":"<p>In addition to masked language modeling, BERT was originally trained with a binary classification task known as next sentence prediction. Given two input segments, the model predicts whether the second segment follows the first in the original corpus or was randomly sampled. Subsequent studies such as RoBERTa showed that removing NSP can improve downstream performance, suggesting that NSP is not essential.</p>"},{"location":"nlp/13_pretrain_strat/#advancements-and-variants","title":"Advancements and Variants","text":"<p>Numerous refinements to the BERT pretraining methodology have been proposed:</p> <ul> <li> <p>RoBERTa:   Trains longer, on more data, removes NSP, uses dynamic masking and larger batch sizes</p> </li> <li> <p>SpanBERT:  Masks contiguous spans of tokens instead of individual ones, promoting span-level representations</p> </li> </ul>"},{"location":"nlp/13_pretrain_strat/#limitations-of-encoder-only-pretraining","title":"Limitations of Encoder-Only Pretraining","text":"<p>While encoder models like BERT excel at understanding and classification tasks (e.g., sentiment analysis, QA), they are not directly suited for sequence generation due to their non-autoregressive architecture. For tasks requiring fluent text generation (e.g., summarization, translation), decoder-based or encoder-decoder models are more appropriate. Nonetheless, pretrained encoders remain foundational across a wide array of NLP applications due to their strong contextual representations and adaptability to fine-tuning for downstream tasks.</p>"},{"location":"nlp/13_pretrain_strat/#pretraining-encoderdecoder-architectures","title":"Pretraining Encoder\u2013Decoder Architectures","text":"<p>Encoder\u2013decoder models combine the strengths of both architectures: the encoder produces rich, bidirectional representations of input sequences, while the decoder performs autoregressive generation conditioned on these representations. This architecture is particularly well-suited for sequence-to-sequence tasks such as machine translation, summarization, and question answering.</p>"},{"location":"nlp/13_pretrain_strat/#why-use-encoderdecoder-models","title":"Why Use Encoder\u2013Decoder Models?","text":"<ul> <li>Encoders build contextual representations using bidirectional attention</li> <li>Decoders enable autoregressive generation conditioned on encoded input and past outputs</li> <li>Encoder\u2013decoder models enable powerful conditional generation while leveraging deep understanding of the input.</li> </ul>"},{"location":"nlp/13_pretrain_strat/#pretraining-strategy-language-modeling-with-encoders","title":"Pretraining Strategy: Language Modeling with Encoders","text":"<p>A naive extension of language modeling to encoder\u2013decoder architectures splits the input sequence:</p> <ul> <li>A prefix of the input (e.g., tokens \\(w_1, \\dots, w_T\\)) is passed to the encoder.</li> <li>The decoder autoregressively generates the continuation</li> </ul> \\[ \\mathbf{h}_{1:T} = \\text{Encoder}(w_{1:T}), \\quad \\mathbf{h}_{T+1:T'} = \\text{Decoder}(w_{1:T'}, \\mathbf{h}_{1:T}) \\] \\[ P(y_i) \\sim \\text{softmax}(\\mathbf{A}\\mathbf{h}_i + \\mathbf{b}), \\quad i &gt; T \\] <p>This allows the encoder to learn bidirectional features while enabling the decoder to condition on them during generation. However, more specialized objectives have shown better performance.</p>"},{"location":"nlp/13_pretrain_strat/#span-corruption-the-t5-objective","title":"Span Corruption: The T5 Objective","text":"<p>Span corruption is the core pretraining objective of the T5 model. Random spans of text are removed from the input and replaced with unique sentinel tokens such as <code>&lt;extra_id_0&gt;</code>. The decoder is trained to reconstruct the missing spans.</p> <ul> <li>Random spans of text are removed from the input</li> <li>Each removed span is replaced with a unique sentinel token (e.g., <code>&lt;extra_id_0&gt;</code>).</li> <li>The model is trained to reconstruct the missing spans from these placeholders.</li> </ul> <p></p> <p>Example:</p> <ul> <li>Input: <code>The quick &lt;extra_id_0&gt; fox jumps &lt;extra_id_1&gt; the lazy dog.</code></li> <li>Target: <code>&lt;extra_id_0&gt; brown &lt;extra_id_1&gt; over &lt;extra_id_2&gt;</code></li> </ul> <p>Advantages:</p> <ul> <li>The encoder benefits from full bidirectional context (unlike standard causal models).</li> <li>The decoder is trained autoregressively, generating spans conditioned on encoder output.</li> <li>The objective is implemented via input preprocessing; the model learns a language modeling task at the decoder side.</li> </ul>"},{"location":"nlp/13_pretrain_strat/#summary_1","title":"Summary","text":"<p>Encoder\u2013decoder pretraining balances bidirectional representation learning (via the encoder) with generative capability (via the decoder). Span corruption, as implemented in T5, has emerged as a highly effective strategy. It produces models that generalize well across a wide range of NLP tasks and are compatible with the \"text-to-text\" paradigm.</p>"},{"location":"nlp/13_pretrain_strat/#pretraining-decoder-architectures","title":"Pretraining Decoder Architectures","text":"<p>Decoder-only language models have emerged as a central architecture in modern NLP, particularly for tasks involving natural language generation. These models are pretrained autoregressively to maximize the likelihood of the next token conditioned on previous tokens:</p> \\[ p_\\theta(w_t \\mid w_{1:t-1}) \\] <p>typically using causal (left-to-right) self-attention to ensure that each token only attends to its left context. This setup enables decoders to learn rich, context-sensitive representations suitable for both generation and classification tasks.</p> <p></p>"},{"location":"nlp/13_pretrain_strat/#representation-learning-for-classification","title":"Representation Learning for Classification","text":"<p>Despite being trained for generation, pretrained decoders can be repurposed for classification tasks by leveraging their hidden representations. A common approach is to use the final token's hidden state \\(h_T\\) as a summary of the sequence and apply a linear classifier on top:</p> \\[ h_1, \\dots, h_T = \\text{Decoder}(w_1, \\dots, w_T) \\] \\[ y \\sim \\text{softmax}(A h_T + b) \\] <p>where \\(A\\) and \\(b\\) are task-specific parameters initialized randomly and optimized during finetuning. Importantly, gradients are backpropagated through the entire decoder, enabling the model to adapt its internal representations to the downstream task.</p>"},{"location":"nlp/13_pretrain_strat/#sequence-generation-via-pretrained-decoders","title":"Sequence Generation via Pretrained Decoders","text":"<p>In generation settings, decoder models are used directly in the autoregressive manner in which they were pretrained. At each timestep, the decoder produces a distribution over the next token given the previously generated tokens:</p> \\[ h_1, \\dots, h_{t-1} = \\text{Decoder}(w_1, \\dots, w_{t-1}) \\] \\[ w_t \\sim \\text{softmax}(A h_{t-1} + b) \\] <p>Here, \\(A\\) and \\(b\\) represent the output projection matrix and bias that were learned during pretraining. In many applications, these parameters are reused without modification; however, they may also be further finetuned alongside the decoder, depending on the task and data availability.</p> <p>This generation paradigm is particularly effective for tasks such as:</p> <ul> <li>Dialogue modeling: where the decoder generates a response conditioned on the prior dialogue history.</li> <li>Summarization: where the decoder generates a summary conditioned on a document input.</li> </ul>"},{"location":"nlp/13_pretrain_strat/#architectural-context-and-transferability","title":"Architectural Context and Transferability","text":"<p>Decoder-only models differ from encoder-decoder architectures (e.g., T5) in that they do not encode the input with a separate encoder. Instead, all information is processed through the decoder itself. This simplicity enables direct reuse of the pretrained decoder for diverse tasks with minimal architectural modification.</p> <p>Pretraining provides:</p> <ul> <li>Transferability: Learned representations encode rich syntactic, semantic, and factual knowledge that generalize well across tasks.</li> <li>Architectural reuse: The same pretrained model can support both classification and generation, reducing deployment complexity.</li> <li>Scalability: Autoregressive training benefits from large-scale data and scales effectively with model size.</li> </ul> <p>Pretrained decoder architectures, as used in models such as GPT-2, GPT-3, and LLaMA, have demonstrated remarkable performance across a wide range of NLP benchmarks, confirming the utility of this simple yet powerful design.</p>"},{"location":"nlp/14_finetuning/","title":"14. Fine-tuning","text":""},{"location":"nlp/14_finetuning/#fine-tuning-and-parameter-efficient-adaptation","title":"Fine-tuning and Parameter-Efficient Adaptation","text":"<p>The standard adaptation strategy for pretrained language models involves updating all model parameters on a task-specific dataset. This approach is typically effective but computationally expensive.</p> <ul> <li>Method: Unfreeze and update all weights during training</li> <li>Pros:<ul> <li>Strong performance across tasks</li> <li>Deep integration of task-specific signal</li> </ul> </li> <li>Cons:<ul> <li>High memory and compute cost</li> <li>Risk of overfitting, especially in low-resource settings</li> <li>Each task or domain requires storing a full model copy</li> </ul> </li> </ul>"},{"location":"nlp/14_finetuning/#parameter-efficient-fine-tuning-peft","title":"Parameter-Efficient Fine-tuning (PEFT)","text":"<p>PEFT methods aim to adapt large language models by updating only a small fraction of the parameters, or by introducing new lightweight modules, while keeping the backbone frozen.</p> <ul> <li>Reduce training time and GPU memory usage</li> <li>Support multi-task and multi-user deployment</li> <li>Preserve generalization by avoiding full weight drift</li> </ul> <p></p>"},{"location":"nlp/14_finetuning/#prompt-tuning","title":"Prompt Tuning","text":"<p>Prompt tuning learns a set of continuous embeddings (soft prompts) that are prepended to the input. The rest of the model remains frozen.</p> <ul> <li>Method: Optimize only a small set of input-level vectors</li> <li>Pros:<ul> <li>Extremely lightweight (less than 0.1% of total parameters)</li> <li>Easy to implement</li> </ul> </li> <li>Cons:<ul> <li>Unstable for small models</li> <li>Performance gap compared to other PEFT methods on many tasks</li> </ul> </li> </ul>"},{"location":"nlp/14_finetuning/#prefix-tuning","title":"Prefix Tuning","text":"<p>Prefix tuning prepends trainable vectors to the key and value matrices of each Transformer layer, guiding attention.</p> <ul> <li>Method: Learn prefix vectors while freezing the pretrained model</li> <li>Pros:<ul> <li>Efficient (approximately 1\u20132% of total parameters)</li> <li>Stronger performance than prompt tuning on many tasks</li> <li>Task-specific prefixes allow batched inference across tasks</li> </ul> </li> <li>Cons:<ul> <li>More intrusive implementation than prompt tuning</li> </ul> </li> </ul> <p></p>"},{"location":"nlp/14_finetuning/#adapter-tuning","title":"Adapter Tuning","text":"<p>Adapters are small bottleneck layers inserted within each Transformer block. Only adapter parameters are trained.</p> <ul> <li>Method: Add adapter layers after attention and after the feed-forward network, while freezing the base model</li> <li>Pros:<ul> <li>Competitive performance on many NLP tasks</li> <li>Modular design allows composition for multi-task learning</li> </ul> </li> <li>Cons:<ul> <li>Slightly larger parameter footprint than other PEFT methods</li> <li>Requires architecture modification</li> </ul> </li> </ul>"},{"location":"nlp/14_finetuning/#low-rank-adaptation-lora","title":"Low-Rank Adaptation (LoRA)","text":"<p>LoRA learns a low-rank decomposition of the difference between pretrained and fine-tuned weights.</p> <ul> <li>Method: For a linear layer \\(W \\in \\mathbb{R}^{d \\times k}\\), learn \\(\\Delta W = A B\\) where \\(A \\in \\mathbb{R}^{d \\times r}\\) and \\(B \\in \\mathbb{R}^{r \\times k}\\), with \\(r \\ll \\min(d, k)\\)</li> <li>Pros:<ul> <li>Highly parameter-efficient (typically less than 1%)</li> <li>Stable training and strong task performance</li> <li>Easy to integrate into existing models</li> </ul> </li> <li>Cons:<ul> <li>Adds a small compute overhead at inference time</li> </ul> </li> </ul> <p></p> <p>Location of LoRA:</p> <ul> <li>Originally proposed in masked multi-head attention matrices.</li> <li>Nowadays recommended in the feed-forward layers (after attention)</li> </ul> <p>Empirical observations with LoRA:</p> <ul> <li>LoRA needs a higher learning rate than full fine-tuning</li> <li>LoRA does poorly on large batch size compared to full fine-tuning</li> </ul>"},{"location":"nlp/14_finetuning/#quantized-lora","title":"Quantized LoRA","text":"<p>QLoRA trains low-rank adapters (LoRA) on top of a 4-bit quantized frozen LLM, keeping the base weights memory-efficient while still allowing learning. The implementation stores the base model in 4-bit NF4 precision and only updates small \\(A\\) and \\(B\\) matrices (LoRA matrices) in full precision. </p> <p>Intuition: freeze most of the model to save memory, and let tiny trainable adapters learn task-specific changes without touching the huge pretrained weights.</p>"},{"location":"nlp/14_finetuning/#empirical-comparison-of-fine-tuning-methods","title":"Empirical Comparison of Fine-tuning Methods","text":"Method Trainable Params GPU Memory Performance Stability Full Fine-tuning 100% High Strong Stable Prompt Tuning &lt;0.1% Very Low Moderate\u2013Low Unstable (small models) Prefix Tuning ~1\u20132% Low Moderate\u2013High Stable Adapter Tuning ~1\u20135% Moderate High Stable LoRA ~0.1\u20131% Low High Stable <p>Parameter-efficient fine-tuning strategies provide practical tools for adapting large language models without retraining or duplicating the full network. LoRA, adapters, and prefix tuning are currently among the most effective approaches, with trade-offs between memory usage, ease of integration, and task performance. As the size and scope of language models continue to grow, PEFT methods will be essential for scaling personalization, domain adaptation, and multi-task inference.</p>"},{"location":"nlp/15_cot/","title":"15. Chain of Thought Reasoning","text":""},{"location":"nlp/15_cot/#chain-of-thought-reasoning","title":"Chain of Thought Reasoning","text":"<p>While decoder-only models are powerful text generators, they can also exhibit complex reasoning behavior when prompted effectively. One prominent prompting technique is chain of thought (CoT) prompting, which encourages the model to generate intermediate reasoning steps before arriving at a final answer.</p> <p>Unlike direct answer prompting, CoT decomposes complex problems into step-by-step explanations, aligning better with how the model internally represents procedural and logical information. This is particularly beneficial in tasks such as arithmetic reasoning, commonsense inference, and symbolic manipulation.</p>"},{"location":"nlp/15_cot/#standard-prompt-vs-chain-of-thought-prompt","title":"Standard Prompt vs. Chain of Thought Prompt","text":"<p>Consider the task of solving a multi-step math problem. A standard prompt might look like:</p> <p>Q: If there are 3 cars and each car has 4 wheels, how many wheels are there in total? A:</p> <p>A model may correctly answer with <code>12</code>, but it might also fail if it tries to answer without reasoning explicitly.</p> <p></p> <p>In contrast, a chain of thought prompt elicits intermediate steps:</p> <p>Q: If there are 3 cars and each car has 4 wheels, how many wheels are there in total? A: Each car has 4 wheels. There are 3 cars. So the total number of wheels is \\(3 \\times 4 = 12\\). The answer is 12.</p>"},{"location":"nlp/15_cot/#benefits-of-chain-of-thought-prompting","title":"Benefits of Chain of Thought Prompting","text":"<ul> <li>Improved accuracy: CoT often leads to significantly higher accuracy on reasoning-heavy tasks, particularly in few-shot or zero-shot settings  </li> <li>Interpretable outputs: Intermediate steps provide insights into model behavior, making it easier to debug or assess alignment  </li> <li>Alignment with human reasoning: CoT mimics how humans solve problems, which can be useful in educational or assistive applications  </li> </ul>"},{"location":"nlp/15_cot/#model-compatibility","title":"Model Compatibility","text":"<p>Chain of thought prompting has shown the most benefit in large decoder-only models such as GPT-2 and GPT-4. These models have sufficient capacity to maintain coherent multistep reasoning traces when prompted appropriately.</p>"},{"location":"nlp/16_nlg/","title":"16. Nature Language Generation","text":""},{"location":"nlp/16_nlg/#natural-language-generation","title":"Natural Language Generation","text":"<p>Natural Language Generation (NLG) is one of the two primary components of Natural Language Processing (NLP), the other being Natural Language Understanding (NLU). In simple terms:</p> \\[ \\text{NLP} = \\text{NLU} + \\text{NLG} \\] <p>While NLU focuses on interpreting and extracting meaning from human language, NLG is concerned with generating coherent, fluent, and contextually appropriate text intended for human readers.</p> <p>Recent advances in deep learning have significantly improved the quality and capabilities of NLG systems, enabling them to produce natural-sounding language across a wide range of applications.</p>"},{"location":"nlp/16_nlg/#applications-of-natural-language-generation","title":"Applications of Natural Language Generation","text":"<p>NLG plays a central role in many real-world applications, including:</p> <ul> <li> <p>Machine translation (MT):</p> <ul> <li>Input: Text in a source language</li> <li>Output: Translated text in a target language</li> </ul> </li> <li> <p>Dialogue systems / digital assistants:</p> <ul> <li>Input: Dialogue history or user queries</li> <li>Output: Text that appropriately continues the conversation</li> </ul> </li> <li> <p>Text summarization:</p> <ul> <li>Input: Long-form documents (e.g., research papers, emails, meeting transcripts)</li> <li>Output: Concise and coherent summaries</li> </ul> </li> <li> <p>Creative writing and story generation</p> </li> <li> <p>Data-to-text generation:</p> <ul> <li>Converts structured data (e.g., tables, databases) into natural language reports or descriptions</li> </ul> </li> <li> <p>Image and video captioning:</p> <ul> <li>Generates natural language descriptions based on visual input</li> </ul> </li> </ul>"},{"location":"nlp/16_nlg/#basics-of-natural-language-generation","title":"Basics of Natural Language Generation","text":"<p>Most modern NLG systems rely on autoregressive language models, which generate text one token at a time. At each time step \\(t\\), the model receives the preceding sequence of tokens \\(x_{&lt;t}\\) and predicts the next token \\(x_t\\).</p> <p>Let \\(f(\\cdot)\\) denote the model and \\(V\\) be the vocabulary. For a given context \\(x_{&lt;t}\\), the model computes a score \\(s_v = f(x_{&lt;t}, v) \\in \\mathbb{R}\\) for each token \\(v \\in V\\). These scores are converted into probabilities via the softmax function:</p> \\[ P(x_t = v \\mid x_{&lt;t}) = \\frac{\\exp(s_v)}{\\sum_{v' \\in V} \\exp(s_{v'})} \\]"},{"location":"nlp/16_nlg/#architectures","title":"Architectures","text":"<ul> <li> <p>Non-open-ended tasks (e.g., machine translation):   Typically use an encoder-decoder architecture:</p> <ul> <li>The encoder (often bidirectional) processes the input</li> <li>The decoder (autoregressive) generates output one token at a time</li> </ul> </li> <li> <p>Open-ended tasks (e.g., story generation):   May rely solely on an autoregressive decoder without a separate encoder</p> </li> </ul>"},{"location":"nlp/16_nlg/#training-objective","title":"Training Objective","text":"<p>Models are trained using maximum likelihood estimation (MLE), which aims to maximize the probability of each token in the training sequence given its preceding context.</p> <p>This is a token-level classification problem and is often trained using a method known as teacher forcing, where at each time step the model receives the true previous token instead of its own prediction. This aligns training conditions with the ground truth.</p>"},{"location":"nlp/17_imp_nlg/","title":"17. Improving Generation and Training for NLG","text":""},{"location":"nlp/17_imp_nlg/#improving-training-and-generation-for-nlg","title":"Improving Training and Generation for NLG","text":""},{"location":"nlp/17_imp_nlg/#improving-decoding-in-language-generation","title":"Improving Decoding in Language Generation","text":"<p>Decoding strategies play a crucial role in transforming a model\u2019s token probabilities into coherent and high-quality output. Below are commonly used techniques and strategies to improve decoding.</p>"},{"location":"nlp/17_imp_nlg/#greedy-decoding","title":"Greedy Decoding","text":"<ul> <li>At each time step, select the token with the highest probability:  </li> <li>Simple and fast, but may yield repetitive or suboptimal text due to lack of exploration.</li> </ul>"},{"location":"nlp/17_imp_nlg/#beam-search","title":"Beam Search","text":"<ul> <li>Keeps the top-\\(k\\) most likely partial sequences (beams) at each time step.</li> <li>Aims to maximize the overall sequence log-probability:  </li> <li>Length normalization is often applied to prevent a bias toward shorter sequences:  </li> </ul>"},{"location":"nlp/17_imp_nlg/#sampling-based-methods","title":"Sampling-Based Methods","text":""},{"location":"nlp/17_imp_nlg/#top-k-sampling","title":"Top-\\(k\\) Sampling","text":"<ul> <li>At each time step, restrict sampling to the top-\\(k\\) tokens in the probability distribution.</li> <li>Higher \\(k\\) increases output diversity but may introduce incoherence.</li> <li>Lower \\(k\\) leads to safer, more predictable outputs.</li> </ul>"},{"location":"nlp/17_imp_nlg/#top-p-nucleus-sampling","title":"Top-\\(p\\) (Nucleus) Sampling","text":"<ul> <li>Dynamically selects the smallest set of tokens whose cumulative probability exceeds a threshold \\(p\\):  </li> <li>More adaptive than top-\\(k\\), especially when token probability mass is unevenly distributed.</li> </ul>"},{"location":"nlp/17_imp_nlg/#temperature-scaling","title":"Temperature Scaling","text":"<ul> <li>Temperature controls the sharpness of the softmax distribution:  </li> <li>\\(\\tau &gt; 1\\): flattens the distribution, increasing randomness and diversity.</li> <li>\\(\\tau &lt; 1\\): sharpens the distribution, making outputs more deterministic.</li> </ul>"},{"location":"nlp/17_imp_nlg/#re-ranking-generated-sequences","title":"Re-ranking Generated Sequences","text":"<ul> <li>Decode multiple candidate sequences (e.g., 10), then rank them by a quality metric.</li> <li>Perplexity is a common metric but often favors generic or repetitive text due to high token likelihoods.</li> <li>Advanced re-rankers may consider:<ul> <li>Style</li> <li>Discourse coherence</li> <li>Factuality and entailment</li> <li>Logical consistency</li> </ul> </li> <li>Multiple re-rankers may be combined, but calibration issues can arise.</li> </ul>"},{"location":"nlp/17_imp_nlg/#improving-training-for-natural-language-generation-models","title":"Improving Training for Natural Language Generation Models","text":"<p>Training natural language generation (NLG) models with maximum likelihood estimation (MLE) using teacher forcing is effective but suffers from critical limitations. This section outlines the central issues, mitigation techniques, and advanced strategies used in both academic and production systems.</p>"},{"location":"nlp/17_imp_nlg/#exposure-bias","title":"Exposure Bias","text":"<ul> <li>During training, the model conditions on ground-truth tokens \\(y_{&lt;t}^*\\); at test time, it conditions on its own previous predictions \\(\\hat{y}_{&lt;t}\\).</li> <li>This mismatch can cause errors to compound\u2014early mistakes can derail generation.</li> </ul> <p>The standard teacher forcing objective is:  </p>"},{"location":"nlp/17_imp_nlg/#scheduled-sampling","title":"Scheduled Sampling","text":"<ul> <li>With probability \\(p\\), the model uses its own previous prediction instead of the ground-truth token.</li> <li>\\(p\\) increases over training to gradually expose the model to its own distribution.</li> <li>However, this violates the assumption that training inputs are independent of predictions, leading to inconsistent gradients and potential instability</li> </ul>"},{"location":"nlp/17_imp_nlg/#dataset-aggregation","title":"Dataset Aggregation","text":"<ul> <li>Periodically generate sequences using the current model.</li> <li>Add these self-generated sequences to the training set.</li> <li>The model learns to correct its own outputs over time.</li> </ul>"},{"location":"nlp/17_imp_nlg/#retrieval-augmented-generation","title":"Retrieval-Augmented Generation","text":"<ul> <li>Retrieve prototype sentences from a corpus.</li> <li>Two common paradigms:<ul> <li>Edit-based generation: learn to modify retrieved examples.</li> <li>Search-and-generate: use retrieval as a prompt (e.g., KNN-LM, RAG, RETRO).</li> </ul> </li> <li>Grounding generation in human-written text improves fluency and informativeness.</li> </ul>"},{"location":"nlp/17_imp_nlg/#reinforcement-learning-for-text-generation","title":"Reinforcement Learning for Text Generation","text":"<ul> <li>Model generation as a Markov Decision Process:<ul> <li>States: current context representation</li> <li>Actions: next-token choices</li> <li>Policy: decoder distribution</li> <li>Rewards: external scalar signal (e.g., BLEU)</li> </ul> </li> <li>Objective: maximize expected total reward:  </li> <li>Methods include REINFORCE, actor-critic, and policy gradients.</li> </ul>"},{"location":"nlp/17_imp_nlg/#limitations-of-reinforcement-learning","title":"Limitations of Reinforcement Learning","text":"<ul> <li>Sample inefficiency and high gradient variance.</li> <li>Difficult credit assignment over long sequences.</li> <li>Risk of overfitting the reward metric rather than true quality.</li> </ul>"},{"location":"nlp/17_imp_nlg/#defining-reward-functions","title":"Defining Reward Functions","text":"<p>Common automatic metrics include: - BLEU for machine translation - ROUGE for summarization - CIDEr and SPIDEr for image captioning</p> <p>These metrics are proxies and may correlate poorly with human judgment.</p>"},{"location":"nlp/17_imp_nlg/#rewarding-specific-behaviors","title":"Rewarding Specific Behaviors","text":"<p>Reinforcement learning can target fine-grained objectives: - Politeness - Simplicity - Temporal coherence - Cross-modality alignment - Formality - Human preference via reinforcement learning from human feedback (RLHF)</p>"},{"location":"nlp/17_imp_nlg/#training-takeaways","title":"Training Takeaways","text":"<ul> <li>Teacher forcing is standard but introduces exposure bias.</li> <li>Mitigation strategies include scheduled sampling, dataset aggregation, retrieval augmentation, and reinforcement learning.</li> <li>Reinforcement learning is powerful but expensive and should be used when desired behaviors are not captured by likelihood-based training.</li> <li>Human feedback and reward modeling are central to aligning modern language models with user preferences.</li> </ul>"},{"location":"nlp/18_eval_nlu/","title":"18. Evaluation of NLU Systems","text":""},{"location":"nlp/18_eval_nlu/#evaluating-nlu-systems","title":"Evaluating NLU Systems","text":"<p>Evaluating the performance of Natural Language Understanding (NLU) systems requires standardized benchmarks and carefully chosen evaluation metrics. Unlike open-ended generation tasks, NLU tasks are typically close-ended, meaning they have a limited and well-defined set of correct outputs\u2014such as binary, categorical, or span-based labels.</p> <p>These tasks lend themselves to automatic evaluation using standard supervised learning techniques, making them particularly well-suited for large-scale benchmarking. Their constrained output space allows for clear-cut metrics such as accuracy, precision, recall, and F1-score, enabling reproducible and objective comparisons across models.</p>"},{"location":"nlp/18_eval_nlu/#common-close-ended-nlp-tasks-and-benchmarks","title":"Common Close-ended NLP Tasks and Benchmarks","text":"<ul> <li> <p>Sentiment Analysis: Classify text by sentiment polarity (positive, negative, neutral).</p> <ul> <li>Benchmarks: SST-2, IMDB, Yelp Reviews</li> </ul> </li> <li> <p>Natural Language Inference (NLI): Determine whether a hypothesis is entailed, contradicted by, or neutral to a premise.</p> <ul> <li>Benchmarks: SNLI, MultiNLI</li> </ul> </li> <li> <p>Named Entity Recognition (NER): Identify and classify named entities (e.g., people, organizations, locations) in text.</p> <ul> <li>Benchmark: CoNLL-2003</li> </ul> </li> <li> <p>Part-of-Speech (POS) Tagging: Assign syntactic categories to each token in a sentence.</p> <ul> <li>Benchmark: Penn Treebank (PTB)</li> </ul> </li> <li> <p>Coreference Resolution: Identify mentions in text that refer to the same entity.</p> <ul> <li>Benchmark: Winograd Schema Challenge (WSC)</li> </ul> </li> <li> <p>Question Answering (QA): Answer factual questions based on a passage of text.</p> <ul> <li>Benchmark: SQuAD v2.0 (includes unanswerable questions)</li> </ul> </li> <li> <p>Multi-task Benchmarking:</p> <ul> <li>SuperGLUE is a prominent multi-task benchmark for close-ended evaluation. It extends GLUE by including harder tasks with a focus on deeper linguistic and reasoning skills.</li> </ul> </li> </ul>"},{"location":"nlp/18_eval_nlu/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The choice of evaluation metric depends on the nature of the task and class balance:</p> <ul> <li> <p>Accuracy: Proportion of correct predictions. Suitable for balanced datasets with a single label per instance.</p> </li> <li> <p>Precision / Recall / F1-score: Useful in imbalanced settings.</p> <ul> <li>Precision: Ratio of true positives to all predicted positives.</li> <li>Recall: Ratio of true positives to all actual positives.</li> <li>F1-score: Harmonic mean of precision and recall.</li> </ul> </li> <li> <p>ROC-AUC: Area under the receiver operating characteristic curve. Appropriate for binary classifiers, especially in imbalanced settings.</p> </li> </ul> <p>Aggregating Performance Across Tasks or Metrics: For multi-task evaluations (e.g., SuperGLUE), scores across tasks may be aggregated by computing a macro-average or weighted average of individual task metrics. However, task heterogeneity can make aggregate scoring misleading without task-level inspection.</p>"},{"location":"nlp/18_eval_nlu/#evaluation-challenges","title":"Evaluation Challenges","text":"<ul> <li> <p>Source of Labels: Many benchmarks rely on crowd-sourced annotations, which can introduce label noise and inconsistencies. It's important to consider inter-annotator agreement and annotation protocols.</p> </li> <li> <p>Spurious Correlations: Models often exploit superficial patterns in training data (e.g., specific lexical cues) that do not generalize to out-of-distribution examples. This can result in overestimated performance on test sets that share similar artifacts.</p> </li> <li> <p>Benchmark Saturation: Many established benchmarks (e.g., GLUE, SQuAD) have been nearly saturated by large models. Performance gains on these datasets may no longer reflect true improvements in generalization or reasoning.</p> </li> <li> <p>Generalization vs Memorization: Close-ended evaluations often do not measure generalization under distribution shift. It is critical to supplement them with robustness tests or adversarial datasets.</p> </li> </ul>"},{"location":"nlp/18_eval_nlu/#summary","title":"Summary","text":"<p>Close-ended evaluations remain a cornerstone of NLP model assessment due to their scalability and reliability. They are particularly useful for low-level tasks (e.g., tagging, classification) and for benchmarking multi-task generalization. However, their automatic nature can obscure deeper limitations in model reasoning, generalization, and alignment with real-world goals. Careful metric selection, robust dataset construction, and task-specific error analysis remain essential.</p>"},{"location":"nlp/19_eval_nlg/","title":"19. Evaluation of NLG Systems","text":""},{"location":"nlp/19_eval_nlg/#evaluating-nlg-systems","title":"Evaluating NLG Systems","text":"<p>Evaluation of Natural Language Generation (NLG) systems presents unique challenges due to the open-ended nature of their outputs. Unlike NLU tasks, where there is typically a single correct answer or label, NLG tasks often involve generating long-form text with many plausible outputs. As a result, standard machine learning metrics like accuracy or F1-score are not directly applicable.</p> <p>In open-ended tasks, the quality of a generation cannot be assessed solely based on correctness, but rather on a spectrum of qualitative dimensions such as fluency, coherence, relevance, and factuality. Multiple responses may be acceptable to varying degrees, making evaluation inherently more subjective.</p> <p>Common NLG tasks include:</p> <ul> <li>Summarization: CNN/DailyMail, Gigaword  </li> <li>Machine Translation: WMT (Workshop on Machine Translation)  </li> <li>Instruction-following and dialogue: Chatbot Arena, AlpacaEval, MT-Bench  </li> </ul> <p>Evaluation of Natural Language Generation (NLG) systems is challenging due to the diverse forms and goals of generated text. Evaluation methods can be categorized into three primary types:</p> <ol> <li>Content Overlap Metrics  </li> <li>Model-Based Metrics  </li> <li>Human Evaluations  </li> </ol>"},{"location":"nlp/19_eval_nlg/#1-content-overlap-metrics","title":"1. Content Overlap Metrics","text":"<p>These metrics assess the lexical similarity between generated text and reference (gold-standard) outputs. They rely on \\(n\\)-gram overlap and are fast, easy to compute, and widely used.</p> <p>Common Metrics:</p> <ul> <li>BLEU (Precisiong) - MT  </li> <li>ROUGE (Recall) - Summarization  </li> <li>METEOR  </li> <li>CIDEr  </li> </ul> <p>Limitations:</p> <ul> <li>Do not account for semantic similarity or paraphrasing.  </li> <li>Declining effectiveness with more open-ended tasks:<ul> <li>Summarization: Sensitive to lexical variation in longer texts.  </li> <li>Dialogue: Penalizes legitimate, diverse responses.  </li> <li>Story generation: May report high scores due to long-sequence overlap without true quality.  </li> </ul> </li> </ul>"},{"location":"nlp/19_eval_nlg/#2-model-based-metrics","title":"2. Model-Based Metrics","text":"<p>These leverage pretrained neural representations (learned representation) to assess semantic similarity, often with better correlation to human judgments.</p>"},{"location":"nlp/19_eval_nlg/#a-embedding-based-metrics","title":"(a) Embedding-Based Metrics","text":"<ul> <li>Embedding Average  </li> <li>Vector Extrema  </li> <li>Word Mover's Distance (WMD)  </li> <li>MEANT  </li> <li>YISI  </li> </ul>"},{"location":"nlp/19_eval_nlg/#b-contextual-embedding-metrics","title":"(b) Contextual Embedding Metrics","text":"<ul> <li>BERTScore: Uses contextual embeddings from BERT to compute word-level cosine similarity.</li> </ul> <ul> <li>BLEURT: BERT-based regression model fine-tuned to predict human judgment.  </li> <li>Sentence Mover's Similarity: Extends WMD to sentence-level embeddings for evaluating coherence.  </li> </ul>"},{"location":"nlp/19_eval_nlg/#c-distributional-metrics","title":"(c) Distributional Metrics","text":"<ul> <li>MAUVE: Computes divergence between the distribution of generated and reference texts in embedding space, interpolating between them. Suited for open-ended tasks.  </li> </ul>"},{"location":"nlp/19_eval_nlg/#3-human-evaluations","title":"3. Human Evaluations","text":"<p>Automatic metrics often fail to fully capture generation quality. Human evaluations remain the most reliable and nuanced method.</p> <p>Evaluation Dimensions:</p> <ul> <li>Fluency  </li> <li>Coherence / Consistency  </li> <li>Factual Accuracy  </li> <li>Commonsense Reasoning  </li> <li>Grammaticality  </li> <li>Style and Formality  </li> <li>Redundancy  </li> <li>Typicality / Appropriateness  </li> </ul> <p>Challenges:</p> <ul> <li>Expensive and time-consuming  </li> <li>Variability across annotators  </li> <li>Inconsistent or subjective judgments  </li> <li>Human only evaluate precision not recall.  </li> </ul> <p>Best Practices:</p> <ul> <li>Use standardized scales (e.g., Likert)  </li> <li>Report inter-rater agreement  </li> <li>Combine with automatic metrics  </li> </ul>"},{"location":"nlp/19_eval_nlg/#emerging-trend-llm-as-evaluator","title":"Emerging Trend: LLM-as-Evaluator","text":""},{"location":"nlp/19_eval_nlg/#reference-based-evaluation","title":"Reference-based Evaluation","text":"<p>This traditional approach compares model-generated text to one or more human-written reference outputs. It assumes the existence of gold-standard references and uses lexical or semantic similarity metrics to assess quality. While effective for tasks like summarization and translation, it struggles with open-ended generation where multiple valid outputs exist.</p> <ul> <li>Widely used in early NLP benchmarks.  </li> <li>Examples: BLEU, ROUGE, METEOR, BERTScore.  </li> </ul>"},{"location":"nlp/19_eval_nlg/#reference-free-evaluation","title":"Reference-free Evaluation","text":"<p>In this approach, model outputs are evaluated without gold references\u2014typically by using another model (or the same model) to assign scores based on perceived quality. This has gained traction with the advent of large instruction-tuned models like GPT-4, which can perform relatively consistent evaluations aligned with human preferences.</p> <ul> <li>Especially useful for instruction-following or multi-turn dialogue.  </li> <li>Examples: AlpacaEval, MT-Bench, Chatbot Arena.  </li> </ul> <p>Recent approaches employ large language models to act as automated evaluators (e.g., G-Eval, GPT-4-as-a-judge), showing promising alignment with human judgments while offering scale and speed.</p>"},{"location":"nlp/19_eval_nlg/#comparison-of-evaluation-metrics","title":"Comparison of Evaluation Metrics","text":"Metric Type Semantic Awareness Best for BLEU Content overlap No Translation ROUGE Content overlap Partial Summarization METEOR Content overlap Partial Translation CIDEr Content overlap No Image captioning BERTScore Model-based Yes Summarization, QA BLEURT Model-based Yes Open-domain gen MAUVE Model-based (dist.) Yes Long-form gen Human Eval Human Yes All tasks"},{"location":"nlp/19_eval_nlg/#takeaways","title":"Takeaways","text":"<ul> <li>Content overlap metrics are fast but insufficient for evaluating meaning or diversity.  </li> <li>Model-based metrics offer semantic insight but can be opaque.  </li> <li>Human judgments are indispensable, despite their cost and variability.  </li> <li>Inspect model outputs manually\u2014don\u2019t rely on metrics alone.  </li> <li>Publicly release model outputs to promote reproducibility and transparency.  </li> </ul>"},{"location":"nlp/1_intro/","title":"1. NLP an introduction","text":""},{"location":"nlp/1_intro/#chapter-1-nlp-an-introduction","title":"Chapter 1: NLP an introduction","text":"<p>Natural language processing (NLP) focuses on enabling machines to understand and work with human language. </p> <p>At a high level, NLP systems follow a common pipeline: text is first represented numerically, then modeled statistically or with neural networks, and finally evaluated on language tasks. This chapter focuses on the first and most fundamental step\u2014how language is represented.</p>"},{"location":"nlp/1_intro/#word-representations","title":"Word Representations","text":"<p>Early NLP systems relied on fixed, count-based representations. While simple and interpretable, these approaches struggle to capture deeper semantic relationships, motivating the move toward learned representations.</p> <ul> <li> <p>One-hot encoding: Represents each word as a sparse vector with a single 1. Fails to capture semantic similarity or relationships between words.</p> </li> <li> <p>n-grams: Represent text as sequences of \\(n\\) consecutive words or characters (e.g., bigrams, trigrams). Capture limited context but increase sparsity and dimensionality with higher \\(n\\).</p> </li> <li> <p>Bag of Words (BoW): Represents documents by word frequency vectors. Simple and effective, but ignores word order and context.</p> </li> <li> <p>TF-IDF (Term Frequency \u2013 Inverse Document Frequency): Adjusts raw word counts by penalizing frequent words and highlighting informative ones.</p> </li> <li>TF (term frequency): Measures how often a word appears in a document.</li> <li>IDF (inverse document frequency): Reduces the weight of common words across documents.</li> <li> <p>TF-IDF score for term \\(t\\) in document \\(d\\):          where \\(N\\) is the total number of documents and \\(\\text{DF}(t)\\) is the number of documents containing \\(t\\).</p> </li> <li> <p>Co-occurrence Matrix: Counts how often words appear near each other within a context window. Captures word associations but leads to large, sparse matrices.</p> </li> <li> <p>Latent Semantic Analysis (LSA): Applies Singular Value Decomposition (SVD) to the co-occurrence matrix to uncover latent semantic dimensions and reduce dimensionality.</p> </li> </ul>"},{"location":"nlp/1_intro/#word2vec-learning-word-representations","title":"Word2Vec: Learning Word Representations","text":"<p>Word2Vec was one of the first widely successful methods to demonstrate that simple neural objectives, trained at scale, can produce rich semantic representations without explicit supervision.</p> <p>Goal: Learn word embeddings such that words appearing in similar contexts have similar vector representations.</p> <ul> <li>Input: Large corpus of text, fixed vocabulary of size \\(|V|\\).</li> <li>Output: Two embedding matrices:</li> <li>Input (center) word matrix \\(W \\in \\mathbb{R}^{d \\times |V|}\\)</li> <li>Output (context) word matrix \\(W' \\in \\mathbb{R}^{d \\times |V|}\\)</li> <li>Two main training architectures:</li> </ul>"},{"location":"nlp/1_intro/#1-skip-gram-model","title":"1. Skip-gram Model","text":"<ul> <li>Predict context words \\(w_{t+j}\\) given a center word \\(w_t\\), for \\(-m \\le j \\le m,\\ j \\neq 0\\).</li> <li>Maximize conditional likelihood:    </li> <li>Softmax formulation:    </li> <li>Challenge: Softmax denominator scales with vocabulary size \\(|V|\\), making training expensive.</li> </ul>"},{"location":"nlp/1_intro/#2-cbow-continuous-bag-of-words","title":"2. CBOW (Continuous Bag of Words)","text":"<ul> <li>Predict the center word \\(w_t\\) from its surrounding context words.</li> <li>Average context embeddings:    </li> <li>Predict \\(w_t\\) using softmax:    </li> <li>Typically faster to train than Skip-gram, and better for frequent words.</li> </ul> <p>Together, Skip-gram and CBOW illustrate two complementary perspectives: predicting context from a word versus predicting a word from its context.</p>"},{"location":"nlp/1_intro/#glove-global-vectors-for-word-representation","title":"GloVe: Global Vectors for Word Representation","text":"<p>Unlike Word2Vec, which learns embeddings implicitly through prediction tasks, GloVe makes the statistical structure of language explicit through global co-occurrence counts.</p> <p>Goal: Capture the meaning of words using statistics of word co-occurrence across the entire corpus.</p> <ul> <li>While Word2Vec uses local context windows, GloVe (Global Vectors) constructs a global co-occurrence matrix, where \\(X_{ij}\\) is the number of times word \\(j\\) appears near word \\(i\\).</li> <li>GloVe seeks to find word vectors \\(\\mathbf{w}_i\\), \\(\\tilde{\\mathbf{w}}_j\\) such that:    </li> <li>The underlying insight: ratios of co-occurrence probabilities encode meaning. For example, consider how a target word \\(x\\) relates to two context words \\(a\\) and \\(b\\):      This shows that linear differences between word vectors reflect meaningful relationships (e.g., \\(\\texttt{king} - \\texttt{man} + \\texttt{woman} \\approx \\texttt{queen}\\)).</li> <li>To learn these representations, GloVe minimizes a weighted least squares loss:    </li> <li>The weighting function \\(f(X_{ij})\\) ensures that very frequent co-occurrences do not dominate the loss, and very rare ones do not introduce noise. A typical choice:      where \\(\\alpha \\in [0,1]\\), e.g., \\(\\alpha = 0.75\\).</li> <li>Interpretation: GloVe combines the global statistical strength of matrix factorization (like LSA) with the expressive power of vector differences. This allows it to learn embeddings that perform well on tasks like analogy completion, clustering, and semantic similarity.</li> </ul> <p>Both Word2Vec and GloVe produce static word embeddings, where each word has a single vector regardless of context. While powerful, this assumption will later be revisited with contextualized representations.</p>"},{"location":"nlp/1_intro/#evaluating-word-vectors-intrinsic-vs-extrinsic","title":"Evaluating Word Vectors: Intrinsic vs. Extrinsic","text":"<p>Because embeddings are intermediate representations rather than end goals, evaluating their quality requires careful consideration of what \u201cgood\u201d representations mean in practice.</p> <ul> <li>Evaluation of word vectors can be divided into intrinsic and extrinsic methods.</li> </ul>"},{"location":"nlp/1_intro/#intrinsic-evaluation","title":"Intrinsic Evaluation","text":"<ul> <li>Tests word vectors on intermediate, well-defined subtasks.</li> <li>Fast to compute and useful for understanding specific aspects of embeddings.</li> <li>However, it is not always clear if performance on intrinsic tasks translates to improvements in real-world tasks.</li> <li>Example: Compute cosine similarity between word pairs and compare with human similarity ratings.</li> </ul>"},{"location":"nlp/1_intro/#extrinsic-evaluation","title":"Extrinsic Evaluation","text":"<ul> <li>Measures performance on a real downstream NLP task.</li> <li>Slower and more expensive to compute but provides more practical insight.</li> <li>Hard to isolate whether improvements are due to the word vectors or other parts of the system.</li> <li>Example: Use word embeddings in a Named Entity Recognition (NER) system and evaluate changes in accuracy.</li> <li>If replacing one embedding model with another improves end-task performance, the new model is considered better.</li> </ul> <p>This chapter establishes the foundational idea that representation learning lies at the heart of modern NLP, setting the stage for deeper neural architectures and contextual language models explored in later chapters.</p>"},{"location":"nlp/20_post_training/","title":"20. Post-training","text":""},{"location":"nlp/20_post_training/#post-trainings-prompting-instruction-finetuning-and-dporlhf","title":"Post-trainings: Prompting, Instruction Finetuning, and DPO/RLHF","text":""},{"location":"nlp/20_post_training/#zero-shot-and-few-shot-in-context-learning","title":"Zero-Shot and Few-Shot In-Context Learning","text":""},{"location":"nlp/20_post_training/#zero-shot-in-context-learning","title":"Zero-Shot In-Context Learning","text":"<p>Large language models (LLMs), beginning with GPT-2 and especially prominent in GPT-3 and beyond, exhibit a remarkable emergent ability: zero-shot learning. In this setting, a model can perform a task without any task-specific gradient updates or labeled examples\u2014just by conditioning on the right input format.</p>"},{"location":"nlp/20_post_training/#example-qa-as-language-modeling","title":"Example (QA as Language Modeling):","text":"<p>Passage: Tom Brady is an American football player born in San Mateo, California. Q: Where was Tom Brady born? A:</p> <p>The model simply continues the sequence by generating the most probable completion (\u201cSan Mateo, California\u201d).</p>"},{"location":"nlp/20_post_training/#example-winograd-schema","title":"Example (Winograd Schema):","text":"<p>The model can also answer commonsense questions by comparing the likelihood of different completions:</p> <p>The cat couldn't fit into the hat because it was too big.</p> <p>Here, the model disambiguates the pronoun \u201cit\u201d by comparing:  A higher probability assigned to the second option indicates the model's preference for \u201chat\u201d as the correct referent, aligning with human intuition.</p>"},{"location":"nlp/20_post_training/#few-shot-in-context-learning","title":"Few-Shot (In-Context) Learning","text":"<p>In few-shot in-context learning, the model is given a small number of input-output examples as context, followed by a new input to predict. No parameter updates occur\u2014learning happens purely via conditioning on the prompt.</p>"},{"location":"nlp/20_post_training/#example-sentiment-classification","title":"Example (Sentiment Classification):","text":"<p>Review: This movie was amazing. Sentiment: Positive Review: The plot was dull and slow. Sentiment: Negative Review: The acting was brilliant. Sentiment:</p> <p>The model is expected to complete the pattern with <code>Positive</code>.</p>"},{"location":"nlp/20_post_training/#terminology-note","title":"Terminology Note:","text":"<p>This is often referred to as in-context learning (ICL) to distinguish it from traditional few-shot learning involving optimization-based methods.</p>"},{"location":"nlp/20_post_training/#limitations-of-prompting","title":"Limitations of Prompting","text":"<p>Despite the versatility of prompting, in-context learning has limitations:</p> <ul> <li>Context window size: Prompt length is bounded by the model's maximum context window, limiting how many examples or instructions can be included.</li> <li>Reasoning complexity: Some tasks, especially those involving multi-hop or logical reasoning, remain difficult even for very large models using sophisticated prompts.</li> <li>Interpretability and control: Prompting alone provides limited control over internal reasoning steps or modular behavior.</li> </ul>"},{"location":"nlp/20_post_training/#summary","title":"Summary","text":"<ul> <li>In-context learning enables task generalization without gradient updates.</li> <li>Prompt design (e.g., Chain-of-Thought, few-shot examples) significantly influences performance.</li> <li>However, for complex reasoning tasks, prompting may not be sufficient\u2014gradient-based finetuning or hybrid techniques (e.g., tool use, scratchpads) may be necessary.</li> </ul>"},{"location":"nlp/20_post_training/#instruction-finetuning","title":"Instruction Finetuning","text":"<p>Instruction finetuning is a paradigm in which a pretrained language model is adapted using a diverse set of (instruction, output) pairs across a wide range of natural language tasks. The goal is to teach the model to follow explicit instructions phrased in natural language, enabling it to generalize to unseen tasks at inference time without further finetuning.</p>"},{"location":"nlp/20_post_training/#core-methodology","title":"Core Methodology","text":"<p>The core idea is to treat language modeling as a multitask learning problem, where each task is presented as a textual instruction, and the model is finetuned to produce the appropriate output. This process involves:</p> <ul> <li>Collecting a large, heterogeneous corpus of \\((\\text{instruction}, \\text{response})\\) pairs spanning tasks such as question answering, summarization, classification, translation, commonsense reasoning, etc.</li> <li>Optionally applying task mixtures (e.g., T0) or task reformulations to promote robustness.</li> <li>Finetuning the model using a standard language modeling objective on these paired samples.</li> </ul>"},{"location":"nlp/20_post_training/#benefits","title":"Benefits","text":"<ul> <li>Simplicity: A unified finetuning pipeline can teach the model to perform a wide range of tasks, framed uniformly as instruction following.</li> <li>Generalization: Models trained this way can often generalize to novel tasks simply by being given a well-phrased prompt\u2014even tasks not seen during training (zero-shot generalization).</li> <li>Improved usability: End-users do not need to know the model\u2019s internal API or finetune further; they can interact with the model via natural language instructions.</li> </ul>"},{"location":"nlp/20_post_training/#challenges-and-limitations","title":"Challenges and Limitations","text":"<ul> <li>Data cost: Collecting high-quality demonstrations for hundreds of diverse tasks is resource-intensive and often requires human annotation.</li> <li>Alignment mismatch: The model is optimized for a maximum likelihood objective, which may not align with human preferences (e.g., helpfulness, harmlessness, or truthfulness), leading to outputs that are technically correct but pragmatically unhelpful.</li> <li>Ambiguity in instructions: Natural language instructions can be underspecified or ambiguous, requiring either better dataset design or model mechanisms for disambiguation.</li> </ul>"},{"location":"nlp/20_post_training/#representative-models","title":"Representative Models","text":"<p>Instruction finetuning has been a foundational technique behind several recent LLMs:</p> <ul> <li>T5 (Text-to-Text Transfer Transformer): Casts all tasks into a text-to-text format with task-specific prefixes (e.g., \u201ctranslate English to German: ...\u201d).</li> <li>FLAN (Finetuned Language Net): Builds on T5 with instruction finetuning across many tasks and task variants.</li> <li>T0: Trains on prompt collections to enable strong zero-shot task generalization.</li> <li>InstructGPT: Combines instruction finetuning with reinforcement learning from human feedback (RLHF) to better align with human intent.</li> </ul>"},{"location":"nlp/20_post_training/#outlook","title":"Outlook","text":"<p>Instruction finetuning plays a key role in aligning language models with human goals and making them more broadly useful without requiring users to craft task-specific prompts. However, as models scale and tasks become more complex, instruction finetuning alone may be insufficient, motivating hybrid approaches that integrate tool use, memory, and interaction (e.g., agents or retrievers).</p>"},{"location":"nlp/20_post_training/#optimizing-for-human-preferences-rlhf-dpo","title":"Optimizing for Human Preferences (RLHF / DPO)","text":"<p>While instruction finetuning significantly improves a language model's usability by teaching it to follow task instructions, it does not guarantee alignment with human values, preferences, or expectations. To bridge this gap, researchers have developed techniques that explicitly optimize language models based on human preferences. Two leading approaches in this domain are Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO).</p>"},{"location":"nlp/20_post_training/#from-instructions-to-rewards","title":"From Instructions to Rewards","text":"<p>Instruction tuning is typically the first step in building an aligned model. However, aligning model outputs more precisely with human expectations requires further optimization:</p> <ul> <li>Step 1: Instruction finetuning on diverse (instruction, output) pairs.</li> <li>Step 2: Learning a reward function from human feedback.</li> <li>Step 3: Optimizing the model to produce outputs that maximize this learned reward.</li> </ul> <p></p>"},{"location":"nlp/20_post_training/#learning-human-preferences","title":"Learning Human Preferences","text":"<p>Key challenge: Obtaining human feedback at scale is expensive, inconsistent, and noisy.</p> <p>Solution: Instead of relying on absolute scores or direct ratings\u2014which are often uncalibrated\u2014use pairwise comparisons, where humans are asked to choose the preferred output among two or more options. These comparisons can then be modeled as a binary classification task.</p> <p>A commonly used model for interpreting such data is the Bradley\u2013Terry model, which assigns a higher latent score to the preferred (\u201cwinning\u201d) output over the non-preferred (\u201closing\u201d) one.</p>"},{"location":"nlp/20_post_training/#reinforcement-learning-from-human-feedback-rlhf","title":"Reinforcement Learning from Human Feedback (RLHF)","text":"<p>RLHF is a three-step pipeline:</p> <ol> <li>Supervised Instruction Tuning: Finetune the language model on instruction datasets to teach basic task-following behavior.</li> <li>Reward Modeling: Collect human preference data in the form of comparisons between model outputs. Train a reward model (RM) to predict human preferences by learning to assign higher scores to preferred responses.</li> <li>Policy Optimization: Use reinforcement learning (typically Proximal Policy Optimization, PPO) to optimize the language model to produce outputs that maximize the predicted reward, while remaining close to the supervised model (e.g., via a KL-divergence penalty).</li> </ol> <p>Strengths:</p> <ul> <li>Produces high-quality, aligned outputs when tuned well.</li> <li>Flexible reward modeling enables fine-grained preference learning.</li> </ul> <p>Challenges:</p> <ul> <li>RL optimization is unstable, sensitive to reward shaping and hyperparameters.</li> <li>Computationally expensive: PPO with large models requires extensive resources.</li> <li>Human preferences are not always consistent, and reward models may overfit artifacts in the training data.</li> </ul>"},{"location":"nlp/20_post_training/#direct-preference-optimization-dpo","title":"Direct Preference Optimization (DPO)","text":"<p>DPO offers a simpler, stable alternative to RLHF by bypassing the need for reinforcement learning altogether. Instead, it directly finetunes the language model on human preference data using a contrastive objective.</p> <p>Given a dataset of preferred (\u201cchosen\u201d) and non-preferred (\u201crejected\u201d) outputs, the model is trained to increase the likelihood of preferred completions relative to rejected ones.</p> <p>Advantages:</p> <ul> <li>No reinforcement learning loop \u2014 avoids instability and simplifies training.</li> <li>Leverages the pretrained model\u2019s probabilities directly.</li> <li>Comparable performance to RLHF on many benchmarks.</li> </ul> <p>Limitations:</p> <ul> <li>Assumes high-quality, diverse comparison data.</li> <li>Does not inherently model long-term or interactive objectives.</li> </ul>"},{"location":"nlp/20_post_training/#summary_1","title":"Summary","text":"<ul> <li>Optimizing for human preferences is essential for building safe, aligned LLMs.</li> <li>RLHF has shown strong results (e.g., InstructGPT, ChatGPT), but is computationally intensive.</li> <li>DPO is emerging as a lightweight, scalable alternative with competitive results.</li> <li>Both methods rely on accurate human preference modeling\u2014an inherently noisy and subjective signal.</li> </ul> Term Description Instruction Fine-Tuning (IFT) Training a language model to follow user instructions, typically using an autoregressive language modeling loss. Supervised Fine-Tuning (SFT) Training a model on task-specific labeled data to acquire desired capabilities, generally with an autoregressive LM loss. Alignment A broad goal of making models behave according to user intentions or societal values; can be optimized via any suitable loss function. Reinforcement Learning from Human Feedback (RLHF) A specific technique that fine-tunes models using human-generated preference data via reinforcement learning. Preference Fine-Tuning Uses human-labeled preference data to fine-tune models. Can be implemented via RLHF, Direct Preference Optimization (DPO), or learning-to-rank methods."},{"location":"nlp/21_advanced_topics/","title":"21. Advanced Topics in Language Modelling","text":""},{"location":"nlp/21_advanced_topics/#advanced-topics-in-language-modelling","title":"Advanced Topics in Language Modelling","text":""},{"location":"nlp/21_advanced_topics/#knowledge-distillation-for-language-models","title":"Knowledge Distillation for Language Models","text":"<p>Knowledge distillation is a model compression and capability transfer technique in NLP and LLMs that trains a compact student model to approximate a high-capacity teacher model. Instead of learning only from one-hot labels, the student also learns from the teacher\u2019s soft predictive distribution and, optionally, its intermediate reasoning samples, improving both generalization and reliability while reducing inference cost.</p>"},{"location":"nlp/21_advanced_topics/#teacher-and-student-distributions","title":"Teacher and Student Distributions","text":"<p>Given input \\(x\\), the teacher outputs logits \\(z_t \\in \\mathbb{R}^{|V|}\\) over vocabulary \\(V\\). A temperature-scaled softmax produces softened probabilities:</p> \\[ p_t^T = softmax(z_t / T), \\quad T &gt; 1 \\] <p>The student model generates its own logits \\(z_s\\) and corresponding distribution:</p> \\[ p_s^T = softmax(z_s / T) \\] <p>When \\(T &gt; 1\\), distributions become smoother, exposing relationships between alternative token choices. This is critical for multi-step reasoning, where each token builds on implicit intermediate deductions. Distillation transfers not only correct answers but also richer decision structure and uncertainty.</p>"},{"location":"nlp/21_advanced_topics/#distillation-loss","title":"Distillation Loss","text":"<p>Training minimizes a weighted objective balancing:</p> <ul> <li>Correctness using cross-entropy with ground truth \\(y\\)</li> <li>Imitation using KL divergence between teacher and student distributions</li> </ul> <p>Loss formulation:</p> \\[ L = \\alpha T^2 \\, KL(p_t^T \\| p_s^T) + (1 - \\alpha) \\, CE(y, softmax(z_s)) \\] <p>Where:</p> \\[ KL(p \\| q) = \\sum_i p_i \\log \\frac{p_i}{q_i} \\] <p>The term \\(T^2\\) rescales gradients to preserve signal magnitude when using high temperature, preventing vanishing updates.</p>"},{"location":"nlp/21_advanced_topics/#representation-alignment-encoder-models","title":"Representation Alignment (Encoder Models)","text":"<p>For encoder models, teacher and student hidden states \\(h_t\\) and \\(h_s\\) may differ in size. A learned projector \\(W\\) aligns them:</p> \\[ L_{rep} = MSE(h_t W, h_s) \\] <p>Alternative alignment objectives include cosine similarity or attention map matching across selected layers \\(\\mathcal{L}_k\\).</p>"},{"location":"nlp/21_advanced_topics/#distillation-algorithm","title":"Distillation Algorithm","text":""},{"location":"nlp/21_advanced_topics/#initial-setup","title":"Initial Setup","text":"<ul> <li>Teacher Model: Large LM (e.g., 3.7B\u2013175B parameters)</li> <li>Student Model: Smaller LM (e.g., 125M\u20131.3B parameters)</li> <li>Training Inputs: \\(\\mathcal{D}_{Train} = \\{x_i\\}\\)</li> <li>Prompt Set: \\(P = \\{(x_i, y_i, z_i)\\}\\) where \\(z_i\\) are teacher reasoning samples</li> </ul>"},{"location":"nlp/21_advanced_topics/#sampling-process","title":"Sampling Process","text":"<p>For each example \\(x_i\\):</p> <ol> <li>Sample \\(N\\) teacher reasoning\u2013prediction pairs:     </li> <li>Construct sample set:     </li> <li>Typical setting:     </li> </ol> <p>Create corpus:</p> \\[ C = \\{(x_i, (\\hat{z}_i^j, \\hat{y}_i^j))\\}_{j=1}^N \\]"},{"location":"nlp/21_advanced_topics/#training-process","title":"Training Process","text":"<p>Train student on teacher samples using LM objective:</p> \\[ L(z_s, y_s \\mid C) = CE(\\hat{y}_i^j, \\hat{z}_i^j \\mid x_i) \\] <p>This objective integrates into the full distillation loss defined earlier.</p>"},{"location":"nlp/21_advanced_topics/#evaluation-options","title":"Evaluation Options","text":"<p>After training, evaluate output quality using:</p> <ul> <li> <p>Greedy Decoding    </p> </li> <li> <p>Self-Consistency    </p> </li> </ul>"},{"location":"nlp/21_advanced_topics/#optional-generative-extension-sample-and-rank","title":"Optional Generative Extension: Sample-and-Rank","text":"<ol> <li>Sample \\(n\\) teacher completions per prompt</li> <li>Score using reward model or teacher log-likelihood</li> <li>Select top-\\(k\\) sequences \\(\\mathcal{S}_{top}\\)</li> <li>Train student via:</li> <li>\\(CE(\\mathcal{S}_{top}, p_{student})\\), or</li> <li>policy distillation on teacher action distributions</li> </ol>"},{"location":"nlp/21_advanced_topics/#mixture-of-experts","title":"Mixture of Experts","text":"<p>In LLMs and NLP, mixture of experts (MoE) replaces dense feed-forward layers with a set of expert networks \\(\\{E_1, ..., E_N\\}\\) and a router \\(G\\) that selects a sparse subset of experts per token. Given input representation \\(h\\), the router outputs gating logits \\(r = G(h)\\) and expert selection is typically top-\\(k\\):</p> \\[ g_i = softmax(r)_i,\\quad \\mathcal{I} = topk(r, k) \\] <p>Only experts in \\(\\mathcal{I}\\) are executed, producing \\(e_i = E_i(h)\\), and the MoE layer output is:</p> \\[ y = \\sum_{i \\in \\mathcal{I}} g_i \\, e_i \\] <p>Training minimizes the task loss \\(L_{task}\\) (e.g., next-token cross-entropy) plus auxiliary load-balancing and routing regularization to prevent expert collapse and imbalance. A common load-balancing loss uses batch-level expert utilization \\(f_i\\) and mean gate probability \\(\\bar{g}_i\\):</p> \\[ L = L_{task} + \\lambda \\sum_{i=1}^N f_i \\cdot \\bar{g}_i \\] <p>Alternative formulations use entropy bonuses on \\(g\\), z-loss on router logits, or differentiable routing approximations.</p> <p>Algorithm steps:</p> <ol> <li>Replace MLP layers with MoE block containing \\(N\\) experts and router \\(G\\).</li> <li>For each token, compute router logits \\(r = G(h)\\).</li> <li>Select expert indices \\(\\mathcal{I} = topk(r, k)\\).</li> <li>Compute gates \\(g_i = softmax(r)_i\\) for \\(i \\in \\mathcal{I}\\).</li> <li>Execute selected experts \\(e_i = E_i(h)\\).</li> <li>Aggregate \\(y = \\sum_{i \\in \\mathcal{I}} g_i \\, e_i\\).</li> <li>Compute loss \\(L = L_{task} + L_{aux}\\) and backprop to student and router.</li> <li>Optionally apply capacity limits per expert, expert dropout, router jitter noise, or switch to single-expert routing (top-1) variants.</li> </ol> <p>Efficiency note: MoE increases parameter count while reducing per-token FLOPs via sparse routing, improving scaling at controlled inference cost.</p>"},{"location":"nlp/21_advanced_topics/#predicting-the-next-token-in-language-models","title":"Predicting the Next Token in Language Models","text":"<ol> <li> <p>Greedy Decoding: Greedy decoding selects the token with the highest predicted probability at each step: </p> \\[x_{t+1} = \\arg\\max_{x} \\, p(x \\mid x_{1:t})\\] <p>While simple and efficient, it often produces suboptimal sequences that are repetitive, lack diversity, or fail to capture long-range coherence, since it ignores alternative plausible continuations.</p> </li> <li> <p>Beam Search: Beam search maintains the top-\\(k\\) most probable partial sequences (beams) at each timestep. The algorithm expands each beam with all possible next tokens, retains the \\(k\\) highest-scoring sequences, and repeats until termination. Benefits include higher likelihood sequences compared to greedy decoding, but beam search increases computational cost and can still lack diversity, often producing deterministic or generic outputs if the beam width is small or length penalties are not applied.</p> </li> <li> <p>Sampling-Based Decoding: Sampling introduces stochasticity to token selection, enabling more diverse and natural outputs. Common strategies include:  </p> <ul> <li> <p>Top-\\(k\\) sampling: Restrict the candidate set to the \\(k\\) most probable tokens and sample according to their normalized probabilities.  </p> \\[p'(x \\mid x_{1:t}) = \\frac{p(x \\mid x_{1:t})}{\\sum_{i \\in topk} p(i \\mid x_{1:t})}, \\quad x \\sim p'\\] </li> <li> <p>Top-\\(p\\) (nucleus) sampling: Select the smallest set of tokens whose cumulative probability exceeds a threshold \\(p\\) and sample from this set.  </p> \\[\\mathcal{V}_p = \\min \\{V' \\mid \\sum_{i \\in V'} p(i \\mid x_{1:t}) \\ge p \\}, \\quad x \\sim p(i \\mid i \\in \\mathcal{V}_p)\\] </li> <li> <p>Temperature scaling: Adjusts the sharpness of the predicted distribution to control randomness:  </p> </li> </ul> \\[p_T(x \\mid x_{1:t}) = softmax \\left( \\frac{\\log p(x \\mid x_{1:t})}{T} \\right)\\] <p>Low temperatures (\\(T&lt;1\\)) make the distribution peakier, favoring high-probability tokens and reducing diversity. High temperatures (\\(T&gt;1\\)) flatten the distribution, increasing randomness and creative outputs.</p> </li> </ol> <p>Overall, the choice of decoding strategy involves a tradeoff between likelihood, diversity, and computational efficiency. Greedy and beam search optimize for probability, whereas sampling-based methods enhance diversity and naturalness in generated text.</p>"},{"location":"nlp/21_advanced_topics/#inference-optimization-in-llms","title":"Inference Optimization in LLMs","text":""},{"location":"nlp/21_advanced_topics/#kv-caching","title":"KV caching","text":"<p>In autoregressive transformers, each new token attends to all prior tokens. Instead of recomputing past key/value projections, we cache them. For token step \\(t\\), the attention computation becomes:</p> \\[ Q_t = W_Q h_t,\\quad K_{1:t} = [K_{cache}; W_K h_t],\\quad V_{1:t} = [V_{cache}; W_V h_t] \\] \\[ A_t = softmax\\left(\\frac{Q_t K_{1:t}^\\top}{\\sqrt{d_k}}\\right) V_{1:t} \\] <p>The cache stores \\((K_{1:t-1}, V_{1:t-1})\\) from earlier steps. This reduces per-token FLOPs from \\(O(t)\\) to \\(O(1)\\) for projections, leaving only the attention dot product with cached states. Latency improves significantly for long contexts, at the cost of \\(O(n_{layers} \u00b7 seq_{len} \u00b7 d_{kv})\\) memory.</p> <p>Algorithm steps:</p> <ol> <li>For each layer, compute \\(K = W_K h, V = W_V h\\) for prompt tokens.</li> <li>Store \\((K, V)\\) in cache.</li> <li>During generation, compute only \\(K_t, V_t\\) for the new token.</li> <li>Append to cache and attend using the full cached \\(K, V\\).</li> </ol>"},{"location":"nlp/21_advanced_topics/#multi-query-attention-mqa-and-grouped-query-attention-gqa","title":"Multi-Query Attention (MQA) and Grouped-Query Attention (GQA)","text":"<p>MQA shares a single key/value head across all query heads, reducing memory and bandwidth:</p> <p> </p> <p>All query heads attend to the same \\(K, V\\). This cuts cache size by \\(h\\)\u00d7 (number of Q heads).    GQA generalizes this by sharing \\(K, V\\) across groups of query heads:</p> <p> </p> <p>If there are \\(h_q\\) query heads and \\(h_{kv}\\) KV heads, each KV head serves \\(h_q / h_{kv}\\) query heads. GQA balances quality and efficiency better than full sharing (MQA) while still reducing memory bandwidth and cache footprint.</p> <p>Benefits:</p> <ul> <li>Smaller KV cache (\\(\u2193\\) memory, \\(\u2193\\) GPU bandwidth pressure)</li> <li>Faster decoding, especially in memory-bound regimes</li> <li>Better quality than MQA when \\(h_{kv} &gt; 1\\)</li> </ul> <p>Limitations:</p> <ul> <li>Some capacity loss from reduced key/value specialization</li> <li>Requires careful grouping choice to avoid quality drop</li> </ul>"},{"location":"nlp/21_advanced_topics/#pagedattention","title":"PagedAttention","text":"<p>Standard KV caches allocate contiguous memory per sequence, causing fragmentation and over-allocation when sequences vary in length. PagedAttention stores KV blocks in fixed-size pages (like virtual memory), allowing non-contiguous storage:</p> <p>Idea:</p> <ul> <li>Preallocate memory into pages of size \\(P\\).</li> <li>Store \\((K, V)\\) in page slots, not one long tensor.</li> <li>Map logical token positions \u2192 physical page addresses.</li> </ul> <p>Memory model:</p> <p> </p> <p>Attention reads KV by gathering relevant pages:</p> <p> </p> <p>Benefits:</p> <ul> <li>Eliminates wasted padding memory</li> <li>Enables large-batch serving without OOM</li> <li>Efficient for dynamic and very long contexts</li> <li>Reduces fragmentation and improves throughput</li> </ul> <p>Algorithm steps:</p> <ol> <li>Partition prompt tokens into pages of length \\(P\\).</li> <li>Store each page\u2019s \\(K, V\\) into free page slots.</li> <li>Maintain a page table mapping token index \u2192 page slot.</li> <li>During decoding, gather only required pages for attention.</li> <li>Append new token KV into next free page.</li> </ol>"},{"location":"nlp/22_llm_training_basics/","title":"22. LLM Training Basics","text":""},{"location":"nlp/22_llm_training_basics/#tensors-and-pytorch","title":"Tensors and PyTorch","text":"<p>Tensors are the basic building block for storing everything in deep learning: parameters, gradients, optimizer state, data, and activations. Almost all of these are stored as floating-point numbers.</p> <p>By default, tensors live in CPU memory. To leverage the massive parallelism of GPUs, we move them to GPU memory via <code>.to(device)</code> or <code>.cuda()</code>.</p>"},{"location":"nlp/22_llm_training_basics/#what-is-a-tensor","title":"What is a Tensor?","text":"<p>PyTorch tensors are pointers into allocated memory, plus metadata describing:</p> <ul> <li><code>shape</code></li> <li><code>stride</code> \u2014 how many elements to skip to move along each axis</li> <li><code>dtype</code></li> <li><code>device</code></li> </ul> <p>Most tensors are created from performing operations on other tensors. Each operation has some memory and compute consequence. Many operations simply provide a different view of the tensor. This does not make a copy, and therefore mutations in one tensor affect the other.</p>"},{"location":"nlp/22_llm_training_basics/#optional-math-example","title":"Optional math example","text":"<p>A tensor \\(x \\in \\mathbb{R}^{m \\times n}\\) is stored as a contiguous 1-D block of memory, with metadata describing how to interpret it. The stride vector \\(s\\) defines indexing via:</p> <p> </p> <p>If a view operation reshapes \\(x\\) without copying, the underlying memory remains the same:</p> <p> </p>"},{"location":"nlp/22_llm_training_basics/#flops-and-performance-in-deep-learning","title":"FLOPs and Performance in Deep Learning","text":"<p>A FLOP (floating-point operation) is a basic arithmetic operation (e.g., addition or multiplication) on floating-point numbers. It is a standard measure of computational work in deep learning.</p>"},{"location":"nlp/22_llm_training_basics/#example-linear-layer-flops","title":"Example: Linear Layer FLOPs","text":"<p>Suppose we apply a linear model to a batch of \\(B\\) vectors of dimension \\(D\\), mapping to \\(K\\) outputs:</p> <pre><code>B = 16384  # Batch size (e.g., number of tokens)\nD = 32768  # Input dimension\nK = 8192   # Output dimension\n\ndevice = get_device()\nx = torch.ones(B, D, device=device)\nw = torch.randn(D, K, device=device)\ny = x @ w\n</code></pre> <p>Each output \\(y_{ik} = \\sum_j x_{ij} w_{jk}\\) requires \\(D\\) multiplications and \\(D - 1\\) additions.</p> <p>Approximate total FLOPs:</p> \\[ FLOPs = 2  \\times B  \\times D  \\times K \\]"},{"location":"nlp/22_llm_training_basics/#other-flops-estimates","title":"Other FLOPs Estimates","text":"<ul> <li>Elementwise operation (e.g., ReLU on \\(m imes n\\) matrix): \\(mn\\) FLOPs</li> <li>Matrix addition (\\(m \\times n + m \\times n\\)): \\(mn\\) FLOPs</li> <li>Normalization layers: \\(O(mn)\\) FLOPs</li> </ul> <p>In general, matrix multiplications dominate FLOPs in large models like Transformers.</p>"},{"location":"nlp/22_llm_training_basics/#interpreting-flops-in-llms","title":"Interpreting FLOPs in LLMs","text":"<ul> <li>\\(B\\) is number of tokens or batch size</li> <li>\\(D \\times K\\) is number of parameters in a linear layer</li> <li>Forward pass FLOPs: \\(2 \\cdot B \\cdot D \\cdot K\\)</li> <li>Total FLOPs: Sum over all layers, multiplied by batch size and sequence length</li> </ul>"},{"location":"nlp/22_llm_training_basics/#rule-of-thumb","title":"Rule of thumb","text":"\\[FLOPs \\approx 2 \\cdot    \\text{#tokens} \\cdot  \\text{#parameters (per layer)}\\]"},{"location":"nlp/22_llm_training_basics/#model-flops-utilization-mfu","title":"Model FLOPs Utilization (MFU)","text":"<p>Definition:</p> \\[ MFU = \\frac{\\text{Actual FLOPs/sec}}{\\text{Peak Theoretical FLOPs/sec}} \\] <p>MFU indicates how closely the training process approaches the maximum compute capability of the hardware.</p>"},{"location":"nlp/22_llm_training_basics/#why-mfu-is-not-always-1","title":"Why MFU is not always 1","text":"<ul> <li>Non-matmul operations (activations, normalization, data movement)     are often memory-bound</li> <li>Kernel launch overhead from many small kernels</li> <li>Memory bandwidth bottlenecks</li> <li>Irregular workloads (uneven tensor sizes, short sequences)</li> <li>Communication overhead in multi-GPU setups (sync, data transfer)</li> </ul>"},{"location":"nlp/22_llm_training_basics/#what-is-a-good-mfu","title":"What is a good MFU?","text":"<p>Typically:</p> \\[ MFU \\ge 0.5   \\text{ is considered good} \\] <p>MFU improves when:</p> <ul> <li>Matrix multiplications dominate the workload</li> <li>Batch sizes and sequence lengths are large</li> <li>Code is optimized for the hardware</li> </ul>"},{"location":"nlp/22_llm_training_basics/#mixed-precision-training","title":"Mixed-Precision Training","text":""},{"location":"nlp/22_llm_training_basics/#floating-point-formats-in-gpus","title":"Floating-Point Formats in GPUs","text":""},{"location":"nlp/22_llm_training_basics/#floating-point-101","title":"Floating-point 101","text":"<p>A binary floating-point number is encoded as:</p> \\[\\underbrace{\\text{sign}}_{\\;1\\;\\text{bit}}\\; \\underbrace{\\text{exponent}}_{\\;e\\;\\text{bits}}\\; \\underbrace{\\text{mantissa / significand}}_{\\;m\\;\\text{bits}} \\] Format Bits Exponent Mantissa Approx. Range FP32 (single) 32 8 23 \\(10^{-45}\\) \u2013 \\(10^{38}\\) FP16 (half) 16 5 10 \\(2 \\times 10^{-14}\\) \u2013 \\(2 \\times 10^{15}\\) <p>Precision vs. range: Reducing the mantissa increases spacing between representable values, so small increments (e.g. \\(1.0001\\)) round to \\(1.0\\) in FP16. A narrower exponent field also shrinks dynamic range, increasing overflow/underflow risk.</p>"},{"location":"nlp/22_llm_training_basics/#fp32-vs-fp16-in-neural-network-training","title":"FP32 vs. FP16 in Neural-Network Training","text":"<ul> <li> <p>Default FP32 training: Parameters and gradients are stored in FP32. High memory usage \\(\\rightarrow\\) risk of OOM on large models.</p> </li> <li> <p>Na\u00efve FP16 swap: Reduces memory and bandwidth by 50%, but:</p> <ul> <li>Gradients may underflow to zero</li> <li>Weight updates lose precision \\(\\rightarrow\\) poor convergence</li> </ul> </li> </ul>"},{"location":"nlp/22_llm_training_basics/#mixed-precision-training-core-idea","title":"Mixed-Precision Training: Core Idea","text":"<p>Keep critical operations in FP32 while using FP16 where safe:</p> <ol> <li>Maintain a set of FP32 master weights</li> <li>Cast a working copy of the model to FP16; do the forward pass there.</li> <li>Back-propagate (initial gradients in FP16)</li> <li>Convert (copy) gradients to FP32</li> <li>Update the FP32 master weights with your optimizer (SGD/Adam, etc.).</li> <li>Cast the updated weights back to FP16 for the next forward pass.</li> </ol>"},{"location":"nlp/22_llm_training_basics/#preventing-gradient-underflow-dynamic-loss-scaling","title":"Preventing Gradient Underflow: Dynamic Loss Scaling","text":"<p>Even with mixed precision, tiny gradients can vanish. A practical recipe, adapted from:</p> <p>Procedure:</p> <ul> <li>Choose a loss\u2011scaling factor \\(S\\) (e.g.\\ powers of two).\\footnote{Modern frameworks adjust \\(S\\) automatically (\u201cdynamic\u201d scaling).}</li> <li>Multiply the computed loss by \\(S\\) before back\u2011propagation.</li> <li>Compute FP16 gradients of the scaled loss.</li> <li>Convert gradients to FP32 and divide them by \\(S\\).</li> <li>Proceed with the usual FP32 weight update \\&amp; casting loop above.</li> </ul> <p></p>"},{"location":"nlp/22_llm_training_basics/#benefits","title":"Benefits","text":"<ul> <li>\\(\\approx 2\\times\\) faster math on Tensor Cores (Volta/Ampere+)</li> <li>\\(\\approx 2\\times\\) lower memory footprint, enabling larger batch sizes or models.</li> <li>Same convergence as FP32 when loss scaling is correct</li> </ul>"},{"location":"nlp/22_llm_training_basics/#caveats","title":"Caveats","text":"<ul> <li>Verify numerical stability on your model; some niche layers (e.g. custom CUDA kernels) may not be FP16\u2011safe.</li> <li>Loss scaling adds minor overhead if done manually; use the built\u2011in API of your deep\u2011learning framework whenever possible.</li> </ul>"},{"location":"nlp/22_llm_training_basics/#bfloat16","title":"BFloat16","text":"<p>Google Brain developed bfloat (brain floating point) in 2018 to address this issue. bfloat16 uses the same memory as float16 but has the same dynamic range as float32! The only catch is that the resolution is worse, but this matters less for deep learning.</p> <p>Core idea: BFloat16 (16-bit float) keeps the same exponent size as FP32, so it has a very wide range but with lower precision (7 mantissa bits).  </p> <p>This makes it:</p> <ul> <li>Fast and memory-efficient like FP16</li> <li>Avoids gradient underflow</li> <li>No loss scaling required</li> </ul>"},{"location":"nlp/22_llm_training_basics/#implementation-steps","title":"Implementation steps","text":"<ol> <li>Keep a set og FP32 master weights</li> <li>Cast model to BFloat16 for forward/backward passes(<code>model.to(torch.bfloat16)</code>)</li> <li>Gradients accumulate in FP32 by default</li> <li>Update FP32 master weights as usual</li> <li>Cast updated weights back to BFloat16 for next step</li> </ol> <p>No loss scaling required, BFloat16 simplifies mixed-precision training by providing FP32-like range with FP16-like speed and memory savings.</p>"},{"location":"nlp/22_llm_training_basics/#multi-gpu-training-from-ddp-to-zero","title":"Multi-GPU Training: From DDP to ZeRO","text":"<ol> <li> <p>Single-GPU Setup: In a typical mixed-precision setup:</p> <ul> <li>Model parameters: Stored in FP16 on GPU VRAM.</li> <li>Optimizer state (FP32):<ul> <li>Master weights (for precision-preserving updates)</li> <li>Momentum buffers (e.g., Adam)</li> <li>Variance estimates (e.g., Adam)</li> </ul> </li> </ul> </li> <li> <p>Distributed Data Parallel (DDP) \u2014 Baseline: DDP replicates the entire model and optimizer state across all GPUs and splits the input data:</p> <ul> <li> <p>Each GPU computes a forward and backward pass on its mini-batch.</p> </li> <li> <p>During backpropagation, gradients are synchronized across all GPUs using an AllReduce.</p> </li> <li> <p>Each GPU independently applies the optimizer update to its local copy of the model.</p> </li> </ul> </li> </ol> <p>Communication cost: Each gradient (typically in FP32 unless cast to FP16) is sent across GPUs. The bandwidth cost is roughly \u223c4 bytes per parameter (or \u223c2 bytes with FP16 compression)</p> <p>Limitation: Memory overhead scales poorly \u2014 each GPU stores a full copy of model parameters and optimizer state.</p>"},{"location":"nlp/22_llm_training_basics/#zero-zero-redundancy-optimizer-deepspeed","title":"ZeRO: Zero Redundancy Optimizer (DeepSpeed)","text":"<p>ZeRO removes the major inefficiency of Distributed Data Parallel (DDP), where every GPU keeps a full copy of model parameters, gradients, and optimizer states. Instead, ZeRO partitions (shards) training states across GPUs so that each device holds only the slice of memory it actually needs, while still participating in full model training.</p>"},{"location":"nlp/22_llm_training_basics/#zero-stage-1-optimizer-state-sharding","title":"ZeRO Stage-1: Optimizer State Sharding","text":"<ul> <li>Each GPU stores:<ul> <li>The full FP16 model</li> <li>Only a shard of the optimizer state</li> </ul> </li> <li>Each GPU computes gradients on its data shard</li> <li>Gradients are reduce-scattered so each GPU receives only the gradients for its parameter shard</li> <li>Each GPU updates its own optimizer shard and corresponding parameters</li> <li>Updated parameters are all-gathered so all GPUs hold a synchronized model</li> </ul>"},{"location":"nlp/22_llm_training_basics/#zero-stage-2-optimizer-state-gradient-sharding","title":"ZeRO Stage-2: Optimizer State + Gradient Sharding","text":"<p>Builds upon Stage-1 by also sharding gradients:</p> <ul> <li>Gradients are never fully instantiated in memory</li> <li>During backward pass:<ul> <li>Each GPU computes gradients for a layer</li> <li>Immediately reduces gradients to the GPU responsible for that parameter shard</li> <li>Frees local gradient memory once sent</li> </ul> </li> <li>After backpropagation, each GPU:<ul> <li>Updates its optimizer and parameter shards locally</li> <li>Parameters are synchronized via all-gather before the next forward pass</li> </ul> </li> </ul> <p>Benefit: significantly reduces memory footprint \u2014 gradients and optimizer states are distributed.</p>"},{"location":"nlp/22_llm_training_basics/#zero-stage-3-full-model-gradient-and-optimizer-sharding","title":"ZeRO Stage-3: Full Model, Gradient, and Optimizer Sharding","text":"<p>In this final stage:</p> <ul> <li>Model parameters are sharded across GPUs \u2014 no GPU stores the full model</li> <li>Parameters are materialized just-in-time during forward/backward and deallocated afterward</li> <li>Training requires orchestration of:<ul> <li>Parameter gathering</li> <li>Gradient sharding and reduction</li> <li>Activation checkpointing (optional but common)</li> </ul> </li> <li>Implemented in PyTorch via <code>torch.distributed.fsdp</code> (Fully Sharded Data Parallel)</li> </ul> <p>Use case: enables training of models with hundreds of billions to trillions of parameters on commodity GPU clusters.</p> Training Strategy Model Sharded Gradients Sharded Optimizer Sharded DDP (Baseline) \\(\\times\\) \\(\\times\\) \\(\\times\\) ZeRO Stage-1 \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) ZeRO Stage-2 \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) ZeRO Stage-3 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\)"},{"location":"nlp/22_llm_training_basics/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>ZeRO reduces memory consumption linearly with GPU count.</li> <li>Enables training of large models without sacrificing batch size or needing model parallelism.</li> <li>Fully Sharded Data Parallel (FSDP) in PyTorch and DeepSpeed provide user-friendly APIs to leverage ZeRO at scale.</li> </ul>"},{"location":"nlp/23_reasoning/","title":"23. Reasoning in LLMs","text":""},{"location":"nlp/23_reasoning/#reasoning","title":"Reasoning","text":""},{"location":"nlp/23_reasoning/#what-do-we-mean-by-reasoning","title":"What Do We Mean by \u201cReasoning\u201d?","text":"<p>Reasoning is the disciplined use of facts and logic to reach new conclusions.</p> <ul> <li>Deductive reasoning derives conclusions that must be true if the premises are true.</li> <li>Inductive reasoning generalises from repeated observations to predict what is likely to hold in future.</li> <li>Abductive reasoning infers the most plausible explanation for an observation (\u201cinference to the best explanation\u201d).</li> </ul>"},{"location":"nlp/23_reasoning/#reasoning-in-large-language-models-llms","title":"Reasoning in Large Language Models (LLMs)","text":"<p>Large language models excel at predicting the next token given a context \u2014 essentially a massively multivariate pattern-completion task. Whether this is reasoning is an open research question, but several techniques reliably elicit reasoning-like behavior.</p>"},{"location":"nlp/23_reasoning/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>Chain-of-Thought (CoT) prompts append reasoning demonstrations (e.g., \"Let's think step-by-step\") to encourage the model to reveal latent intermediate states. </li> <li>Least-to-Most (LtM) decomposition breaks a hard task into a sequence of smaller sub\u2011problems; the LLM solves each sub\u2011problem in order, reducing error accumulation. </li> </ul>"},{"location":"nlp/23_reasoning/#counterfactual-probes","title":"Counterfactual Probes","text":"<p>By editing premises and comparing completions, researchers can distinguish memorisation from real generalisation: if the model\u2019s answer tracks the \\emph{counterfactual} change, it has formed some causal abstraction rather than merely retrieving training data.</p>"},{"location":"nlp/23_reasoning/#limitations","title":"Limitations","text":"<ul> <li> <p>Current LLMs have no explicit logical machinery or world model; apparent <code>logic</code> emerges from statistical correlations. </p> </li> <li> <p>Faithfulness is not guaranteed: the model may generate fluent but incorrect reasoning chains (hallucinations).</p> </li> <li> <p>Working memory is finite, so long multi\u2011step proofs can overflow the context window unless external scratchpads or tool use are provided.</p> </li> </ul>"},{"location":"nlp/2_lmnp/","title":"2. Language Models and Word Sequence Probability","text":""},{"location":"nlp/2_lmnp/#chapter-2-language-models-and-word-sequence-probability","title":"Chapter 2: Language Models and Word Sequence Probability","text":"<p>Language models formalize the idea that natural language is not random, but follows statistical regularities. By assigning probabilities to word sequences, they provide a principled way to reason about which sentences are more likely than others.</p>"},{"location":"nlp/2_lmnp/#purpose-of-language-models","title":"Purpose of Language Models","text":"<ul> <li>A language model computes the probability of a sequence of words occurring.</li> <li>Useful in applications like machine translation and speech recognition.</li> </ul> <p>At their core, language models answer a simple question: given what we have seen so far, what word is likely to come next?</p>"},{"location":"nlp/2_lmnp/#probability-of-word-sequences","title":"Probability of Word Sequences","text":"<ul> <li>The probability of a word sequence \\(\\{w_1, w_2, \\ldots, w_m\\}\\) is:    </li> <li>Approximated using only the previous \\(n\\) words (n-gram model):    </li> </ul> <p>This approximation reflects a practical trade-off: modeling long-range dependencies is difficult with limited data and computation.</p>"},{"location":"nlp/2_lmnp/#importance-in-machine-translation","title":"Importance in Machine Translation","text":"<ul> <li>Systems generate multiple candidate translations.</li> <li>Examples: {I have, I had, I has, me have, me had}</li> <li>Each candidate sequence is scored using a probability function.</li> <li>The system selects the one with the highest score.</li> </ul> <p>In this setting, the language model acts as a fluency filter, favoring grammatically and statistically plausible sentences.</p>"},{"location":"nlp/2_lmnp/#n-gram-language-models","title":"N-gram Language Models","text":"<p>To compute word sequence probabilities, we can use the frequency of \\(n\\)-grams compared to the frequency of shorter sequences (e.g., uni-grams). This is the foundation of the n-gram language model.</p> <p>These models rely purely on observed counts and make no use of semantic representations.</p> <ul> <li>Bigram model:    </li> <li>Trigram model:    </li> </ul> <p>These models use a fixed-size window (e.g., previous \\(n\\) words) to predict the next word. However, choosing the right window size is critical. For example:</p> <ul> <li>In the sentence: \u201cAs the proctor started the clock, the students opened their...\u201d</li> <li>A 3-word context window (\u201cthe students opened their\u201d) may predict \u201cbooks.\u201d</li> <li>A longer window capturing \u201cproctor\u201d may increase the likelihood of predicting \u201cexam.\u201d</li> </ul> <p>This illustrates how limited context can restrict the model\u2019s ability to capture meaning.</p>"},{"location":"nlp/2_lmnp/#challenges-with-n-gram-models","title":"Challenges with N-gram Models","text":""},{"location":"nlp/2_lmnp/#1-sparsity","title":"1. Sparsity","text":"<ul> <li>If a specific \\(n\\)-gram (e.g., \\((w_1, w_2, w_3)\\)) never appears, the model assigns a zero probability.</li> <li>Smoothing: Add a small value \\(\\delta\\) to all counts to avoid zero probabilities.</li> <li>Backoff: If the context (e.g., \\(w_1, w_2\\)) is missing, fall back to a shorter context (e.g., \\(w_2\\) alone).</li> <li>Increasing \\(n\\) makes sparsity worse, which is why typically \\(n \\leq 5\\).</li> </ul> <p>Sparsity fundamentally limits how much context classical count-based models can exploit.</p>"},{"location":"nlp/2_lmnp/#2-storage","title":"2. Storage","text":"<ul> <li>All \\(n\\)-gram counts must be stored.</li> <li>Larger \\(n\\) and larger corpora increase model size significantly.</li> </ul> <p>Together, sparsity and storage constraints motivate the transition from count-based language models to neural language models, which replace explicit counting with learned distributed representations.</p>"},{"location":"nlp/3_nn/","title":"3. Window-based Neural Language Models","text":""},{"location":"nlp/3_nn/#chapter-3-window-based-neural-language-model","title":"Chapter 3: Window-based Neural Language Model","text":"<p>To address sparsity and storage issues, Bengio et al. proposed a Neural Probabilistic Language Model, the first large-scale deep learning model for NLP.</p> <ul> <li>Learns distributed representations (embeddings) for words  </li> <li>Uses a neural network to compute probabilities instead of raw counts  </li> </ul> <p>Proposed in 2003, this model marked a shift from purely count-based language models to neural approaches that learn representations jointly with the language model.</p>"},{"location":"nlp/3_nn/#neural-architecture-simplified","title":"Neural Architecture (Simplified)","text":"<p>Given a fixed-length context window, the model estimates the conditional probability of the next word:  </p> <ul> <li> <p>Input: Concatenated word embeddings </p> </li> <li> <p>Hidden layer: </p> </li> <li> <p>Output (softmax over vocabulary): </p> </li> <li> <p>Learns distributed representations (embeddings) for words that capture syntactic and semantic similarity.</p> </li> </ul>"},{"location":"nlp/3_nn/#full-model-equation","title":"Full Model Equation","text":"\\[ \\hat{y} = \\text{softmax}\\!\\left( W^{(2)} \\tanh(W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}) + W^{(3)} \\mathbf{x} + \\mathbf{b}^{(3)} \\right) \\] <ul> <li>\\(W^{(1)}\\) applied to word vectors (input \\(\\rightarrow\\) hidden layer)  </li> <li>\\(W^{(2)}\\) applied to hidden layer (hidden \\(\\rightarrow\\) output)  </li> <li>\\(W^{(3)}\\) connects input directly to output (shortcut connection)  </li> </ul>"},{"location":"nlp/3_nn/#limitations-of-the-window-based-model","title":"Limitations of the Window-based Model","text":"<p>While the window-based neural language model marked a significant advancement, it still suffers from key limitations:</p> <ul> <li> <p>Fixed context size (Markov assumption):   The model relies on a fixed-size context window (e.g., 4 or 5 previous words), enforcing a strong Markov assumption. This limits the model\u2019s ability to capture long-range dependencies and contextual information beyond the window. As a result, important linguistic patterns that span wider contexts may be missed.</p> </li> <li> <p>Position-specific parameterization:   Each word in the context window is embedded and concatenated in a fixed order, meaning that words at different positions are treated differently by the model. This prevents the model from generalizing across positions; for example, the same word appearing in position 1 versus position 4 will be processed differently due to distinct weights applied in the hidden layer.</p> </li> <li> <p>Lack of permutation invariance and poor scalability:   Since the architecture depends on fixed positions and fully connected layers, it does not scale well with longer contexts and lacks flexibility for variable-length input. It also cannot handle unseen word orders or reorderings effectively.</p> </li> </ul> <p>These limitations were among the motivations for later architectures such as recurrent neural networks (RNNs) and transformers, which can model variable-length contexts and better capture sequential dependencies.</p>"},{"location":"nlp/4_rnn/","title":"4. Recurrent Neural Networks","text":""},{"location":"nlp/4_rnn/#chapter-4-recurrent-neural-networks","title":"Chapter 4: Recurrent Neural Networks","text":"<p>Unlike traditional language models that condition on a fixed-size window of previous tokens, RNNs are capable of conditioning on the entire history of previous tokens.</p> <p>This is achieved by maintaining a recurrent hidden state that summarizes past information and is updated at every time-step.</p> <p></p>"},{"location":"nlp/4_rnn/#rnn-architecture","title":"RNN Architecture","text":"<p>The RNN architecture (Figure above) shows three time-steps. Each hidden layer at time-step \\(t\\) receives two inputs:</p> <ul> <li>The input vector at time \\(t\\), \\(x_t\\)</li> <li>The hidden state from the previous time-step, \\(h_{t-1}\\)</li> </ul> <p>These are combined using weights \\(W_{hh}\\) and \\(W_{hx}\\):</p> \\[ h_t = \\sigma(W_{hh} h_{t-1} + W_{hx} x_t + b_1) \\] <p>The hidden state \\(h_t\\) acts as a continuous memory vector that compresses information from all previous inputs \\(x_1, \\dots, x_t\\).</p> <p>The output is calculated as:</p> \\[ \\hat{y}_t = \\text{softmax}(W_S h_t + b_2) \\] <p>Key parameters:</p> <ul> <li>\\(x_t \\in \\mathbb{R}^d\\): Input word vector  </li> <li>\\(W_{hx} \\in \\mathbb{R}^{D_h \\times d}\\): Input weight matrix  </li> <li>\\(W_{hh} \\in \\mathbb{R}^{D_h \\times D_h}\\): Hidden state weight matrix  </li> <li>\\(W_S \\in \\mathbb{R}^{|V| \\times D_h}\\): Output weight matrix  </li> <li>\\(\\sigma\\): Non-linear function such as tanh  </li> <li>\\(\\hat{y}_t \\in \\mathbb{R}^{|V|}\\): Probability distribution over vocabulary  </li> </ul>"},{"location":"nlp/4_rnn/#training-an-rnn-language-model","title":"Training an RNN Language Model","text":"<p>To train a Recurrent Neural Network Language Model (RNN-LM), we begin with a large corpus of text, represented as a sequence of word tokens. The model is trained to predict the next word at each time-step, given all preceding words in the sequence.</p> <p>At each time-step \\(t\\):</p> <ul> <li>The model receives an input vector \\(x_t\\) and computes a hidden state \\(h_t\\) using the current input and the previous hidden state \\(h_{t-1}\\).</li> <li>The output layer produces a probability distribution \\(\\hat{y}_t\\) over the vocabulary.</li> <li>The cross-entropy loss is computed between \\(\\hat{y}_t\\) and the ground-truth one-hot vector \\(y_t\\).</li> </ul> <p>Training an RNN language model corresponds to maximizing the log-likelihood of the observed sequence:</p> \\[\\log P(x_1, \\ldots, x_T) = \\sum_{t=1}^{T} \\log P\\!\\left(x_{t+1} \\mid x_1, \\ldots, x_t\\right)\\] <p>The total training objective is to minimize the average cross-entropy loss across all time-steps and sequences in the training set.</p> \\[ J^{(t)}(\\theta) = -\\sum_{j=1}^{|V|} y_{t,j} \\log \\hat{y}_{t,j} \\]"},{"location":"nlp/4_rnn/#teacher-forcing","title":"Teacher Forcing","text":"<p>During training, a technique called teacher forcing is commonly used. Instead of feeding the model\u2019s own prediction \\(\\hat{y}_{t-1}\\) as input at the next time-step, we use the ground-truth word \\(y_{t-1}\\). This stabilizes and accelerates learning by preventing the model from compounding its own mistakes during early training. However, it introduces a discrepancy between training and inference, known as exposure bias, since the model must rely on its own predictions at test time. Exposure bias becomes more pronounced for long sequences, where early prediction errors can cascade through the remainder of the sequence.</p>"},{"location":"nlp/4_rnn/#backpropagation-through-time","title":"Backpropagation Through Time","text":"<p>Training an RNN requires computing gradients over sequences of time-dependent operations. This is done using Backpropagation Through Time (BPTT):</p> <ul> <li>The RNN is unrolled over \\(T\\) time-steps, creating a computational graph where the same parameters are shared at each step.</li> <li>Gradients are computed by propagating errors backward through time from \\(t = T\\) to \\(t = 0\\), summing contributions from all steps.</li> <li>This process allows the model to adjust its parameters based on dependencies that span multiple time-steps.</li> </ul> <p>In practice, full BPTT over long sequences can be computationally expensive and unstable. Therefore, a variant called truncated BPTT is often used, where backpropagation is limited to a fixed number of time-steps (e.g., 20 or 50). This trades off full temporal context for efficiency and stability. Truncated BPTT implicitly limits the temporal horizon over which dependencies can be learned, acting as a practical approximation to full sequence optimization.</p> <p>BPTT is sensitive to gradient vanishing and exploding problems, especially with non-linear activations like tanh or sigmoid. Techniques such as gradient clipping and using gated architectures like LSTMs or GRUs are commonly employed to address these issues.</p>"},{"location":"nlp/4_rnn/#efficient-training-with-mini-batches","title":"Efficient Training with Mini-Batches","text":"<p>Computing the loss and gradients over the entire corpus simultaneously is impractical due to memory limitations. Instead, training is performed on smaller units using mini-batches:</p> <ul> <li>The text corpus is divided into sequences such as sentences or fixed-length token chunks.</li> <li>A batch of these sequences is processed together to compute the average loss and gradients.</li> <li>Stochastic Gradient Descent (SGD) or its variants (e.g., Adam) are used to update parameters based on each batch.</li> </ul> <p>Handling variable-length sequences within batches requires padding and masking:</p> <ul> <li>Shorter sequences are padded with special tokens to match the longest sequence in the batch.</li> <li>Padding positions are masked to prevent them from contributing to the loss or gradient updates.</li> </ul> <p>This mini-batch training paradigm enables scalable computation and leverages hardware acceleration such as GPUs and TPUs.</p>"},{"location":"nlp/4_rnn/#training-vs-inference","title":"Training vs. Inference","text":"<p>At inference time, teacher forcing is not available. Each predicted word is fed back into the model as input for the next step. This difference in input distribution between training and inference can degrade performance if the model is not robust to its own prediction errors. Techniques such as scheduled sampling or fine-tuning with generated sequences are sometimes used to mitigate this mismatch.</p>"},{"location":"nlp/4_rnn/#perplexity","title":"Perplexity","text":"<ul> <li>The standard evaluation metric for language models is perplexity.</li> </ul> \\[ \\text{perplexity} = \\left( \\prod_{t=1}^{T} \\frac{1}{P_{\\text{LM}}(x^{(t+1)} \\mid x^{(1)}, \\ldots, x^{(t)})} \\right)^{\\frac{1}{T}} \\] <ul> <li>Inverse probability of corpus according to the language model, normalized by the number of words.</li> <li>This is equal to the exponential of the cross-entropy loss \\(J(\\theta)\\):</li> </ul> \\[ \\left( \\prod_{t=1}^{T} \\frac{1}{\\hat{y}^{(t)}_{x_{t+1}}} \\right)^{\\frac{1}{T}} = \\exp\\left( \\frac{1}{T} \\sum_{t=1}^{T} -\\log \\hat{y}^{(t)}_{x_{t+1}} \\right) = \\exp(J(\\theta)) \\] <p>Perplexity can be viewed as the geometric mean of the inverse predicted probabilities assigned to the true next tokens.</p> <p>Perplexity can be interpreted as the effective average number of choices the model is considering at each time-step:</p> <ul> <li>A perplexity of 1 means the model predicts every word perfectly.</li> <li>A perplexity of 50 means the model is as uncertain as choosing uniformly from 50 possible words.</li> </ul> <p>Lower perplexity indicates better predictive performance and correlates with fluency and accuracy in language generation.</p> <p>Note: Perplexity is sensitive to vocabulary size and tokenization. Models should be compared under identical preprocessing conditions.</p>"},{"location":"nlp/4_rnn/#memory-scaling-in-recurrent-neural-networks","title":"Memory Scaling in Recurrent Neural Networks","text":"<p>From a machine learning perspective, the memory requirements of RNNs can be divided into model parameters and runtime memory.</p>"},{"location":"nlp/4_rnn/#model-parameters-static-memory","title":"Model Parameters (Static Memory)","text":"<p>These are the weights and biases defining the RNN structure:</p> <ul> <li>\\(W_{hx} \\in \\mathbb{R}^{D_h \\times d}\\)</li> <li>\\(W_{hh} \\in \\mathbb{R}^{D_h \\times D_h}\\)</li> <li>\\(W_S \\in \\mathbb{R}^{|V| \\times D_h}\\)</li> <li>Bias vectors for each layer</li> </ul> <p>The memory required to store these parameters is constant with respect to corpus size. That is, increasing the number of sentences or tokens in the dataset does not change the size of these matrices.</p>"},{"location":"nlp/4_rnn/#runtime-memory-dynamic-per-sequence","title":"Runtime Memory (Dynamic, per sequence)","text":"<p>During training with BPTT, the RNN stores:</p> <ul> <li>Input vectors \\(x_t\\)</li> <li>Hidden states \\(h_t\\)</li> <li>Intermediate gradient-related states</li> </ul> <p>Runtime memory scales linearly with sequence length \\(T\\). A sentence with \\(T\\) words requires storage for \\(T\\) hidden states and possibly \\(T\\) sets of gradients.</p> <ul> <li>Model parameter count depends only on architecture parameters \\(D_h\\), \\(d\\), and \\(|V|\\).</li> <li>Training memory grows proportionally with input sequence length.</li> </ul> <p>Insight: RNNs are more memory-efficient than traditional \\(n\\)-gram models in terms of model size, but training on long sequences increases memory usage due to the need to store intermediate activations over time.</p>"},{"location":"nlp/4_rnn/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":"<p>Advantages:</p> <ol> <li>Can handle variable-length sequences  </li> <li>Parameter size independent of input length  </li> <li>Can model long-range dependencies  </li> <li>Weight sharing across time-steps  </li> </ol> <p>Disadvantages:</p> <ol> <li>Sequential computation limits parallelization  </li> <li>Prone to vanishing and exploding gradients  </li> <li>Difficulty in learning very long-term dependencies in practice</li> </ol>"},{"location":"nlp/4_rnn/#vanishing-and-exploding-gradients","title":"Vanishing and Exploding Gradients","text":"<p>The total gradient with respect to parameters is:</p> \\[ \\frac{\\partial E}{\\partial W} = \\sum_{t=1}^{T} \\sum_{k=1}^{t} \\frac{\\partial E_t}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial h_t} \\cdot \\left( \\prod_{j=k+1}^{t} \\frac{\\partial h_j}{\\partial h_{j-1}} \\right) \\cdot \\frac{\\partial h_k}{\\partial W} \\] <p>Jacobian norm bound:</p> \\[ \\left\\| \\frac{\\partial h_t}{\\partial h_k} \\right\\| \\le (\\beta_W \\beta_h)^{t-k} \\] <p>Gradient clipping:</p> \\[ \\text{if } \\|g\\| \\ge \\text{threshold}, \\quad g \\leftarrow \\frac{\\text{threshold}}{\\|g\\|} \\cdot g \\] <p>Solutions to vanishing gradients:</p> <ul> <li>Identity initialization for \\(W_{hh}\\)</li> <li>Use ReLU instead of sigmoid or tanh</li> </ul> <p>While RNNs provide a principled framework for sequence modeling and variable-length context, their training difficulties and limited parallelism motivated the development of gated recurrent architectures and attention-based models, which we study next.</p>"},{"location":"nlp/5_lstm/","title":"5. Long Short-Term Memory (LSTM) Networks","text":""},{"location":"nlp/5_lstm/#chapter-5-long-short-term-memory","title":"Chapter 5: Long Short-term Memory","text":"<p>LSTMs were introduced specifically to address the vanishing gradient problem encountered in vanilla RNNs when modeling long sequences. LSTMs are a special kind of RNN capable of learning long-term dependencies. At each time-step \\(t\\), the LSTM maintains a hidden state \\(h^{(t)}\\) and a cell state \\(c^{(t)}\\). The key components of an LSTM are the input, forget, and output gates, which regulate the flow of information.</p> <p></p>"},{"location":"nlp/5_lstm/#gates-and-their-roles","title":"Gates and Their Roles","text":"<p>Each gate is implemented as a sigmoid-activated affine transformation of the previous hidden state and the current input.</p> <ul> <li> <p>Forget Gate: Controls what information from the previous cell state \\(c^{(t-1)}\\) should be kept versus forgotten:    </p> </li> <li> <p>Input Gate: Determines what new information should be stored in the cell state:    </p> </li> <li> <p>Output Gate: Decides what part of the cell state should be output:    </p> </li> </ul>"},{"location":"nlp/5_lstm/#internal-computation","title":"Internal Computation","text":"<ul> <li> <p>New cell content: this is the new content to be written to the cell:</p> \\[\\tilde{c}^{(t)} = \\tanh(W_c h^{(t-1)} + U_c x^{(t)} + b_c)\\] </li> <li> <p>Cell state update: erase (forget) some content from the last cell state and write (input) some new cell content:    </p> <p>The cell state therefore evolves through time via controlled addition and deletion of information, rather than complete overwriting.</p> </li> <li> <p>Hidden state output: read (output) some content from the cell:    </p> <p>While the cell state stores long-term information, the hidden state serves as the short-term representation exposed to the rest of the network.</p> </li> </ul> <p>All gate outputs are vectors with values in \\([0, 1]\\) using the sigmoid function \\(\\sigma\\). The operator \\(\\odot\\) denotes element-wise (Hadamard) product.</p> <p>LSTMs mitigate the vanishing gradient problem by enabling the cell state \\(c^{(t)}\\) to carry forward important information over long sequences.</p>"},{"location":"nlp/5_lstm/#lstm-additive-memory-and-gradient-flow","title":"LSTM: Additive Memory and Gradient Flow","text":"<p>One of the key strengths of LSTMs lies in the additive nature of its cell state update, which is visually emphasized in the diagram by the \\(\\oplus\\) (plus) operation in the center of the cell.</p>"},{"location":"nlp/5_lstm/#why-the-is-the-secret","title":"Why the \u201c+\u201d Is the Secret","text":"<p>The key architectural difference between LSTMs and vanilla RNNs lies in how information is propagated through time. Traditional RNNs use repeated multiplications when propagating hidden states across time, which can lead to gradients either vanishing (becoming very small) or exploding (becoming very large). This makes learning long-term dependencies extremely difficult.</p> <p>LSTMs avoid this through the structure of their cell state update:  </p> <p>When the forget gate is close to 1 and the input gate is close to 0, the cell state behaves like an identity mapping across time-steps.</p> <p>This equation is element-wise additive, rather than multiplicative. The additive interaction allows gradients to flow back through time more stably.</p>"},{"location":"nlp/5_lstm/#mitigating-vanishing-gradients","title":"Mitigating Vanishing Gradients","text":"<ul> <li>The forget gate \\(f^{(t)}\\) controls how much of the previous cell state is retained. When \\(f^{(t)}\\) is close to 1, \\(c^{(t-1)}\\) is passed forward nearly unchanged.</li> <li>This allows information and gradients to flow across many time-steps with minimal decay.</li> <li>Because of the additive pathway through \\(c^{(t)}\\), backpropagation through time (BPTT) can preserve useful gradients over long sequences.</li> </ul>"},{"location":"nlp/5_lstm/#summary","title":"Summary","text":"<p>The \u201c\\(\\oplus\\)\u201d operator (additive memory update) is the core innovation in LSTMs. It allows the model to accumulate and preserve information over long time horizons, effectively addressing the vanishing gradient problem that affects vanilla RNNs.</p> <p></p> <p>This design makes LSTMs significantly more effective than vanilla RNNs for tasks involving long-range dependencies, such as language modeling and sequence labeling.</p>"},{"location":"nlp/6_app_rnn/","title":"6. Applications of RNN","text":""},{"location":"nlp/6_app_rnn/#chapter-6-application-of-rnns","title":"Chapter 6: Application of RNNs","text":"<p>Recurrent neural networks are particularly well suited for tasks involving sequential structure, where predictions depend on both current input and past context.</p> <ol> <li>Part-of-Speech Tagging and Named Entity Recognition</li> </ol> <p>These are sequence labeling tasks where each word in a sentence is assigned a tag. An RNN processes the input word sequence one token at a time and produces an output tag for each time-step. In these tasks, the output sequence has the same length as the input sequence.</p> <ul> <li>Input: A sequence of word vectors \\((x_1, x_2, \\dots, x_T)\\) </li> <li>Architecture: A unidirectional RNN (e.g., vanilla RNN, LSTM, or GRU) computes hidden states \\((h_1, h_2, \\dots, h_T)\\) </li> <li>Output: Each \\(h_t\\) is passed through a softmax classifier to predict a tag \\(y_t\\) for the corresponding word  </li> </ul> <p>Bidirectional RNNs are also commonly used in practice, as tag predictions often depend on both left and right context.</p> <p></p> <ol> <li> <p>Sentiment Analysis Unlike sequence labeling, sentiment analysis produces a single label for the entire sequence. This is a sequence classification task where the entire input sequence is mapped to a single output label (e.g., positive or negative sentiment).</p> </li> <li> <p>Input: A sequence of word vectors \\((x_1, x_2, \\dots, x_T)\\) </p> </li> <li>Architecture: A bidirectional RNN (BiRNN) is commonly used to capture both past and future context. Hidden states from both directions \\((\\overrightarrow{h_t}, \\overleftarrow{h_t})\\) are concatenated  </li> <li>Output: All hidden states are typically aggregated (e.g., by mean pooling, attention, or taking the final state), then passed through a feedforward layer and softmax for classification. Aggregation converts a variable-length sequence of hidden states into a fixed-dimensional representation suitable for classification.</li> </ol> <p></p> <p>Note: Bidirectional architectures often yield better performance in sentiment tasks because sentiment cues may appear anywhere in the sequence and depend on both preceding and following words.</p> <p></p> <ol> <li>Multi-layer RNNs</li> </ol> <p>Stacking multiple RNN layers allows the model to learn increasingly abstract representations of the input sequence, similar to how deeper CNNs or MLPs work in vision and other tasks. Each layer processes the full sequence, allowing higher layers to operate on representations produced by lower layers.</p> <ul> <li>Lower layers: Learn low-level features (e.g., short-range dependencies or local syntax)  </li> <li>Higher layers: Learn high-level abstractions (e.g., semantics, global structure)  </li> </ul> <p></p> <ol> <li> <p>Seq-to-seq Some tasks require transforming one sequence into another sequence of different length.</p> </li> <li> <p>Sequence-to-sequence models are based on the encoder-decoder architecture  </p> </li> <li>The encoder processes the input sequence into a fixed-length neural representation  </li> <li>The decoder generates the output sequence from this encoded representation  </li> <li>When both the input and output are sequences, the model is referred to as a seq2seq model  </li> </ol> <p>Early seq2seq models rely on a fixed-length encoded representation, which can limit performance on long sequences.</p> <p>Applications of sequence-to-sequence models beyond machine translation:</p> <ul> <li>Summarization: Long documents \\(\\rightarrow\\) short summaries  </li> <li>Dialogue generation: Previous utterances \\(\\rightarrow\\) next response  </li> <li>Syntactic parsing: Input text \\(\\rightarrow\\) parse tree (as a sequence)  </li> <li>Code generation: Natural language \\(\\rightarrow\\) source code (e.g., Python)  </li> </ul> <p></p> <p>These applications illustrate how RNNs can be adapted to a wide range of sequence modeling problems by varying the architecture and output structure.</p>"},{"location":"nlp/7_eval_nlp/","title":"7. Evaluation of Language Models","text":""},{"location":"nlp/7_eval_nlp/#chapter-7-evaluation-metrics-in-nlp","title":"Chapter 7: Evaluation Metrics in NLP","text":"<p>Evaluating NLP models is essential to ensure their performance, generalizability, and utility. The appropriate metric depends on the specific task, data characteristics, and desired outcome. Below, we detail evaluation methods used for key NLP tasks and provide a comparative table of metrics.</p> <p>Evaluation metrics quantify different aspects of model behavior, such as accuracy, fluency, semantic adequacy, and robustness, and no single metric is sufficient for all tasks.</p>"},{"location":"nlp/7_eval_nlp/#machine-translation-evaluation","title":"Machine Translation Evaluation","text":"<p>Machine translation is a structured generation task where outputs are compared against one or more human reference translations.</p>"},{"location":"nlp/7_eval_nlp/#1-bleu-bilingual-evaluation-understudy","title":"1. BLEU (Bilingual Evaluation Understudy)","text":"<p>BLEU is a precision-based metric used to evaluate the quality of machine-translated text by comparing it to one or more human reference translations. It uses modified n-gram precision and a brevity penalty to penalize overly short translations.</p> \\[ \\text{BLEU} = \\text{BP} \\cdot \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right) \\] <p>where:</p> <ul> <li>\\(p_n\\) is the modified (clipped) precision for n-grams of size \\(n\\)</li> <li>\\(w_n\\) is the weight for each n-gram order (typically \\(w_n = \\frac{1}{N}\\))</li> <li>\\(\\text{BP}\\) is the brevity penalt y to penalize translations that are too short BLEU primarily measures surface-level n-gram overlap and does not directly capture semantic adequacy or fluency.</li> </ul> <p>The clipped precision \\(p_n\\) is defined as:</p> \\[ p_n = \\frac{ \\sum_{\\text{ngram} \\in C} \\min\\left( \\text{Count}_{\\text{clip}}(\\text{ngram}), \\text{MaxRefCount}(\\text{ngram}) \\right) }{ \\sum_{\\text{ngram} \\in C} \\text{Count}(\\text{ngram}) } \\] <p>where:</p> <ul> <li>\\(C\\) is the set of n-grams of order \\(n\\) in the candidate translation</li> <li>\\(\\text{Count}(\\text{ngram})\\) is the number of times an n-gram appears in the candidate</li> <li>\\(\\text{MaxRefCount}(\\text{ngram})\\) is the maximum number of times the n-gram appears in any reference translation</li> <li>\\(\\text{Count}_{\\text{clip}}(\\text{ngram})\\) is clipped to avoid overcounting spurious matches</li> </ul> <p>Clipping prevents a system from gaining excessive credit by repeating n-grams that appear only a limited number of times in the references.</p> <p>The brevity penalty (BP) is given by:</p> \\[ \\text{BP} = \\begin{cases} 1 &amp; \\text{if } c &gt; r \\\\ \\exp\\left(1 - \\frac{r}{c}\\right) &amp; \\text{if } c \\le r \\end{cases} \\] <p>where:</p> <ul> <li>\\(c\\) is the length of the candidate translation</li> <li>\\(r\\) is the effective reference length (usually the closest reference length to \\(c\\))</li> </ul> <p>Without the brevity penalty, a system could achieve high precision by generating unnaturally short translations.</p> <p>Use: BLEU is widely used for evaluating machine translation systems and is more reliable at the corpus level than at the sentence level.</p>"},{"location":"nlp/7_eval_nlp/#2-rouge-recall-oriented-understudy-for-gisting-evaluation","title":"2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)","text":"<p>Unlike BLEU, which emphasizes precision, ROUGE is recall-oriented and focuses on how much of the reference content is covered by the generated text.</p> <p>Originally designed for summarization, ROUGE can also be used for machine translation and other generation tasks. It primarily measures recall of overlapping units such as n-grams and longest common subsequences.</p> <ul> <li>ROUGE-N: Overlap of n-grams (e.g., ROUGE-1, ROUGE-2)</li> <li>ROUGE-L: Longest Common Subsequence (LCS)-based recall</li> <li>ROUGE-S: Skip bigram-based F-measure</li> </ul> \\[ \\text{ROUGE-N Recall} = \\frac{ \\sum_{\\text{reference}} \\text{match}_{n\\text{-grams}} }{ \\sum_{\\text{reference}} \\text{total}_{n\\text{-grams}} } \\] <p>Use: Especially useful in summarization tasks and adaptable to translation.</p>"},{"location":"nlp/7_eval_nlp/#other-metrics-for-mt-and-generation","title":"Other Metrics for MT and Generation","text":"<p>Recent evaluation metrics aim to move beyond surface overlap by incorporating linguistic knowledge or learned semantic representations.</p> <ul> <li>METEOR: Considers synonym matching, stemming, and alignment; combines precision and recall using an F-score</li> <li>chrF: Character n-gram F-score; language-agnostic and effective for morphologically rich languages</li> <li>BERTScore: Uses contextual embeddings from BERT to compute similarity between candidate and reference</li> <li>COMET / BLEURT: Learned metrics trained on human judgment data; state-of-the-art for semantic machine translation evaluation</li> </ul> <p>While these metrics correlate better with human judgments, they depend on pretrained models and training data biases.</p>"},{"location":"nlp/7_eval_nlp/#comprehensive-evaluation-table-for-nlp-tasks","title":"Comprehensive Evaluation Table for NLP Tasks","text":"<p>Table below summarizes commonly used metrics across major NLP tasks, highlighting typical evaluation practices rather than exhaustive standards.</p> Task Common Metrics Notes Part-of-Speech Tagging Accuracy Fraction of correctly predicted tags per token Named Entity Recognition Precision, Recall, F1-score (token/span-level) Evaluates boundary and entity type correctness using strict span or loose token overlap Sentiment Analysis Accuracy, F1-score, MCC Binary or multi-class classification; class imbalance is often a concern Text Classification Accuracy, Precision, Recall, F1-score General-purpose classification; often multi-label or hierarchical Machine Translation BLEU, ROUGE, METEOR, chrF, COMET, BERTScore BLEU is standard; newer metrics better reflect semantic adequacy Summarization ROUGE-1/2/L, BERTScore, BLEURT ROUGE-L captures sequence overlap; embedding-based metrics assess meaning Question Answering EM, F1-score Used for span-based QA; F1 accounts for partial overlap Text Generation BLEU, ROUGE, BERTScore, MAUVE, Human evaluation Open-ended generation requires fluency and diversity metrics Dialogue Systems USR, BLEU, METEOR, BERTScore, Human evaluation Measures relevance, coherence, and engagement Text Simplification SARI, FKGL, BLEU SARI evaluates beneficial edits; BLEU may not correlate with simplicity Coreference Resolution MUC, B\\(^3\\), CEAF Measures clustering of entity mentions; metrics are often used together Language Modeling Perplexity, Cross-Entropy Evaluates next-token prediction; lower is better Paraphrase Detection / Text Similarity Accuracy, Cosine similarity, Pearson/Spearman correlation Used in semantic textual similarity and entailment tasks Information Retrieval / Ranking MRR, MAP, Recall@k, nDCG Measures ranking quality for documents or passages <p>In practice, reliable evaluation often combines automatic metrics with human judgment, especially for open-ended generation tasks. Metric selection should be guided by task objectives, data characteristics, and known limitations of each evaluation method.</p>"},{"location":"nlp/8_attention_seq2seq/","title":"8. Attention Mechanism in Sequence-to-Sequence Models","text":""},{"location":"nlp/8_attention_seq2seq/#8-attention-mechanism-in-sequence-to-sequence-models","title":"8. Attention Mechanism in Sequence-to-Sequence Models","text":"<p>Traditional sequence-to-sequence (seq2seq) models suffer from a bottleneck where the entire source sentence is compressed into a single fixed-size vector (i.e., the final encoder hidden state). This poses challenges, particularly for long or information-rich inputs, as important details may be lost. This fixed-length bottleneck becomes increasingly problematic as input sequences grow longer or contain multiple salient elements.</p> <p>The attention mechanism alleviates this issue by allowing the decoder to dynamically attend to different parts of the input sequence at each time step. Rather than depending solely on a single context vector, the decoder computes a weighted combination of all encoder hidden states, tailored for each output step. Intuitively, attention allows the decoder to query the encoder for the most relevant information at each decoding step.</p> <p></p>"},{"location":"nlp/8_attention_seq2seq/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>Assume the encoder outputs a sequence of hidden states. The encoder hidden states represent the source sequence, while the decoder hidden state represents the current generation context.</p> \\[ h_1, h_2, \\dots, h_N \\in \\mathbb{R}^h \\] <p>and the decoder has a hidden state at time step \\(t\\):</p> \\[ s_t \\in \\mathbb{R}^h \\] <p>The attention mechanism proceeds as follows:</p> <ol> <li> <p>Score computation: Compute unnormalized attention scores between the decoder hidden state and each encoder hidden state:</p> \\[ e^t = [s_t^\\top h_1, \\dots, s_t^\\top h_N] \\in \\mathbb{R}^N \\] <p>Each score measures the compatibility between the current decoder state and a specific encoder state.</p> </li> <li> <p>Attention weights: Apply the softmax function to normalize these scores and obtain a probability distribution over the input positions:</p> \\[ \\alpha^t = \\text{softmax}(e^t) \\in \\mathbb{R}^N \\] <p>The resulting weights form a probability distribution over input positions.</p> </li> <li> <p>Context vector: Compute the attention output \\(a_t\\) as the weighted sum of the encoder hidden states:</p> \\[ a_t = \\sum_{i=1}^N \\alpha_i^t h_i \\in \\mathbb{R}^h \\] <p>This context vector changes at every decoding step, enabling dynamic focus over the input sequence.</p> </li> <li> <p>Final decoder input: Concatenate the attention vector \\(a_t\\) with the decoder hidden state \\(s_t\\) to form a rich context vector for output generation:</p> \\[ [a_t; s_t] \\in \\mathbb{R}^{2h} \\] </li> </ol> <p>This combined vector is typically passed through a feedforward layer or used directly for predicting the output token.</p>"},{"location":"nlp/8_attention_seq2seq/#interpretability-and-alignment","title":"Interpretability and Alignment","text":"<p>One of the key advantages of attention is that it provides a degree of interpretability to the model. Specifically:</p> <ul> <li>Alignment visualization: The attention weights \\(\\alpha^t\\) at each decoder step can be visualized to see which parts of the input the model is focusing on.</li> <li>Soft alignment: Attention naturally yields a soft alignment between source and target tokens without explicit supervision. The network learns this alignment purely from end-to-end training.</li> <li>Insight into model behavior: These alignments allow us to debug, explain, and understand the model's translation or generation decisions.</li> </ul> <p>This emergent alignment capability is one of the reasons attention mechanisms are considered both powerful and elegant.</p>"},{"location":"nlp/8_attention_seq2seq/#variants-of-attention-mechanisms","title":"Variants of Attention Mechanisms","text":"<p>While the basic attention mechanism uses a simple dot product between decoder and encoder hidden states, there are several alternative formulations that aim to improve the flexibility or expressiveness of the attention scoring function. Let \\(h_i \\in \\mathbb{R}^{d_1}\\) denote the encoder hidden state at position \\(i\\), and \\(s \\in \\mathbb{R}^{d_2}\\) be the current decoder hidden state.</p>"},{"location":"nlp/8_attention_seq2seq/#1-dot-product-attention-luong-et-al","title":"1. Dot-Product Attention (Luong et al.)","text":"\\[ e_i = s^\\top h_i \\in \\mathbb{R} \\] <p>This is the simplest form of attention, assuming that \\(d_1 = d_2\\). This method is efficient and often works well in practice, but lacks trainable parameters in the scoring function. Its simplicity makes it computationally efficient and well suited for large-scale models.</p>"},{"location":"nlp/8_attention_seq2seq/#2-multiplicative-bilinear-attention","title":"2. Multiplicative (Bilinear) Attention","text":"\\[ e_i = s^\\top W h_i \\in \\mathbb{R} \\] <p>where \\(W \\in \\mathbb{R}^{d_2 \\times d_1}\\) is a learned weight matrix. This adds more expressiveness to the attention mechanism, enabling a trainable compatibility function between \\(s\\) and \\(h_i\\). </p>"},{"location":"nlp/8_attention_seq2seq/#3-reduced-rank-multiplicative-attention","title":"3. Reduced-Rank Multiplicative Attention","text":"\\[ e_i = s^\\top (U^\\top V) h_i = (Us)^\\top (Vh_i) \\] <p>with \\(U \\in \\mathbb{R}^{k \\times d_2}\\) and \\(V \\in \\mathbb{R}^{k \\times d_1}\\), where \\(k \\ll d_1, d_2\\). This low-rank factorization reduces the number of parameters and computations and is conceptually related to self-attention in Transformers. This formulation trades expressiveness for efficiency by constraining the attention interaction to a lower-dimensional subspace.</p>"},{"location":"nlp/8_attention_seq2seq/#4-additive-bahdanau-attention","title":"4. Additive (Bahdanau) Attention","text":"\\[ e_i = v^\\top \\tanh(W_1 h_i + W_2 s) \\in \\mathbb{R} \\] <p>where \\(W_1 \\in \\mathbb{R}^{d_3 \\times d_1}\\), \\(W_2 \\in \\mathbb{R}^{d_3 \\times d_2}\\) are learned weight matrices, and \\(v \\in \\mathbb{R}^{d_3}\\) is a learned vector. Here, \\(d_3\\) is a tunable dimensionality (sometimes called the attention dimension). Despite its name, this formulation involves a nonlinearity and trainable layers, making it functionally a feedforward neural network scoring mechanism. This form of attention was used in early neural machine translation systems and performs well when encoder and decoder dimensions differ.</p> <p>Attention mechanisms eliminate the fixed-length context bottleneck and form the foundation for modern architectures such as Transformers, which rely entirely on attention for sequence modeling.</p>"},{"location":"nlp/9_selfattention/","title":"9. Self- Attention","text":""},{"location":"nlp/9_selfattention/#self-attention-and-transformers","title":"Self-Attention and Transformers","text":"<p>This chapter explains how self-attention addresses the core limitations of recurrent architectures and forms the foundation of Transformer models.</p>"},{"location":"nlp/9_selfattention/#limitations-of-recurrent-models-linear-interaction-distance","title":"Limitations of Recurrent Models: Linear Interaction Distance","text":"<p>Recurrent neural networks (RNNs) process input sequences sequentially, typically from left to right. This enforces a form of linear locality, which reflects the useful heuristic that nearby words often influence each other's meanings. However, this sequential structure introduces several key limitations:</p> <ul> <li> <p>Inefficient long-range dependencies: For two tokens separated by \\(n\\) positions, RNNs require \\(O(n)\\) steps for information to propagate between them. This makes it difficult to model long-distance dependencies, as the gradient signal must traverse many time steps, leading to vanishing or exploding gradients. Even in gated variants such as LSTMs and GRUs, this remains a practical bottleneck.</p> </li> <li> <p>Hardcoded sequential bias: The linear order of token processing is baked into the architecture. However, natural language often exhibits hierarchical or non-sequential structure that is not well captured by strictly left-to-right modeling.</p> </li> <li> <p>Limited parallelization: The computation of each hidden state depends on the previous one. Both forward and backward passes involve \\(O(n)\\) inherently sequential operations, preventing full utilization of modern parallel computing hardware such as GPUs.</p> </li> <li> <p>Scalability challenges: Due to the sequential nature of training, RNNs are slower to train on large datasets and harder to scale to deep architectures.</p> </li> </ul> <p>These issues stem from the requirement that information must pass through a chain of intermediate states. These limitations motivate the development of alternative architectures. In particular, self-attention mechanisms, as introduced in the Transformer model, allow each token to attend to all others directly in a single logical step, enabling efficient modeling of long-range dependencies and highly parallelizable computation.</p>"},{"location":"nlp/9_selfattention/#attention-mechanisms","title":"Attention Mechanisms","text":"<p>Attention mechanisms enable each word in a sentence to dynamically incorporate information from other words. At a high level, attention treats each word\u2019s representation as a query that retrieves and integrates information from a set of key-value pairs derived from the same or another sequence. In self-attention, queries, keys, and values are all derived from the same input sequence.</p> <p>Unlike recurrent models, attention does not rely on sequential processing. This leads to two key computational benefits:</p> <ul> <li> <p>Constant logical interaction distance: Any token can attend to any other token within a sentence in a single logical step, making the maximum dependency path \\(O(1)\\). However, the actual computational cost remains \\(O(n^2)\\) due to pairwise interactions.</p> </li> <li> <p>Fully parallelizable: Since all token interactions can be computed simultaneously, the number of sequential (unparallelizable) operations does not grow with sequence length.</p> </li> </ul> <p>From encoder-decoder to self-attention. In earlier encoder-decoder models (e.g., for machine translation), attention was applied from the decoder to the encoder, allowing the decoder to selectively focus on parts of the input sequence. In self-attention, we apply the same mechanism within a single sequence, allowing each word to attend to every other word in the same sequence.</p> <p>Attention as soft lookup. Conceptually, attention can be viewed as a soft, weighted lookup in a key-value store. Each query computes a similarity score with every key, producing a weight (typically via softmax). The output is a weighted average of the values:</p> \\[ \\text{output} = \\operatorname{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V \\] <p></p>"},{"location":"nlp/9_selfattention/#self-attention-computing-contextual-representations","title":"Self-Attention: Computing Contextual Representations","text":"<p>Let \\(\\mathbf{w}_{1:n}\\) be a sequence of token indices. Each token \\(\\mathbf{w}_i\\) is mapped to an embedding vector \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) using an embedding matrix \\(E \\in \\mathbb{R}^{d \\times |V|}\\).</p> <p>For each input vector \\(\\mathbf{x}_i\\), we compute three linear projections:</p> \\[ \\mathbf{q}_i = Q \\mathbf{x}_i, \\quad \\mathbf{k}_i = K \\mathbf{x}_i, \\quad \\mathbf{v}_i = V \\mathbf{x}_i \\] <p>where:</p> <ul> <li>\\(Q, K, V \\in \\mathbb{R}^{d_a \\times d}\\) are learnable projection matrices  </li> <li>\\(d\\) is the input embedding dimension  </li> <li>\\(d_a\\) is the dimension of the attention subspace (often \\(d_a = d / h\\) for \\(h\\) heads)</li> </ul> <p>The dot product of \\(\\mathbf{q}_i\\) and \\(\\mathbf{k}_j\\) determines how much token \\(i\\) attends to token \\(j\\):</p> \\[ e_{ij} = \\frac{\\mathbf{q}_i^\\top \\mathbf{k}_j}{\\sqrt{d_a}}, \\quad \\alpha_{ij} = \\text{softmax}_j(e_{ij}) \\] <p>The scaling factor \\(\\sqrt{d_a}\\) stabilizes gradients by preventing dot products from growing too large.</p> \\[ \\mathbf{o}_i = \\sum_{j=1}^n \\alpha_{ij} \\mathbf{v}_j \\] <p>Each token\u2019s output \\(\\mathbf{o}_i\\) is a weighted sum of the value vectors from all tokens, where the weights are determined by how similar their keys are to the query. Each output vector therefore represents the token in the context of the entire sequence.</p> <p>Multi-head attention. Instead of one set of \\(Q, K, V\\), Transformers use \\(h\\) sets to learn diverse patterns. Each head uses its own set of \\(Q_h, K_h, V_h \\in \\mathbb{R}^{d_h \\times d}\\) with \\(d_h = d / h\\), and their outputs are concatenated:</p> \\[ \\text{MultiHead}(X) = \\text{Concat}(\\mathbf{o}_i^{(1)}, \\dots, \\mathbf{o}_i^{(h)}) W^O, \\quad W^O \\in \\mathbb{R}^{d \\times d} \\] <p>Using multiple heads allows the model to attend to different types of relationships, such as syntax, coreference, or positional patterns.</p>"},{"location":"nlp/9_selfattention/#nonlinearities-and-the-role-of-feed-forward-networks","title":"Nonlinearities and the Role of Feed-Forward Networks","text":"<p>Attention mixes information across tokens, while feed-forward networks transform information within each token. Note that the self-attention mechanism itself is linear with respect to the input embeddings. There are no activation functions inside the attention computation. Stacking more self-attention layers only re-averages and mixes value vectors.</p> <p>To introduce nonlinearity and enable richer representations, each Transformer block includes a position-wise feed-forward network applied independently to each token:</p> \\[ \\text{FFN}(\\mathbf{o}_i) = W_2 \\cdot \\text{ReLU}(W_1 \\cdot \\mathbf{o}_i + \\mathbf{b}_1) + \\mathbf{b}_2 \\] <p>where:</p> <ul> <li>\\(W_1 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d}\\) and \\(W_2 \\in \\mathbb{R}^{d \\times d_{\\text{ff}}}\\) are learnable parameters  </li> <li>\\(d_{\\text{ff}}\\) is typically larger than \\(d\\)</li> </ul> <p></p> <p>This nonlinear transformation enhances the model's capacity and expressiveness.</p>"},{"location":"nlp/9_selfattention/#masking-the-future-in-self-attention","title":"Masking the Future in Self-Attention","text":"<p>In autoregressive tasks such as language modeling or machine translation decoding, the model must not attend to future tokens.</p> <p>Naive approach (inefficient). At each time step \\(i\\), compute attention using only tokens \\(1\\) through \\(i\\). This requires sequential recomputation and defeats parallelization.</p> <p>Efficient approach: causal masking. Full attention scores are computed, but future tokens are masked by setting their logits to \\(-\\infty\\):</p> \\[ e_{ij} = \\begin{cases} \\frac{\\mathbf{q}_i^\\top \\mathbf{k}_j}{\\sqrt{d_a}}, &amp; j \\le i \\\\ -\\infty, &amp; j &gt; i \\end{cases} \\] <p>This causes \\(\\alpha_{ij} = 0\\) for future positions while preserving parallel computation.</p> <p></p> <p>Effect: Prevents information from future tokens from leaking into past representations while enabling efficient training.</p>"},{"location":"nlp/9_selfattention/#positional-encoding-in-self-attention","title":"Positional Encoding in Self-Attention","text":"<p>Since attention is content-based, it is insensitive to token positions unless position information is added.</p> <p>Absolute positional encoding. Positional vectors \\(\\mathbf{p}_i\\) are added to the input embeddings:</p> \\[ \\tilde{\\mathbf{x}}_i = \\mathbf{x}_i + \\mathbf{p}_i \\] <p>Sinusoidal (fixed) vs learned (trainable):</p> <ul> <li> <p>Sinusoidal:  </p> </li> <li> <p>Learned: Each position has a trainable vector</p> </li> </ul> <p>Relative and rotary positional encodings:</p> <ul> <li>Relative: Represent relative distance between tokens  </li> <li>Rotary (RoPE): Rotate queries and keys in complex space to encode position  </li> </ul> <p>These methods improve generalization to longer contexts and are used in modern architectures such as LLaMA and Transformer-XL.</p>"},{"location":"nlp/9_selfattention/#necessities-for-a-self-attention-building-block","title":"Necessities for a Self-Attention Building Block","text":"<ul> <li>Self-attention: the core mechanism  </li> <li>Position representations: specify sequence order since self-attention is permutation-invariant  </li> <li>Nonlinearities: applied after self-attention, typically via feed-forward networks  </li> <li>Masking: prevents future information leakage while enabling parallel computation  </li> </ul> <p>Together, these components define a complete and scalable self-attention building block.</p> <p>Stacking these blocks yields the Transformer architecture, which has become the dominant model for modern natural language processing.</p>"},{"location":"nlp/9_selfattention/#sequence-stacked-and-multi-headed-attention","title":"Sequence-Stacked and Multi-Headed Attention","text":"<p>This section reformulates self-attention in matrix form and extends it to multi-head attention, which is the core computational unit of Transformer layers.</p>"},{"location":"nlp/9_selfattention/#matrix-formulation-of-self-attention","title":"Matrix formulation of self-attention","text":"<p>Self-attention allows each token in a sequence to attend to every other token, enabling contextual representation learning. Let \\(X = [x_1, \\dots, x_n] \\in \\mathbb{R}^{n \\times d}\\) be the input sequence of \\(n\\) token embeddings, where \\(d\\) is the embedding dimension. Each row \\(x_i \\in \\mathbb{R}^d\\) corresponds to the \\(i\\)-th token. We compute projections of the input to obtain the query, key, and value matrices:</p> \\[ Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V \\] <p>where \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d_k}\\) are learned projection matrices, and \\(Q, K, V \\in \\mathbb{R}^{n \\times d_k}\\) are the resulting matrices of projected queries, keys, and values.</p> <p>This matrix formulation allows attention to be computed for all tokens simultaneously using efficient linear algebra operations.</p> <p>Interpretation of dimensions:</p> <ul> <li>\\(n\\): number of tokens in the input sequence  </li> <li>\\(d\\): original token embedding dimension  </li> <li>\\(d_k\\): dimensionality of projected query, key, and value vectors  </li> <li>\\(Q_{i\\cdot}, K_{j\\cdot} \\in \\mathbb{R}^{d_k}\\): the query vector for token \\(i\\) and the key vector for token \\(j\\) </li> </ul>"},{"location":"nlp/9_selfattention/#scaled-dot-product-attention","title":"Scaled dot-product attention","text":"<p>Attention scores are computed using scaled dot-products between queries and keys:</p> \\[ S = \\frac{Q K^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{n \\times n} \\] <p>The matrix \\(S\\) therefore contains all pairwise token interactions in the sequence. Each entry \\(S_{ij}\\) represents the unnormalized attention score from token \\(i\\) to token \\(j\\). The division by \\(\\sqrt{d_k}\\) stabilizes gradients by preventing dot products from becoming too large in high dimensions. A row-wise softmax is applied to \\(S\\) to produce attention weights, which are then used to compute a weighted sum of the value vectors:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left( \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V \\in \\mathbb{R}^{n \\times d_k} \\] <p>This formulation enables fully parallelizable all-to-all interactions between tokens in a single matrix operation. </p> <p>While single-head attention can model dependencies, it is limited to a single representation subspace.</p>"},{"location":"nlp/9_selfattention/#multi-head-self-attention","title":"Multi-head self-attention.","text":"<p>Single-head attention captures a single type of interaction. To disentangle different kinds of linguistic relationships such as syntactic structure, semantic similarity, or positional alignment, Transformers use multi-head attention, which computes multiple attention distributions in parallel.</p> <p>Let \\(h\\) be the number of attention heads. For each head \\(\\ell \\in \\{1, \\dots, h\\}\\), the input \\(X \\in \\mathbb{R}^{n \\times d}\\) is projected into separate subspaces using head-specific learned matrices:</p> \\[ Q_\\ell = X W_{\\ell Q}, \\quad K_\\ell = X W_{\\ell K}, \\quad V_\\ell = X W_{\\ell V}, \\] <p>where \\(W_{\\ell Q}, W_{\\ell K}, W_{\\ell V} \\in \\mathbb{R}^{d \\times d_h}\\) and \\(d_h = d / h\\), ensuring that total computational cost remains comparable to single-head attention.</p> <p>Each head computes its own scaled dot-product attention:</p> \\[ \\text{head}_\\ell = \\text{softmax}\\!\\left( \\frac{Q_\\ell K_\\ell^\\top}{\\sqrt{d_h}} \\right) V_\\ell \\in \\mathbb{R}^{n \\times d_h} \\] <p>The outputs of all heads are then concatenated and linearly transformed to produce the final result:</p> \\[ \\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O, \\quad W^O \\in \\mathbb{R}^{d \\times d} \\] <p>The output combines multiple perspectives on the sequence, each capturing different relational patterns.</p>"},{"location":"nlp/9_selfattention/#implementation-efficiency","title":"Implementation efficiency","text":"<p>Despite computing multiple attention heads, multi-head attention is efficient in practice. Once the projections \\(Q\\), \\(K\\), and \\(V\\) are computed jointly for all heads, the resulting tensors are reshaped into \\((n, h, d_h)\\) and then transposed to \\((h, n, d_h)\\) to enable batched attention computation across heads. Treating the head index as an additional batch dimension allows highly optimized parallel execution.</p> <p>Summary: Multi-head attention provides rich modeling capacity by enabling the network to focus on different parts of the sequence using separate learned subspaces. Each head attends to different contextual signals, and their combination offers a composite, expressive representation that is crucial for modeling complex dependencies in natural language.</p> <p></p>"},{"location":"nlp/9_selfattention/#optimization-tricks-add-norm","title":"Optimization Tricks \u2013 Add &amp; Norm","text":"<p>The Transformer uses the multi-head self-attention mechanism introduced earlier. To ensure better training dynamics and convergence, it incorporates two critical optimization components in each block:</p> <ul> <li>Residual connections  </li> <li>Layer normalization  </li> </ul> <p>These operations are often summarized as Add &amp; Norm, since they occur together at every sublayer.</p>"},{"location":"nlp/9_selfattention/#residual-connections","title":"Residual Connections","text":"<p>Residual (or skip) connections help mitigate the vanishing gradient problem and make deep networks easier to train.  Instead of applying a transformation layer directly to the input:</p> \\[ \\mathbf{x}^{(i)} = \\text{Layer}(\\mathbf{x}^{(i-1)}) \\] <p>we instead compute:</p> \\[ \\mathbf{x}^{(i)} = \\mathbf{x}^{(i-1)} + \\text{Layer}(\\mathbf{x}^{(i-1)}) \\] <p>This formulation learns only the residual between the input and the output, which often makes optimization easier. Key benefits include:</p> <ul> <li>Stable gradients: the gradient through the residual path is exactly 1, which improves backpropagation through deep networks  </li> <li>Bias toward the identity function: encourages the model to retain information from earlier layers when deeper transformations are unnecessary  </li> </ul> <p>In Transformers, residual connections are applied around both the self-attention sublayer and the feed-forward sublayer.</p>"},{"location":"nlp/9_selfattention/#layer-normalization","title":"Layer Normalization","text":"<p>Layer normalization improves training speed and stability by standardizing intermediate representations. Given a vector \\(\\mathbf{x} \\in \\mathbb{R}^d\\), layer normalization computes:</p> \\[ \\mu = \\frac{1}{d} \\sum_{j=1}^{d} x_j, \\quad \\sigma = \\sqrt{ \\frac{1}{d} \\sum_{j=1}^{d} (x_j - \\mu)^2 } \\] <p>The normalized output is then:</p> \\[ \\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\mu}{\\sigma + \\epsilon} \\odot \\gamma + \\beta \\] <p>where:</p> <ul> <li>\\(\\gamma \\in \\mathbb{R}^d\\) is a learnable gain parameter  </li> <li>\\(\\beta \\in \\mathbb{R}^d\\) is a learnable bias parameter  </li> <li>\\(\\epsilon\\) is a small constant for numerical stability  </li> </ul> <p>The normalization is applied along the feature dimension independently for each token. This reduces internal covariate shift and stabilizes training.</p> <p>Unlike batch normalization, layer normalization does not depend on batch statistics and is therefore well suited for sequence models.</p> <p>Together, multi-head attention, residual connections, and layer normalization form the backbone of modern Transformer architectures.</p>"},{"location":"nlp/tutorials/6_dpo/","title":"6 dpo","text":"<p>Modern AI models often need to optimize against human preferences or satisfaction signals rather than simple supervised targets. For example, dialog systems or summarizers may be tuned using human ratings or pairwise preferences (e.g. \u201cresponse A is better than B\u201d). Historically this has been done via Reinforcement Learning from Human Feedback (RLHF): one first trains a reward model on pairwise human judgments, then uses policy optimization (e.g. PPO) to fine-tune a language model. However, RLHF can be complex and unstable. Recent methods like Direct Preference Optimization (DPO) sidestep explicit RL. DPO directly fits the language model to the preferences using a classification loss, while still implicitly solving the same reward-maximization objective.</p> <p></p> <p>Figure: RLHF vs DPO Rafailov 2023*</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import DPOTrainer, DPOConfig\nfrom datasets import load_dataset\n\n# Load a pretrained LM and tokenizer\nmodel_name = \"gpt2-medium\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load or prepare a preference dataset: each example has 'prompt', 'chosen', 'rejected'\n# Here we use Anthropic HH-RLHF helpfulness data as an example\nprefs = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\", split=\"train\")\n\n# Configure DPO training (choose output dir, beta, etc.)\ndpo_config = DPOConfig(\n    output_dir=\"gpt2_dpo_demo\",\n    num_train_epochs=3,\n    batch_size=16,\n    beta=0.5   # controls strength of preference vs KL (tunable)\n)\n\n# Initialize the DPO trainer\ntrainer = DPOTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=dpo_config,\n    train_dataset=prefs,\n    label_names=(\"chosen\", \"rejected\")  # specify which fields to use\n)\n\n# Run fine-tuning\ntrainer.train()\n</pre> from transformers import AutoModelForCausalLM, AutoTokenizer from trl import DPOTrainer, DPOConfig from datasets import load_dataset  # Load a pretrained LM and tokenizer model_name = \"gpt2-medium\" model = AutoModelForCausalLM.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load or prepare a preference dataset: each example has 'prompt', 'chosen', 'rejected' # Here we use Anthropic HH-RLHF helpfulness data as an example prefs = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\", split=\"train\")  # Configure DPO training (choose output dir, beta, etc.) dpo_config = DPOConfig(     output_dir=\"gpt2_dpo_demo\",     num_train_epochs=3,     batch_size=16,     beta=0.5   # controls strength of preference vs KL (tunable) )  # Initialize the DPO trainer trainer = DPOTrainer(     model=model,     tokenizer=tokenizer,     args=dpo_config,     train_dataset=prefs,     label_names=(\"chosen\", \"rejected\")  # specify which fields to use )  # Run fine-tuning trainer.train()  <pre>c:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre>"},{"location":"nlp/tutorials/6_dpo/#case-study-preference-driven-optimization-in-nlp","title":"Case Study: Preference-Driven Optimization in NLP\u00b6","text":""},{"location":"nlp/tutorials/6_dpo/#direct-preference-optimization-dpo-for-language-models","title":"Direct Preference Optimization (DPO) for Language Models\u00b6","text":"<p>DPO is designed to fine-tune a pretrained language model (LM) on pairwise preference data (prompt plus two responses with a \u201cchosen\u201d better than \u201crejected\u201d label) without a separate reward-model or RL stage. Conceptually, DPO maximizes the log-likelihood of the preferred response relative to the inferior one. Suppose a prompt $x$ has two candidate completions $y^+$ (preferred) and $y^-$ (rejected). DPO posits a Bradley\u2013Terry (logistic) model over the pair, and optimizes:</p> <p>$$ L(\\theta) = -\\log \\sigma\\!\\left(\\beta \\big[ \\log P_\\theta(y^+ \\mid x) - \\log P_\\theta(y^- \\mid x) \\big]\\right), $$</p> <p>where $\\sigma$ is the sigmoid function and $\\beta &gt; 0$ is a scaling hyperparameter (analogous to an inverse KL-penalty). In words, this loss increases the LM\u2019s probability of the preferred response and decreases it for the rejected. Unlike naive probability-ratio losses, DPO\u2019s derivation shows this simple binary cross-entropy on the log-prob difference solves the same constrained RLHF objective (maximizing reward with a KL-constraint) in closed form.</p> <p>Figure 1: Workflow comparison of RLHF versus DPO. RLHF (top) first fits a reward model and then uses policy optimization (e.g. PPO) to tune the language model, whereas DPO (bottom) directly fine-tunes the model with a binary cross-entropy loss on preference pairs.</p> <p>In practice, we gather or use an existing preference dataset (e.g. the Anthropic HH-RLHF data with \u201cchosen\u201d and \u201crejected\u201d responses). We first ensure the LM is in-distribution (e.g. by supervised fine-tuning on original prompts), then run DPO training by maximizing $\\log \\sigma(\\Delta)$ with respect to $\\theta$. The DPO paper and its Hugging Face TRL implementation show that this yields models that match or exceed PPO-trained models on many tasks (e.g. sentiment steering, summarization) while being simpler to train.</p>"},{"location":"nlp/tutorials/6_dpo/#implementation-pytorch-hugging-face","title":"Implementation (PyTorch / Hugging Face)\u00b6","text":"<p>We can use the Hugging Face <code>transformers</code> and <code>trl</code> libraries. Below is a schematic snippet (assumes <code>trl</code> and <code>transformers</code> are installed):</p> <pre>from trl import DPOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ntrainer = DPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=preference_dataset,\n    tokenizer=tokenizer,\n    beta=0.1,\n)\n\ntrainer.train()\n</pre>"}]}